
@misc{noauthor_optical_nodate,
	title = {Optical {Music} {Recognition} {Based} {Deep} {Neural} {Networks} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-981-19-4775-9_136},
	urldate = {2022-10-25},
	file = {Optical Music Recognition Based Deep Neural Networks | SpringerLink:/home/ptorras/Zotero/storage/V4WP9DLA/978-981-19-4775-9_136.html:text/html},
}

@misc{shishido_listen_2021,
	title = {Listen to {Your} {Favorite} {Melodies} with {img2Mxml}, {Producing} {MusicXML} from {Sheet} {Music} {Image} by {Measure}-based {Multimodal} {Deep} {Learning}-driven {Assembly}},
	url = {http://arxiv.org/abs/2106.12037},
	abstract = {Deep learning has recently been applied to optical music recognition (OMR). However, currently OMR processing from various sheet music images still lacks precision to be widely applicable. Here, we present an MMdA (Measure-based Multimodal deep learning (DL)-driven Assembly) method allowing for end-to-end OMR processing from various images including inclined photo images. Using this method, measures are extracted by a deep learning model, aligned, and resized to be used for inference of given musical symbol components by using multiple deep learning models in sequence or in parallel. Use of each standardized measure enables efficient training of the models and accurate adjustment of five staff lines in each measure. Multiple musical symbol component category models with a small number of feature types can represent a diverse set of notes and other musical symbols including chords. This MMdA method provides a solution to end-to-end OMR processing with precision.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Shishido, Tomoyuki and Fati, Fehmiju and Tokushige, Daisuke and Ono, Yasuhiro},
	month = jun,
	year = {2021},
	note = {arXiv:2106.12037 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/BH5223Q6/Shishido et al. - 2021 - Listen to Your Favorite Melodies with img2Mxml, Pr.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/4SWQ2KGY/2106.html:text/html},
}

@article{byrd_towards_2015,
	title = {Towards a {Standard} {Testbed} for {Optical} {Music} {Recognition}: {Definitions}, {Metrics}, and {Page} {Images}},
	volume = {44},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Towards a {Standard} {Testbed} for {Optical} {Music} {Recognition}},
	url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1045424},
	doi = {10.1080/09298215.2015.1045424},
	abstract = {We posit that progress in Optical Music Recognition (OMR) has been held up for years by the absence of anything resembling the standard testbeds in use in other fields that face difficult evaluation problems. One example of such a field is text information retrieval (IR), where the Text Retrieval Conference (TREC) has annually-renewed IR tasks with accompanying data sets. In music informatics, the Music Information Retrieval Exchange (MIREX), with its annual tests and meetings held during the ISMIR conference, is a close analog to TREC; but MIREX has never had an OMR track or a collection of music such a track could employ. We describe why the absence of an OMR testbed is a problem and how this problem may be mitigated. To aid in the establishment of a standard testbed, we provide (1) a set of definitions for the complexity of music notation; (2) a set of performance metrics for OMR tools that gauge score complexity and graphical quality; and (3) a small corpus of music for use as a baseline for a proper OMR testbed.},
	language = {en},
	number = {3},
	urldate = {2022-10-25},
	journal = {Journal of New Music Research},
	author = {Byrd, Donald and Simonsen, Jakob Grue},
	month = jul,
	year = {2015},
	pages = {169--195},
	file = {Byrd and Simonsen - 2015 - Towards a Standard Testbed for Optical Music Recog.pdf:/home/ptorras/Zotero/storage/JTRNWBWI/Byrd and Simonsen - 2015 - Towards a Standard Testbed for Optical Music Recog.pdf:application/pdf},
}

@inproceedings{rios-vila_exploring_2020,
	title = {Exploring the two-dimensional nature of music notation for score recognition with end-to-end approaches},
	doi = {10.1109/ICFHR2020.2020.00044},
	abstract = {Optical Music Recognition workflows perform several steps to retrieve the content in music score images, being symbol recognition one of the key stages. State-of-the-art approaches for this stage currently address the coding of the output symbols as if they were plain text characters. However, music symbols have a two-dimensional nature that is ignored in these approaches. In this paper, we explore alternative output representations to perform music symbol recognition with state-of-the-art end-to-end neural technologies. We propose and describe new output representations which take into account the mentioned two-dimensional nature. We seek answers to the question of whether it is possible to obtain better recognition results in both printed and handwritten music scores. In this analysis, we compare the results given using three output encodings and two neural approaches. We found that one of the proposed encodings outperforms the results obtained by the standard one. This permits us to conclude that it is interesting to keep researching on this topic to improve end-to-end music score recognition.},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Ríos-Vila, Antonio and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	month = sep,
	year = {2020},
	keywords = {Deep Learning, Encoding, Image coding, Machine Learning, Music Handwritten Recognition, Neural networks, Optical imaging, Optical Music Recognition, Printed Recognition, Standards, Task analysis, Vocabulary},
	pages = {193--198},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/AHFHJV5F/9257754.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/UCILJNU8/Ríos-Vila et al. - 2020 - Exploring the two-dimensional nature of music nota.pdf:application/pdf},
}

@inproceedings{rios-vila_use_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Use} of {Transformers} for {End}-to-{End} {Optical} {Music} {Recognition}},
	isbn = {978-3-031-04881-4},
	doi = {10.1007/978-3-031-04881-4\_37},
	abstract = {State-of-the-art end-to-end Optical Music Recognition (OMR) systems use Recurrent Neural Networks to produce music transcriptions, as these models retrieve a sequence of symbols from an input staff image. However, recent advances in Deep Learning have led other research fields that process sequential data to use a new neural architecture: the Transformer, whose popularity has increased over time. In this paper, we study the application of the Transformer model to the end-to-end OMR systems. We produced several models based on all the existing approaches in this field and tested them on various corpora with different types of encodings for the output. The obtained results allow us to make an in-depth analysis of the advantages and disadvantages of applying this architecture to these systems. This discussion leads us to conclude that Transformers, as they were conceived, do not seem to be appropriate to perform end-to-end OMR, so this paper raises interesting lines of future research to get the full potential of this architecture in this field.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Ríos-Vila, Antonio and Iñesta, José M. and Calvo-Zaragoza, Jorge},
	editor = {Pinho, Armando J. and Georgieva, Petia and Teixeira, Luís F. and Sánchez, Joan Andreu},
	year = {2022},
	keywords = {Transformers, Optical Music Recognition, Connectionist Temporal Classification, Image-to-sequence},
	pages = {470--481},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/3FDVS2GD/Ríos-Vila et al. - 2022 - On the Use of Transformers for End-to-End Optical .pdf:application/pdf},
}

@article{alfaro-contreras_decoupling_2022,
	title = {Decoupling music notation to improve end-to-end {Optical} {Music} {Recognition}},
	volume = {158},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865522001428},
	doi = {10.1016/j.patrec.2022.04.032},
	abstract = {Inspired by the Text Recognition field, end-to-end schemes based on Convolutional Recurrent Neural Networks (CRNN) trained with the Connectionist Temporal Classification (CTC) loss function are considered one of the current state-of-the-art techniques for staff-level Optical Music Recognition (OMR). Unlike text symbols, music-notation elements may be defined as a combination of (i) a shape primitive located in (ii) a certain position in a staff. However, this double nature is generally neglected in the learning process, as each combination is treated as a single token. In this work, we study whether exploiting such particularity of music notation actually benefits the recognition performance and, if so, which approach is the most appropriate. For that, we thoroughly review existing specific approaches that explore this premise and propose different combinations of them. Furthermore, considering the limitations observed in such approaches, a novel decoding strategy specifically designed for OMR is proposed. The results obtained with four different corpora of historical manuscripts show the relevance of leveraging this double nature of music notation since it outperforms the standard approaches where it is ignored. In addition, the proposed decoding leads to significant reductions in the error rates with respect to the other cases.},
	language = {en},
	urldate = {2022-10-25},
	journal = {Pattern Recognition Letters},
	author = {Alfaro-Contreras, María and Ríos-Vila, Antonio and Valero-Mas, Jose J. and Iñesta, José M. and Calvo-Zaragoza, Jorge},
	month = jun,
	year = {2022},
	keywords = {Connectionist temporal classification, Deep learning, Optical music recognition, Sequence labeling},
	pages = {157--163},
	file = {ScienceDirect Full Text PDF:/home/ptorras/Zotero/storage/5CRPUK83/Alfaro-Contreras et al. - 2022 - Decoupling music notation to improve end-to-end Op.pdf:application/pdf;ScienceDirect Snapshot:/home/ptorras/Zotero/storage/3UNM7E2W/S0167865522001428.html:text/html},
}

@article{garrido-munoz_holistic_2022,
	title = {A holistic approach for image-to-graph: application to optical music recognition},
	issn = {1433-2825},
	shorttitle = {A holistic approach for image-to-graph},
	url = {https://doi.org/10.1007/s10032-022-00417-4},
	doi = {10.1007/s10032-022-00417-4},
	abstract = {A number of applications would benefit from neural approaches that are capable of generating graphs from images in an end-to-end fashion. One of these fields is optical music recognition (OMR), which focuses on the computational reading of music notation from document images. Given that music notation can be expressed as a graph, the aforementioned approach represents a promising solution for OMR. In this work, we propose a new neural architecture that retrieves a certain representation of a graph—identified by a specific order of its vertices—in an end-to-end manner. This architecture works by means of a double output: It sequentially predicts the possible categories of the vertices, along with the edges between each of their pairs. The experiments carried out prove the effectiveness of our proposal as regards retrieving graph structures from excerpts of handwritten musical notation. Our results also show that certain design decisions, such as the choice of graph representations, play a fundamental role in the performance of this approach.},
	language = {en},
	urldate = {2022-10-25},
	journal = {IJDAR},
	author = {Garrido-Munoz, Carlos and Rios-Vila, Antonio and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2022},
	keywords = {Deep learning, Optical music recognition, Graph representation},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/FZFPBEQC/Garrido-Munoz et al. - 2022 - A holistic approach for image-to-graph applicatio.pdf:application/pdf},
}

@article{calvo-zaragoza_end--end_2018,
	title = {End-to-{End} {Neural} {Optical} {Music} {Recognition} of {Monophonic} {Scores}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/8/4/606},
	doi = {10.3390/app8040606},
	abstract = {Optical Music Recognition is a field of research that investigates how to computationally decode music notation from images. Despite the efforts made so far, there are hardly any complete solutions to the problem. In this work, we study the use of neural networks that work in an end-to-end manner. This is achieved by using a neural model that combines the capabilities of convolutional neural networks, which work on the input image, and recurrent neural networks, which deal with the sequential nature of the problem. Thanks to the use of the the so-called Connectionist Temporal Classification loss function, these models can be directly trained from input images accompanied by their corresponding transcripts into music symbol sequences. We also present the Printed Music Scores dataset, containing more than 80,000 monodic single-staff real scores in common western notation, that is used to train and evaluate the neural approach. In our experiments, it is demonstrated that this formulation can be carried out successfully. Additionally, we study several considerations about the codification of the output musical sequences, the convergence and scalability of the neural models, as well as the ability of this approach to locate symbols in the input score.},
	language = {en},
	number = {4},
	urldate = {2022-10-25},
	journal = {Applied Sciences},
	author = {Calvo-Zaragoza, Jorge and Rizo, David},
	month = apr,
	year = {2018},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Deep Learning, end-to-end recognition, music score images, Optical Music Recognition},
	pages = {606},
	file = {Calvo-Zaragoza_Rizo_2018_End-to-End Neural Optical Music Recognition of Monophonic Scores.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza_Rizo_2018_End-to-End Neural Optical Music Recognition of Monophonic Scores.pdf:application/pdf;Full Text PDF:/home/ptorras/Zotero/storage/T9RAQ8FH/Calvo-Zaragoza and Rizo - 2018 - End-to-End Neural Optical Music Recognition of Mon.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/97MC58EZ/606.html:text/html},
}

@inproceedings{calvo-zaragoza_handwritten_2017,
	title = {Handwritten {Music} {Recognition} for {Mensural} {Notation}: {Formulation}, {Data} and {Baseline} {Results}},
	volume = {01},
	shorttitle = {Handwritten {Music} {Recognition} for {Mensural} {Notation}},
	doi = {10.1109/ICDAR.2017.179},
	abstract = {Music is a key element for cultural transmission, and so large collections of music manuscripts have been preserved over the centuries. In order to develop computational tools for analysis, indexing and retrieval from these sources, it is necessary to transcribe the content to some machine-readable format. In this paper we discuss the Handwritten Music Recognition problem, which refers to the development of automatic transcription systems for musical manuscripts. We focus on mensural notation, one of the most widespread varieties of Western classical music. For that, we present a labeled corpus containing 576 staves, along with a baseline recognition system based on a combination of hidden Markov models and N-gram language models. The baseline error obtained at symbol level is about 40 \% which, given the difficulty of the task, can be considered a good starting point for future developments. Our aim is that these data and preliminary results help to promote this research field, serving as a reference in future developments.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical Music Recognition, Task analysis, Hidden Markov models, Image segmentation, Handwriting recognition, Handwritten Music Recognition, Hidden Markov Models, Mensural notation, Music, Natural languages, Shape, hidden Markov models, speech recognition, indexing, music, automatic transcription systems, baseline error, baseline recognition system, computational tools, cultural transmission, handwritten music recognition problem, labeled corpus, machine-readable format, mensural notation, musical manuscripts, n-gram language models, retrieval, Western classical music},
	pages = {1081--1086},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/KDUXPCQN/8270110.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/W883QAIZ/Calvo-Zaragoza et al. - 2017 - Handwritten Music Recognition for Mensural Notatio.pdf:application/pdf},
}

@inproceedings{hajic_muscima_2017,
	title = {The {MUSCIMA}++ {Dataset} for {Handwritten} {Optical} {Music} {Recognition}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.16},
	abstract = {Optical Music Recognition (OMR) promises to make accessible the content of large amounts of musical documents, an important component of cultural heritage. However, the field does not have an adequate dataset and ground truth for benchmarking OMR systems, which has been a major obstacle to measurable progress. Furthermore, machine learning methods for OMR require training data. We design and collect MUSCIMA++, a new dataset for OMR. Ground truth in MUSCIMA++ is a notation graph, which our analysis shows to be a necessary and sufficient representation of music notation. Building on the CVC-MUSCIMA dataset for staffline removal, the MUSCIMA++ dataset v1.0 consists of 140 pages of handwritten music, with 91254 manually annotated notation symbols and 82247 explicitly marked relationships between symbol pairs. The dataset allows training and directly evaluating models for symbol classification, symbol localization, and notation graph assembly, and indirectly musical content extraction, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and annotating further, and the data is made available under an open license.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Hajič, Jan and Pecina, Pavel},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical Music Recognition, Optical character recognition software, Text analysis, Music, Complexity theory, Dataset, Graph Representation, Image reconstruction, Musical Symbol Detection, Pipelines, handwriting recognition, optical character recognition, document image processing, history, music, music notation, handwritten character recognition, image classification, learning (artificial intelligence), machine learning, MUSCIMA++ dataset, cultural heritage, CVC-MUSCIMA dataset, data visualisation, data visualization, handwritten optical Music Recognition, image preprocessing, manually annotated notation symbols, musical content extraction, musical documents, notation graph assembly, OMR systems, symbol classification, symbol localization, symbol pairs, training data},
	pages = {39--46},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/LMLY4DC8/Hajič and Pecina - 2017 - The MUSCIMA++ Dataset for Handwritten Optical Musi.pdf:application/pdf},
}

@inproceedings{baro_towards_2016,
	title = {Towards the {Recognition} of {Compound} {Music} {Notes} in {Handwritten} {Music} {Scores}},
	doi = {10.1109/ICFHR.2016.0092},
	abstract = {The recognition of handwritten music scores still remains an open problem. The existing approaches can only deal with very simple handwritten scores mainly because of the variability in the handwriting style and the variability in the composition of groups of music notes (i.e. compound music notes). In this work we focus on this second problem and propose a method based on perceptual grouping for the recognition of compound music notes. Our method has been tested using several handwritten music scores of the CVC-MUSCIMA database and compared with a commercial Optical Music Recognition (OMR) software. Given that our method is learning-free, the obtained results are promising.},
	booktitle = {2016 15th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Baró, Arnau and Riba, Pau and Fornés, Alicia},
	month = oct,
	year = {2016},
	note = {ISSN: 2167-6445},
	keywords = {Optical Music Recognition, Optical music recognition, Optical character recognition software, Handwriting recognition, Bars, Compounds, Graphics, Hand-drawn Symbol Recognition, Handwritten Music Scores, Perceptual Grouping, Rhythm, Character recognition, Pattern recognition, Hand-drawn symbols, Handwriting Styles, Music notes, Music scores, Perceptual grouping, Software testing},
	pages = {465--470},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/VJ6FIKUE/Baró et al. - 2016 - Towards the Recognition of Compound Music Notes in.pdf:application/pdf},
}

@inproceedings{baro_handwritten_2020,
	title = {Handwritten {Historical} {Music} {Recognition} by {Sequence}-to-{Sequence} with {Attention} {Mechanism}},
	doi = {10.1109/ICFHR2020.2020.00046},
	abstract = {Despite decades of research in Optical Music Recognition (OMR), the recognition of old handwritten music scores remains a challenge because of the variabilities in the handwriting styles, paper degradation, lack of standard notation, etc. Therefore, the research in OMR systems adapted to the particularities of old manuscripts is crucial to accelerate the conversion of music scores existing in archives into digital libraries, fostering the dissemination and preservation of our music heritage. In this paper we explore the adaptation of sequence-to-sequence models with attention mechanism (used in translation and handwritten text recognition) and the generation of specific synthetic data for recognizing old music scores. The experimental validation demonstrates that our approach is promising, especially when compared with long short-term memory neural networks.},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Baró, Arnau and Badal, Carles and Fornés, Alicia},
	month = sep,
	year = {2020},
	keywords = {Hidden Markov models, Training, Text recognition, Handwriting recognition, Feature extraction, Adaptation models, Decoding, Optical music recognition, Handwritten music recognition, Document image analysis and recognition, Historical Documents, Deep neural networks, Sequence to Sequence},
	pages = {205--210},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/275RLHSR/Baró et al. - 2020 - Handwritten Historical Music Recognition by Sequen.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_early_2016,
	title = {Early {Handwritten} {Music} {Recognition} with {Hidden} {Markov} {Models}},
	doi = {10.1109/ICFHR.2016.0067},
	abstract = {This work presents a statistical method to tackle the Handwritten Music Recognition task for Early notation, which comprises more than 200 different symbols. Unlike previous approaches to deal with music notation, our strategy is to perform a holistic recognition without any previous segmentation or staff removal process. The input consists of a page of a music book, which is processed to extract and normalize the staves contained. Then, a feature extraction process is applied to define such sections as a sequence of numerical vectors. The recognition is based on the use of Hidden Markov Models for the optical processing and smoothed N-grams as language model. Experimentation results over a historical archive of Hispanic music reported an error around 40 \%, which confirms our proposal as a good starting point taking into account the difficulty of the task.},
	booktitle = {2016 15th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	month = oct,
	year = {2016},
	note = {ISSN: 2167-6445},
	keywords = {Optical imaging, Hidden Markov models, Handwriting recognition, Feature extraction, Music, Image reconstruction, Computational modeling, handwritten music recognition, hidden markov models, n-grams, statistical recognition, Markov processes, Character recognition, Computational linguistics, Historical archive, Language model, Music notation, Music recognition, N-grams, Optical processing, Removal process, Statistical recognition},
	pages = {319--324},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/5SJZRNRI/7814083.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/B9CUIUR4/Calvo-Zaragoza et al. - 2016 - Early Handwritten Music Recognition with Hidden Ma.pdf:application/pdf},
}

@techreport{taubman_musichand_nodate,
	title = {{MusicHand}: {A} {Handwritten} {Music} {Recognition} {System}},
	language = {en},
	author = {Taubman, Gabriel and Jenkins, Odest Chadwicke and Hughes, John F},
	pages = {8},
	file = {Taubman et al. - MusicHand A Handwritten Music Recognition System.pdf:/home/ptorras/Zotero/storage/FMZXD3ZQ/Taubman et al. - MusicHand A Handwritten Music Recognition System.pdf:application/pdf},
}

@article{rebelo_optical_2012,
	title = {Optical music recognition: state-of-the-art and open issues},
	volume = {1},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Optical music recognition},
	url = {http://link.springer.com/10.1007/s13735-012-0004-6},
	doi = {10.1007/s13735-012-0004-6},
	language = {en},
	number = {3},
	urldate = {2022-10-25},
	journal = {Int J Multimed Info Retr},
	author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R. S. and Guedes, Carlos and Cardoso, Jaime S.},
	month = oct,
	year = {2012},
	pages = {173--190},
	file = {Rebelo et al_2012_Optical music recognition.pdf:/home/ptorras/zotpapers/Rebelo et al_2012_Optical music recognition.pdf:application/pdf;Rebelo et al. - 2012 - Optical music recognition state-of-the-art and op.pdf:/home/ptorras/Zotero/storage/HIU3RL4Y/Rebelo et al. - 2012 - Optical music recognition state-of-the-art and op.pdf:application/pdf},
}

@article{calvo-zaragoza_hybrid_2019,
	title = {Hybrid hidden {Markov} models and artificial neural networks for handwritten music recognition in mensural notation},
	volume = {22},
	issn = {1433-7541, 1433-755X},
	url = {http://link.springer.com/10.1007/s10044-019-00807-1},
	doi = {10.1007/s10044-019-00807-1},
	abstract = {In this paper, we present a hybrid approach using hidden Markov models (HMM) and artificial neural networks to deal with the task of handwritten Music Recognition in mensural notation. Previous works have shown that the task can be addressed with Gaussian density HMMs that can be trained and used in an end-to-end manner, that is, without prior segmentation of the symbols. However, the results achieved using that approach are not sufficiently accurate to be useful in practice. In this work, we hybridize HMMs with deep multilayer perceptrons (MLPs), which lead to remarkable improvements in optical symbol modeling. Moreover, this hybrid architecture maintains important advantages of HMMs such as the ability to properly model variable-length symbol sequences through segmentation-free training, and the simplicity and robustness of combining optical models with N-gram language models, which provide statistical a priori information about regularities in musical symbol concatenation observed in the training data. The results obtained with the proposed hybrid MLP-HMM approach outperform previous works by a wide margin, achieving symbol-level error rates around 26\%, as compared with about 40\% reported in previous works.},
	language = {en},
	number = {4},
	urldate = {2022-10-25},
	journal = {Pattern Anal Applic},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	month = nov,
	year = {2019},
	pages = {1573--1584},
	file = {Calvo-Zaragoza et al_2019_Hybrid hidden Markov models and artificial neural networks for handwritten.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2019_Hybrid hidden Markov models and artificial neural networks for handwritten.pdf:application/pdf;Calvo-Zaragoza et al. - 2019 - Hybrid hidden Markov models and artificial neural .pdf:/home/ptorras/Zotero/storage/RETJ6N8N/Calvo-Zaragoza et al. - 2019 - Hybrid hidden Markov models and artificial neural .pdf:application/pdf},
}

@misc{hajic_jr_search_2017,
	title = {In {Search} of a {Dataset} for {Handwritten} {Optical} {Music} {Recognition}: {Introducing} {MUSCIMA}++},
	shorttitle = {In {Search} of a {Dataset} for {Handwritten} {Optical} {Music} {Recognition}},
	url = {http://arxiv.org/abs/1703.04824},
	doi = {10.48550/arXiv.1703.04824},
	abstract = {Optical Music Recognition (OMR) has long been without an adequate dataset and ground truth for evaluating OMR systems, which has been a major problem for establishing a state of the art in the field. Furthermore, machine learning methods require training data. We analyze how the OMR processing pipeline can be expressed in terms of gradually more complex ground truth, and based on this analysis, we design the MUSCIMA++ dataset of handwritten music notation that addresses musical symbol recognition and notation reconstruction. The MUSCIMA++ dataset version 0.9 consists of 140 pages of handwritten music, with 91255 manually annotated notation symbols and 82261 explicitly marked relationships between symbol pairs. The dataset allows training and evaluating models for symbol classification, symbol localization, and notation graph assembly, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and further annotation, and the dataset itself is made available under an open license.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Hajič jr., Jan and Pecina, Pavel},
	month = mar,
	year = {2017},
	note = {arXiv:1703.04824 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.7.5},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/VZSFQD5D/Hajič jr. and Pecina - 2017 - In Search of a Dataset for Handwritten Optical Mus.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/AFCYJ9V9/1703.html:text/html;Hajic jr._Pecina_2017_In Search of a Dataset for Handwritten Optical Music Recognition.pdf:/home/ptorras/zotpapers/Hajic jr._Pecina_2017_In Search of a Dataset for Handwritten Optical Music Recognition.pdf:application/pdf},
}

@article{rebelo_optical_2010,
	title = {Optical recognition of music symbols: {A} comparative study},
	volume = {13},
	issn = {1433-2833, 1433-2825},
	shorttitle = {Optical recognition of music symbols},
	url = {http://link.springer.com/10.1007/s10032-009-0100-1},
	doi = {10.1007/s10032-009-0100-1},
	language = {en},
	number = {1},
	urldate = {2022-10-25},
	journal = {IJDAR},
	author = {Rebelo, A. and Capela, G. and Cardoso, Jaime S.},
	month = mar,
	year = {2010},
	keywords = {Music, Performance evaluation, to classify, Document image processing, Off-line recognition, Symbol recognition},
	pages = {19--31},
	file = {Rebelo et al. - 2010 - Optical recognition of music symbols A comparativ.pdf:/home/ptorras/Zotero/storage/WMIB4UW7/Rebelo et al. - 2010 - Optical recognition of music symbols A comparativ.pdf:application/pdf},
}

@inproceedings{pacha_towards_2017,
	title = {Towards {Self}-{Learning} {Optical} {Music} {Recognition}},
	doi = {10.1109/ICMLA.2017.00-60},
	abstract = {Optical Music Recognition (OMR) is a branch of artificial intelligence that aims at automatically recognizing and understanding the content of music scores in images. Several approaches and systems have been proposed that try to solve this problem by using expert knowledge and specialized algorithms that tend to fail at generalization to a broader set of scores, imperfect image scans or data of different formatting. In this paper we propose a new approach to solve OMR by investigating how humans read music scores and by imitating that behavior with machine learning. To demonstrate the power of this approach, we conduct two experiments that teach a machine to distinguish entire music sheets from arbitrary content through frame-by-frame classification and distinguishing between 32 classes of handwritten music symbols which can be a basis for object detection. Both tasks can be performed at high rates of confidence ({\textgreater} 98\%) which is comparable to the performance of humans on the same task.},
	booktitle = {2017 16th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Pacha, Alexander and Eidenberger, Horst},
	month = dec,
	year = {2017},
	keywords = {Dataset, Deep Learning, document analysis, Image recognition, Machine learning, Music, Optical Music Recognition, Robustness, Training, Visualization},
	pages = {795--800},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/VDBKQDLL/Pacha and Eidenberger - 2017 - Towards Self-Learning Optical Music Recognition.pdf:application/pdf},
}

@misc{shatri_optical_2020,
	title = {Optical {Music} {Recognition}: {State} of the {Art} and {Major} {Challenges}},
	shorttitle = {Optical {Music} {Recognition}},
	url = {http://arxiv.org/abs/2006.07885},
	doi = {10.48550/arXiv.2006.07885},
	abstract = {Optical Music Recognition (OMR) is concerned with transcribing sheet music into a machine-readable format. The transcribed copy should allow musicians to compose, play and edit music by taking a picture of a music sheet. Complete transcription of sheet music would also enable more efficient archival. OMR facilitates examining sheet music statistically or searching for patterns of notations, thus helping use cases in digital musicology too. Recently, there has been a shift in OMR from using conventional computer vision techniques towards a deep learning approach. In this paper, we review relevant works in OMR, including fundamental methods and significant outcomes, and highlight different stages of the OMR pipeline. These stages often lack standard input and output representation and standardised evaluation. Therefore, comparing different approaches and evaluating the impact of different processing methods can become rather complex. This paper provides recommendations for future work, addressing some of the highlighted issues and represents a position in furthering this important field of research.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Shatri, Elona and Fazekas, György},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07885 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/FKYVFYAY/Shatri and Fazekas - 2020 - Optical Music Recognition State of the Art and Ma.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/WIQ53NUE/2006.html:text/html},
}

@inproceedings{villarreal_handwritten_2020,
	title = {Handwritten {Music} {Recognition} {Improvement} through {Language} {Model} {Re}-interpretation for {Mensural} {Notation}},
	doi = {10.1109/ICFHR2020.2020.00045},
	abstract = {Handwritten Music Recognition studies techniques for computers to transcribe handwritten musical notation that is registered in document images into electronic format, and to make this music available to the public. This task has been of great interest lately, as the technologies improve and can get better and better results on this problem. Recent machine intelligent approaches based on Deep and Recurrent Neural Networks have already shown how they work significantly better in the problem than traditional HMM-based approaches, especially when we are talking about Mensural Notation. These Neural Network-based researches have investigated the task of recognizing Mensural Notation as another written text recognition task, but have not explored the characteristics of musical elements in depth. Other papers have tried to dig deeper into analyzing musical elements and the extraction of their characteristics from segmented symbols, without reflecting this in holistic way. In this paper, we will try to make a complete recognition system directly from the scores, using techniques that enhance information obtained from symbols. We explore other language model interpretations and test our proposal on a publicly available dataset. In our experiments, we have made a 31\% relative improvement in regards to error at the symbol level. With this, we have gone from a 3.91\% absolute error rate, using Neural Network-based technology, to a 2.70\% absolute error rate, by using language model re-interpretations.},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Villarreal, Manuel and Sánchez, Joan Andreu},
	month = sep,
	year = {2020},
	keywords = {Task analysis, Hidden Markov models, Training, Handwriting recognition, Music, Shape, Error analysis},
	pages = {199--204},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/ZBQX7WAI/9257626.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/F9Z49ATX/Villarreal and Sánchez - 2020 - Handwritten Music Recognition Improvement through .pdf:application/pdf},
}

@inproceedings{pacha_handwritten_2018,
	title = {Handwritten {Music} {Object} {Detection}: {Open} {Issues} and {Baseline} {Results}},
	shorttitle = {Handwritten {Music} {Object} {Detection}},
	doi = {10.1109/DAS.2018.51},
	abstract = {Optical Music Recognition (OMR) is the challenge of understanding the content of musical scores. Accurate detection of individual music objects is a critical step in processing musical documents because a failure at this stage corrupts any further processing. So far, all proposed methods were either limited to typeset music scores or were built to detect only a subset of the available classes of music symbols. In this work, we propose an end-to-end trainable object detector for music symbols that is capable of detecting almost the full vocabulary of modern music notation in handwritten music scores. By training deep convolutional neural networks on the recently released MUSCIMA++ dataset which has symbol-level annotations, we show that a machine learning approach can be used to accurately detect music objects with a mean average precision of over 80\%.},
	booktitle = {2018 13th {IAPR} {International} {Workshop} on {Document} {Analysis} {Systems} ({DAS})},
	author = {Pacha, Alexander and Choi, Kwon-Young and Coüasnon, Bertrand and Ricquebourg, Yann and Zanibbi, Richard and Eidenberger, Horst},
	month = apr,
	year = {2018},
	keywords = {Deep Learning, Optical Music Recognition, Vocabulary, Training, Feature extraction, Music, Detectors, Handwritten Scores, Object detection, Object Detection, Semantics},
	pages = {163--168},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/4EY2Y2JN/Pacha et al. - 2018 - Handwritten Music Object Detection Open Issues an.pdf:application/pdf;Pacha et al_2018_Handwritten Music Object Detection.pdf:/home/ptorras/zotpapers/Pacha et al_2018_Handwritten Music Object Detection.pdf:application/pdf;Pacha et al_2018_Handwritten Music Object Detection.pdf:/home/ptorras/zotpapers/Pacha et al_2018_Handwritten Music Object Detection2.pdf:application/pdf},
}

@article{fornes_cvc-muscima_2012,
	title = {{CVC}-{MUSCIMA}: a ground truth of handwritten music score images for writer identification and staff removal},
	volume = {15},
	issn = {1433-2833, 1433-2825},
	shorttitle = {{CVC}-{MUSCIMA}},
	url = {http://link.springer.com/10.1007/s10032-011-0168-2},
	doi = {10.1007/s10032-011-0168-2},
	abstract = {The analysis of music scores has been an active research ﬁeld in the last decades. However, there are no publicly available databases of handwritten music scores for the research community. In this paper, we present the CVCMUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets written by 50 different musicians. It has been especially designed for writer identiﬁcation and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches.},
	language = {en},
	number = {3},
	urldate = {2022-10-25},
	journal = {IJDAR},
	author = {Fornés, Alicia and Dutta, Anjan and Gordo, Albert and Lladós, Josep},
	month = sep,
	year = {2012},
	keywords = {Graphics recognition, Staff removal, Performance evaluation, Music scores, Ground truths, Handwritten documents, Writer identification},
	pages = {243--251},
	file = {Fornés et al. - 2012 - CVC-MUSCIMA a ground truth of handwritten music s.pdf:/home/ptorras/Zotero/storage/D7ZKEEVR/Fornés et al. - 2012 - CVC-MUSCIMA a ground truth of handwritten music s.pdf:application/pdf},
}

@article{paul_ensemble_2022,
	title = {An ensemble of deep transfer learning models for handwritten music symbol recognition},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06629-9},
	doi = {10.1007/s00521-021-06629-9},
	abstract = {In ancient times, there was no system to record or document music. A basic notation system to write European music was formulated around 14th century in the Baroque period which slowly evolved into the standard notation system that we have today. Later, the musical pieces from the classical and post-classical period of European music were documented as scores using this standard European staff notations. These notations are used by most of the modern genres of music due to their versatility. Hence, it is very important to develop a method that can store such music sheets containing handwritten music scores digitally. Optical music recognition (OMR) is a system that automatically interprets the scanned handwritten music scores. In this work, we have proposed a classiﬁer ensemble of deep transfer learning models with support vector machine (SVM) as the aggregator for handwritten music symbol recognition. We have applied three pre-trained deep learning models, namely ResNet50, GoogleNet and DenseNet161 (each trained on ImageNet), and ﬁne-tuned on our target datasets i.e., music symbol image datasets. The proposed ensemble technique can capture a more complex association of the base classiﬁers, thus improving the overall performance. We have evaluated the proposed model on ﬁve publicly available standard datasets, namely Handwritten Online Music Symbols (HOMUS), Capitan\_Score\_Uniform, Capitan\_Score\_Nonuniform, Rebelo\_real and Forne´s, and achieved state-of-the-art results for all these datasets. Additionally, we have evaluated our model on publicly available two non-music symbols datasets, namely CMATERdb 2.1.2 containing 120 handwritten Bangla city names and CMATERdb 3.1.1 dataset containing handwritten Bangla numerals to validate its effectiveness on diversiﬁed datasets. The source code of this present work is available at https://github.com/ashis0013/ Music-Symbol-Recognition.},
	language = {en},
	number = {13},
	urldate = {2022-10-25},
	journal = {Neural Comput \& Applic},
	author = {Paul, Ashis and Pramanik, Rishav and Malakar, Samir and Sarkar, Ram},
	month = jul,
	year = {2022},
	pages = {10409--10427},
	file = {Paul et al. - 2022 - An ensemble of deep transfer learning models for h.pdf:/home/ptorras/Zotero/storage/VCKPGGMN/Paul et al. - 2022 - An ensemble of deep transfer learning models for h.pdf:application/pdf},
}

@article{wen_new_2015,
	title = {A new optical music recognition system based on combined neural network},
	volume = {58},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865515000392},
	doi = {10.1016/j.patrec.2015.02.002},
	abstract = {Optical music recognition (OMR) is an important tool to recognize a scanned page of music sheet automatically, which has been applied to preserving music scores. In this paper, we propose a new OMR system to recognize the music symbols without segmentation. We present a new classifier named combined neural network (CNN) that offers superior classification capability. We conduct tests on fifteen pages of music sheets, which are real and scanned images. The tests show that the proposed method constitutes an interesting contribution to OMR.},
	language = {en},
	urldate = {2022-10-25},
	journal = {Pattern Recognition Letters},
	author = {Wen, Cuihong and Rebelo, Ana and Zhang, Jing and Cardoso, Jaime},
	month = jun,
	year = {2015},
	keywords = {Optical music recognition, Image processing, Neural network},
	pages = {1--7},
	file = {Accepted Version:/home/ptorras/Zotero/storage/N2IZQRPF/Wen et al. - 2015 - A new optical music recognition system based on co.pdf:application/pdf;Wen et al_2015_A new optical music recognition system based on combined neural network.pdf:/home/ptorras/zotpapers/Wen et al_2015_A new optical music recognition system based on combined neural network.pdf:application/pdf},
}

@article{tardon_optical_2009,
	title = {Optical {Music} {Recognition} for {Scores} {Written} in {White} {Mensural} {Notation}},
	volume = {2009},
	issn = {1687-5176, 1687-5281},
	url = {http://jivp.eurasipjournals.com/content/2009/1/843401},
	doi = {10.1155/2009/843401},
	language = {en},
	urldate = {2022-10-25},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Tardón, Lorenzo J. and Sammartino, Simone and Barbancho, Isabel and Gómez, Verónica and Oliver, Antonio},
	year = {2009},
	pages = {1--23},
	file = {Tardón et al. - 2009 - Optical Music Recognition for Scores Written in Wh.pdf:/home/ptorras/Zotero/storage/GR3CKIFR/Tardón et al. - 2009 - Optical Music Recognition for Scores Written in Wh.pdf:application/pdf},
}

@inproceedings{hajic_jr_how_2018,
	address = {New York, NY, USA},
	series = {{DLfM} '18},
	title = {How current optical music recognition systems are becoming useful for digital libraries},
	isbn = {978-1-4503-6522-2},
	url = {https://doi.org/10.1145/3273024.3273034},
	doi = {10.1145/3273024.3273034},
	abstract = {Optical Music Recognition (OMR) promises to make large collections of sheet music searchable by their musical content. It would open up novel ways of accessing the vast amount of written music that has never been recorded before. For a long time, OMR was not living up to that promise, as its performance was simply not good enough, especially on handwritten music or under non-ideal image conditions. However, OMR has recently seen a number of improvements, mainly due to the advances in machine learning. In this work, we take an OMR system based on the traditional pipeline and an end-to-end system, which represent the current state of the art, and illustrate in proof-of-concept experiments their applicability in retrieval settings. We also provide an example of a musicological study that can be replicated with OMR outputs at much lower costs. Taken together, this indicates that in some settings, current OMR can be used as a general tool for enriching digital libraries.},
	urldate = {2022-10-25},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Hajič jr., Jan and Kolárová, Marta and Pacha, Alexander and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2018},
	keywords = {digital musicology, music digital libraries, music information retrieval, optical music recognition, symbolic music search},
	pages = {57--61},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/3MPBPWPQ/jr et al. - 2018 - How current optical music recognition systems are .pdf:application/pdf},
}

@inproceedings{rizo_muret_2018,
	address = {New York, NY, USA},
	series = {{DLfM} '18},
	title = {{MuRET}: a music recognition, encoding, and transcription tool},
	isbn = {978-1-4503-6522-2},
	shorttitle = {{MuRET}},
	url = {https://doi.org/10.1145/3273024.3273029},
	doi = {10.1145/3273024.3273029},
	abstract = {The transcription process from early and modern notation manuscripts to a structured digital encoding has been traditionally performed following a fully manual workflow. At most it has received some technological support in particular stages, like optical music recognition (OMR) of the source images, or transcription to modern notation with music edition applications. Currently, there is no mature and stable enough solution for the OMR problem, and the most used music editors do not support early notations, such as the mensural one. In this work, a new tool called MUsic Recognition, Encoding, and Transcription (MuRET) is introduced, which covers all transcription phases, from the manuscript source to the encoded digital content. MuRET is designed as a technology-focused research tool, allowing different processing approaches to be used, and producing both the expected transcribed contents in standard encodings and data for the study of the transcription process itself.},
	urldate = {2022-10-25},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Rizo, David and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	month = sep,
	year = {2018},
	keywords = {optical music recognition, encoding, notation transcription},
	pages = {52--56},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/MW8LK8YE/Rizo et al. - 2018 - MuRET a music recognition, encoding, and transcri.pdf:application/pdf},
}

@incollection{fornes_analysis_2014,
	title = {Analysis and recognition of music scores},
	isbn = {978-0-85729-859-1},
	abstract = {© Springer-Verlag London 2014. All rights are reserved. The analysis and recognition of music scores has attracted the interest of researchers for decades. Optical Music Recognition (OMR) is a classical research field of Document Image Analysis and Recognition (DIAR), whose aim is to extract information from music scores. Music scores contain both graphical and textual information, and for this reason, techniques are closely related to graphics recognition and text recognition. Since music scores use a particular diagrammatic notation that follow the rules of music theory, many approaches make use of context information to guide the recognition and solve ambiguities. This chapter overviews the main Optical Music Recognition (OMR) approaches. Firstly, the different methods are grouped according to the OMR stages, namely, staff removal, music symbol recognition, and syntactical analysis. Secondly, specific approaches for old and handwritten music scores are reviewed. Finally, online approaches and commercial systems are also commented.},
	booktitle = {Handbook of {Document} {Image} {Processing} and {Recognition}},
	author = {Fornés, Alicia and Sánchez, Gemma},
	month = jan,
	year = {2014},
	doi = {10.1007/978-0-85729-859-1_24},
	keywords = {Optical music recognition, Graphics recognition, Staff removal, Symbolrecognition},
	pages = {749--774},
}

@inproceedings{homenda_optical_2005,
	address = {Berlin, Heidelberg},
	series = {Advances in {Soft} {Computing}},
	title = {Optical {Music} {Recognition}: the {Case} {Study} of {Pattern} {Recognition}},
	isbn = {978-3-540-32390-7},
	shorttitle = {Optical {Music} {Recognition}},
	doi = {10.1007/3-540-32390-2_98},
	abstract = {The paper presents a pattern recognition study aimed on music notation recognition. The study is focused on practical aspect of optical music recognition; it presents a variety of methods applied in optical music recognition technology. The following logically separated stages of music notation recognition are distinguished: acquiring music notation structure, recognizing symbols of music notation, analyzing contextual information. The directions for OMR package development are drawn.},
	language = {en},
	booktitle = {Computer {Recognition} {Systems}},
	publisher = {Springer},
	author = {Homenda, Wladyslaw},
	editor = {Kurzyński, Marek and Puchała, Edward and Woźniak, Michał and żołnierek, Andrzej},
	year = {2005},
	keywords = {Music Knowledge, Music Notation, Music Symbol, Rhythmic Grouping, Staff Line},
	pages = {835--842},
}

@inproceedings{riba_towards_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards the {Alignment} of {Handwritten} {Music} {Scores}},
	isbn = {978-3-319-52159-6},
	doi = {10.1007/978-3-319-52159-6_8},
	abstract = {It is very common to find different versions of the same music work in archives of Opera Theaters. These differences correspond to modifications and annotations from the musicians. From the musicologist point of view, these variations are very interesting and deserve study. This paper explores the alignment of music scores as a tool for automatically detecting the passages that contain such differences. Given the difficulties in the recognition of handwritten music scores, our goal is to align the music scores and at the same time, avoid the recognition of music elements as much as possible. After removing the staff lines, braces and ties, the bar lines are detected. Then, the bar units are described as a whole using the Blurred Shape Model. The bar units alignment is performed by using Dynamic Time Warping. The analysis of the alignment path is used to detect the variations in the music scores. The method has been evaluated on a subset of the CVC-MUSCIMA dataset, showing encouraging results.},
	language = {en},
	booktitle = {Graphic {Recognition}. {Current} {Trends} and {Challenges}},
	publisher = {Springer International Publishing},
	author = {Riba, Pau and Fornés, Alicia and Lladós, Josep},
	editor = {Lamiroy, Bart and Dueire Lins, Rafael},
	year = {2017},
	keywords = {Optical music recognition, Dynamic time warping alignment, Handwritten music scores, Character recognition, Pattern recognition, Music scores, Bar units, Dynamic time warping, Shape model},
	pages = {103--116},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/P2BBMNR8/Riba et al. - 2017 - Towards the Alignment of Handwritten Music Scores.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_probabilistic_2018,
	title = {Probabilistic {Music}-{Symbol} {Spotting} in {Handwritten} {Scores}},
	doi = {10.1109/ICFHR-2018.2018.00103},
	abstract = {Content-based search on musical manuscripts is usually performed assuming that there are accurate transcripts of the sources in a symbolic, structured format. Given that current systems for Handwritten Music Recognition are far from offering guarantees about their accuracy, this traditional approach does not represent a scalable scenario. In this work we propose a probabilistic framework for Music-Symbol Spotting (MSS), that allows for content-based music search directly over the images of the manuscripts. By means of statistical recognition systems, a probabilistic index is built upon which the search can be carried out efficiently. Our experiments over a dataset of an Early handwritten music manuscript in Mensural notation demonstrates that this MSS framework can be presented as a promising alternative to the traditional approach for content-based music search.},
	booktitle = {2018 16th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	month = aug,
	year = {2018},
	keywords = {Handwriting recognition, Hidden Markov models, Indexes, Music, Music Spotting, Music manuscripts, Hidden Markov Models, Probabilistic indexing, Mensural Notation, Probabilistic logic, Random variables, Task analysis},
	pages = {558--563},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/YR8H7IRP/Calvo-Zaragoza et al. - 2018 - Probabilistic Music-Symbol Spotting in Handwritten.pdf:application/pdf},
}

@inproceedings{pacha_towards_2017-1,
	title = {Towards a {Universal} {Music} {Symbol} {Classifier}},
	volume = {02},
	doi = {10.1109/ICDAR.2017.265},
	abstract = {Optical Music Recognition (OMR) aims to recognize and understand written music scores. With the help of Deep Learning, researchers were able to significantly improve the stateof- the-art in this research area. However, Deep Learning requires a substantial amount of annotated data for supervised training. Various datasets have been collected in the past, but without a common standard that defines data formats and terminology, combining them is a challenging task. In this paper we present our approach towards unifying multiple datasets into the largest currently available body of over 90000 musical symbols that belong to 79 classes, containing both handwritten and printed music symbols. A universal music symbol classifier, trained on such a dataset using Deep Learning, can achieve an accuracy that exceeds 98\%.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Pacha, Alexander and Eidenberger, Horst},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {classification, dataset, deep learning, Machine learning, Music, Neural networks, Optical imaging, Optical Music Recognition, Text analysis, Tools, Training},
	pages = {35--36},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/6I9X5II2/Pacha and Eidenberger - 2017 - Towards a Universal Music Symbol Classifier.pdf:application/pdf},
}

@phdthesis{hajic_jr_optical_2019,
	title = {Optical {Recognition} of {Handwritten} {Music} {Notation}},
	url = {https://dspace.cuni.cz/handle/20.500.11956/110342},
	abstract = {Optical Music Recognition (OMR) is the field of computationally reading music notation. This thesis presents, in the form of dissertation by publication, contributions to the theory, resources, and methods of OMR especially for handwritten notation. The main contributions are (1) the Music Notation Graph (MuNG) formalism for describing arbitrarily complex music notation using an oriented graph that can be unambiguously interpreted in terms of musical semantics, (2) the MUSCIMA++ dataset of musical manuscripts with MuNG as ground truth that can be used to train and evaluate OMR systems and subsystems from the image all the way to extracting the musical semantics encoded therein, and (3) a pipeline for performing OMR on musical manuscripts that relies on machine learning both for notation symbol detection and the notation assembly stage, and on properties of the inferred MuNG representation to deterministically extract the musical semantics. While the the OMR pipeline does not perform flawlessly, this is the first OMR system to perform at basic useful tasks over musical semantics extracted from handwritten music notation of arbitrary complexity.},
	language = {en\_US},
	urldate = {2022-10-25},
	author = {Hajič jr., Jan},
	month = jun,
	year = {2019},
	note = {Accepted: 2019-10-18T12:22:42Z
Publisher: Univerzita Karlova, Matematicko-fyzikální fakulta},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/JJVEVSX8/Hajič - 2019 - Optical Recognition of Handwritten Music Notation.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/3GYPQIYG/110342.html:text/html},
}

@inproceedings{lee_comparison_2010,
	title = {A comparison of {HMM}, {Naïve} {Bayesian}, and {Markov} model in exploiting knowledge content in digital ink: {A} case study on handwritten music notation recognition},
	shorttitle = {A comparison of {HMM}, {Naïve} {Bayesian}, and {Markov} model in exploiting knowledge content in digital ink},
	doi = {10.1109/ICME.2010.5583292},
	abstract = {The performance of a model is dependent not only on the amount of knowledge available to the model but also on how the knowledge is exploited. We investigate the recognition of handwritten musical notation based on three related probabilistic inference techniques: Hidden Markov Models (HMMs), Markov Models (MMs) and Naïve Bayes (NBs). Music notes are written on a tablet. A sequence of ink patterns representing this symbol is captured and subsequently employed for constructing the models of HMMs, MMs and NBs. The proposed approach exploits both global and local information derived from ink patterns which we have demonstrated the exploitation of this information via different features employed in different HMMs. The specificity and sensitivity measures of these classification models are compared using unseen test datasets. The findings show that HMM outperformed MM and NB models, due to the ability of HMM in exploiting both transitional probability (transition matrix A) and the overall likelihood of the observed events (emission matrix B). Also, HMMs with more hidden states outperformed those with less states, since a larger model has more capacity. In conclusion, our approach demonstrated that HMM can better exploit information extracted from ink patterns than models of MM or NB, and therefore is an optimal inference technique to encoding useful information for musical notation representation.},
	booktitle = {2010 {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Lee, Kian Chin and Phon-Amnuaisuk, Somnuk and Ting, Choo Yee},
	month = jul,
	year = {2010},
	note = {ISSN: 1945-788X},
	keywords = {Hidden Markov models, Handwriting recognition, Music, Hidden Markov Model, Ink, Markov processes, Niobium, Numerical models, Pen-based Music Editing, Recognizing Handwritten Music Notation, Representing and Reasoning about Music},
	pages = {292--297},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/AYC4NWEE/Lee et al. - 2010 - A comparison of HMM, Naïve Bayesian, and Markov mo.pdf:application/pdf},
}

@inproceedings{fornes_old_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Old {Handwritten} {Musical} {Symbol} {Classification} by a {Dynamic} {Time} {Warping} {Based} {Method}},
	isbn = {978-3-540-88188-9},
	doi = {10.1007/978-3-540-88188-9_6},
	abstract = {A growing interest in the document analysis field is the recognition of old handwritten documents, towards the conversion into a readable format. The difficulties when we work with old documents are increased, and other techniques are required for recognizing handwritten graphical symbols that are drawn in such these documents. In this paper we present a Dynamic Time Warping based method that outperforms the classical descriptors, being also invariant to scale, rotation, and elastic deformations typical found in handwriting musical notation.},
	language = {en},
	booktitle = {Graphics {Recognition}. {Recent} {Advances} and {New} {Opportunities}},
	publisher = {Springer},
	author = {Fornés, Alicia and Lladós, Josep and Sánchez, Gemma},
	editor = {Liu, Wenyin and Lladós, Josep and Ogier, Jean-Marc},
	year = {2008},
	keywords = {Dynamic Time Warping, Handwritten Document, Input Symbol, Symbol Recognition, Zernike Moment},
	pages = {51--60},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/5QK5R5ZE/Fornés et al. - 2008 - Old Handwritten Musical Symbol Classification by a.pdf:application/pdf},
}

@inproceedings{mateiu_domain_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain {Adaptation} for {Handwritten} {Symbol} {Recognition}: {A} {Case} of {Study} in {Old} {Music} {Manuscripts}},
	isbn = {978-3-030-31321-0},
	shorttitle = {Domain {Adaptation} for {Handwritten} {Symbol} {Recognition}},
	doi = {10.1007/978-3-030-31321-0_12},
	abstract = {The existence of a large amount of untranscripted music manuscripts has caused initiatives that use Machine Learning (ML) for Optical Music Recognition, in order to efficiently transcribe the music sources into a machine-readable format. Although most music manuscript are similar in nature, they inevitably vary from one another. This fact can negatively influence the complexity of the classification task because most ML models fail to transfer their knowledge from one domain to another, thereby requiring learning from scratch on new domains after manually labeling new data. This work studies the ability of a Domain Adversarial Neural Network for domain adaptation in the context of classifying handwritten music symbols. The main idea is to exploit the knowledge of a specific manuscript to classify symbols from different (unlabeled) manuscripts. The reported results are promising, obtaining a substantial improvement over a conventional Convolutional Neural Network approach, which can be used as a basis for future research.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Mateiu, Tudor N. and Gallego, Antonio-Javier and Calvo-Zaragoza, Jorge},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	keywords = {Convolutional Neural Network, Domain Adaptation, Handwritten music symbols, Optical Music Recognition},
	pages = {135--146},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/BX3DNR72/Mateiu et al. - 2019 - Domain Adaptation for Handwritten Symbol Recogniti.pdf:application/pdf},
}

@incollection{alfaro-contreras_approaching_2019,
	title = {Approaching {End}-to-{End} {Optical} {Music} {Recognition} for {Homophonic} {Scores}},
	isbn = {978-3-030-31320-3},
	abstract = {The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Recurrent Neural Networks (RNN) have been broadly applied to solve these task with good results in an end-to-end fashion. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements at the same horizontal position, disrupting the linear flow of the time line. In this paper we study the ability of the RNNs to learn codes that represent this disruption in homophonic scores. The results prove that our serialized ways of encoding the music content are appropriate for Deep Learning-based OMR and they deserve further study.},
	author = {Alfaro-Contreras, María and Calvo-Zaragoza, Jorge and Iñesta, Jose},
	month = sep,
	year = {2019},
	doi = {10.1007/978-3-030-31321-0_13},
	pages = {147--158},
	file = {Alfaro-Contreras et al_2019_Approaching End-to-End Optical Music Recognition for Homophonic Scores.pdf:/home/ptorras/zotpapers/Alfaro-Contreras et al_2019_Approaching End-to-End Optical Music Recognition for Homophonic Scores.pdf:application/pdf},
}

@article{alfaro-contreras_exploiting_2021,
	title = {Exploiting the two-dimensional nature of agnostic music notation for neural optical music recognition},
	volume = {11},
	number = {8},
	journal = {Applied Sciences},
	author = {Alfaro-Contreras, María and Valero-Mas, Jose J.},
	year = {2021},
	note = {Publisher: MDPI},
	pages = {3621},
	file = {Alfaro-Contreras_Valero-Mas_2021_Exploiting the two-dimensional nature of agnostic music notation for neural.pdf:/home/ptorras/zotpapers/Alfaro-Contreras_Valero-Mas_2021_Exploiting the two-dimensional nature of agnostic music notation for neural.pdf:application/pdf},
}

@article{calvo-zaragoza_understanding_2021,
	title = {Understanding {Optical} {Music} {Recognition}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3397499},
	doi = {10.1145/3397499},
	abstract = {For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: Few introductory materials are available, and, furthermore, the field has struggled with defining itself and building a shared terminology. In this work, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, and (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.},
	language = {en},
	number = {4},
	urldate = {2022-10-26},
	journal = {ACM Comput. Surv.},
	author = {Calvo-Zaragoza, Jorge and Hajič Jr., Jan and Pacha, Alexander},
	month = jul,
	year = {2021},
	keywords = {Optical music recognition, music notation, music scores},
	pages = {1--35},
	file = {Calvo-Zaragoza et al_2019_Understanding Optical Music Recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2019_Understanding Optical Music Recognition.pdf:application/pdf;Calvo-Zaragoza et al_2020_Understanding Optical Music Recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2020_Understanding Optical Music Recognition.pdf:application/pdf;Calvo-Zaragoza et al. - 2021 - Understanding Optical Music Recognition.pdf:/home/ptorras/Zotero/storage/34XKQLKT/Calvo-Zaragoza et al. - 2021 - Understanding Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{choi_cnn-based_2019,
	title = {{CNN}-{Based} {Accidental} {Detection} in {Dense} {Printed} {Piano} {Scores}},
	doi = {10.1109/ICDAR.2019.00082},
	abstract = {The recognition of mid-18th to mid-20th century piano scores presents segmentation challenges caused by touching and broken symbols produced by imprinting techniques and time degradation. We present a new notehead accidental dataset containing 2955 images from dense and damaged piano scores. We address this detection problem with very small training samples using a simple Spatial Transformer (ST)-based Convolutional Neural Network detector improved through bootstrapping and contextual information, and more powerful deep learning detectors (Faster R-CNN, R-FCN, and SSD) with transfer-learning on the COCO dataset. We trained all our detectors using 5 fold cross-validation and obtain 98.73\% mean Average Precision (mAP) for an Intersection over Union (IoU) threshold of 0.75 with our best detector. Our ST-based detector obtains a slightly lower mAP of 94.81\%, but runs 40 times faster, and uses 18 times less memory.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Choi, Kwon-Young and Coüasnon, Bertrand and Ricquebourg, Yann and Zanibbi, Richard},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Deep Learning, Optical Music Recognition, Task analysis, Training, Feature extraction, Detectors, Data Augmentation, Grammar, Head, Proposals, Symbol Detection},
	pages = {473--480},
	file = {Choi et al_2019_CNN-Based Accidental Detection in Dense Printed Piano Scores.pdf:/home/ptorras/zotpapers/Choi et al_2019_CNN-Based Accidental Detection in Dense Printed Piano Scores.pdf:application/pdf},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/ptorras/Zotero/storage/E9V5EXR8/1409.html:text/html;Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/home/ptorras/zotpapers/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2022-10-27},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {He et al_2016_Deep Residual Learning for Image Recognition.pdf:/home/ptorras/zotpapers/He et al_2016_Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{graves_novel_2009,
	title = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},
	volume = {31},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2008.137},
	abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Graves, Alex and Liwicki, Marcus and Fernández, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, Jürgen},
	month = may,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {\_tablet, bidirectional long short-term memory, Character recognition, connectionist temporal classification, Connectionist temporal classification, Databases, Handwriting recognition, hidden Markov model., Hidden Markov models, Labeling, Long Short-Term Memory, offline handwriting, Offline handwriting recognition, online handwriting, Online handwriting recognition, recurrent neural networks, Recurrent neural networks, Robustness, Size measurement, Speech, Text recognition, Unconstrained handwriting recognition},
	pages = {855--868},
	file = {Graves et al_2009_A Novel Connectionist System for Unconstrained Handwriting Recognition.pdf:/home/ptorras/zotpapers/Graves et al_2009_A Novel Connectionist System for Unconstrained Handwriting Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/GWBDGYRT/4531750.html:text/html},
}

@article{baro_optical_2019,
	title = {From {Optical} {Music} {Recognition} to {Handwritten} {Music} {Recognition}: {A} baseline},
	volume = {123},
	issn = {01678655},
	shorttitle = {From {Optical} {Music} {Recognition} to {Handwritten} {Music} {Recognition}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865518303386},
	doi = {10.1016/j.patrec.2019.02.029},
	language = {en},
	urldate = {2022-11-14},
	journal = {Pattern Recognition Letters},
	author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
	month = may,
	year = {2019},
	keywords = {Optical music recognition, Deep neural networks, Document image analysis and recognition, Handwritten music recognition, LSTM},
	pages = {1--8},
	file = {Baro et al_2019_From Optical Music Recognition to Handwritten Music Recognition.pdf:/home/ptorras/zotpapers/Baro et al_2019_From Optical Music Recognition to Handwritten Music Recognition.pdf:application/pdf;Baró et al. - 2019 - From Optical Music Recognition to Handwritten Musi.pdf:/home/ptorras/Zotero/storage/Y7CYINMC/Baró et al. - 2019 - From Optical Music Recognition to Handwritten Musi.pdf:application/pdf},
}

@inproceedings{torras_integration_2021,
	address = {Online},
	title = {On the {Integration} of {Language} {Models} into {Sequence} to {Sequence} {Architectures} for {Handwritten} {Music} {Recognition}},
	url = {https://zenodo.org/record/5624451},
	abstract = {Despite the latest advances in Deep Learning, the recognition of handwritten music scores is still a challenging endeavour. Even though the recent Sequence to Sequence (Seq2Seq) architectures have demonstrated its capacity to reliably recognise handwritten text, their performance is still far from satisfactory when applied to historical handwritten scores. Indeed, the ambiguous nature of handwriting, the non-standard musical notation employed by composers of the time and the decaying state of old paper make these scores remarkably difficult to read, sometimes even by trained humans. Thus, in this work we explore the incorporation of language models into a Seq2Seq-based architecture to try to improve transcriptions where the aforementioned unclear writing produces statistically unsound mistakes, which as far as we know, has never been attempted for this field of research on this architecture. After studying various Language Model integration techniques, the experimental evaluation on historical handwritten music scores shows a significant improvement over the state of the art, showing that this is a promising research direction for dealing with such difficult manuscripts.},
	urldate = {2022-11-15},
	author = {Torras, Pau and Baró, Arnau and Kang, Lei and Fornés, Alicia},
	month = nov,
	year = {2021},
	doi = {10.5281/zenodo.5624451},
	note = {Pages: 690-696
Publication Title: Proceedings of the 22nd International Society for Music Information Retrieval Conference
Publisher: ISMIR},
	file = {Torras et al_2021_On the Integration of Language Models into Sequence to Sequence Architectures.pdf:/home/ptorras/zotpapers/Torras et al_2021_On the Integration of Language Models into Sequence to Sequence Architectures.pdf:application/pdf},
}

@article{wen_sequence--sequence_2022,
	title = {A {Sequence}-to-{Sequence} {Framework} {Based} on {Transformer} {With} {Masked} {Language} {Model} for {Optical} {Music} {Recognition}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3220878},
	abstract = {Optical music recognition technology is of great significance in the development of the digital music. In recent years, the convolutional recurrent neural network framework with connectionist temporal classification has been used in music recognition. However, its loss function is calculated in serial mode, which leads to low efficiency in training and difficulty in convergence. Additionally, because of the gradient disappearance of excessive long music sequences, the existing music recognition models are hard to learn the relationships between musical symbols, resulting in high sequence error rate. Therefore, we propose a sequence-to-sequence framework based on transformer with masked language model to deal with these problems. The context representation between musical symbols can be captured further by the self-attention module in the transformer, which will reduce the sequence error rate. In addition, we refer to the masked language model and design a mask matrix to predict each musical symbol in a parallel way, so as to speed up the training process. Our experiments are carried out on the printed images of music stave dataset, and the results show that our proposed method is training-efficient and has great improvement in sequence accuracy rate.},
	journal = {IEEE Access},
	author = {Wen, Cuihong and Zhu, Longjiao},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Transformers, Neural networks, Hidden Markov models, Feature extraction, Music, Decoding, optical music recognition, Masked language model, neural networks, sequence-to-sequence, Sequential analysis, Symbols, transformer},
	pages = {118243--118252},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/ISPEQGBH/9943538.html:text/html;Wen_Zhu_2022_A Sequence-to-Sequence Framework Based on Transformer With Masked Language.pdf:/home/ptorras/zotpapers/Wen_Zhu_2022_A Sequence-to-Sequence Framework Based on Transformer With Masked Language.pdf:application/pdf},
}

@article{barbosa_convolutional_2022,
	title = {Convolutional {Neural} {Networks} and {Ensemble} {Methods} to {Identify} {Musical} {Elements} in {Optical} {Music} {Recognition}},
	volume = {20},
	copyright = {Copyright (c) 2022},
	issn = {1519-8219},
	url = {https://sol.sbc.org.br/journals/index.php/reic/article/view/2761},
	abstract = {Optical Music Recognition (OMR) is an important tool to recognize a scanned page of music sheet automatically, which has been applied to preserving music scores. In this paper, we present a comparative study among a Convolutional Neural Network (CNN) architecture, named CREATES, and Ensemble Learning methods, such as Random Forest and XGBoost, to classify musical symbols. The initial results show that CREATES is promising in this task and it outperforms ensemble methods on the HOMUS dataset. However, CNN require more computing power.},
	language = {en},
	number = {4},
	urldate = {2023-01-17},
	journal = {Revista Eletrônica de Iniciação Científica em Computação},
	author = {Barbosa, Jenaro Augusto and Santos, Edimilson Batista dos},
	month = dec,
	year = {2022},
	note = {Number: 4},
	keywords = {Ensemble Learning Methods},
	file = {Barbosa_Santos_2022_Convolutional Neural Networks and Ensemble Methods to Identify Musical Elements.pdf:/home/ptorras/zotpapers/Barbosa_Santos_2022_Convolutional Neural Networks and Ensemble Methods to Identify Musical Elements.pdf:application/pdf},
}

@article{zhang_detector_2023,
	title = {A detector for page-level handwritten music object recognition based on deep learning},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-023-08216-6},
	doi = {10.1007/s00521-023-08216-6},
	abstract = {Handwritten music recognition (HMR) is the technology of transcribing the content of images of music scores. The accurate detection of music objects at the page level is one of the main challenges of HMR. Thus far, the existing methods suffer from the tiny and dense nature of handwritten music notations and realize positive detection accuracy only on snippets. In this paper, we propose a detector that consists of a staff line removal model and a handwritten music object detection model for page-level handwritten music object recognition. First, an end-to-end staff line removal model R\_Staff\_Net based on residual learning reduces the complexity of page-level detection. Second, we developed an improved YOLO-V4 model for handwritten music object detection. The improvements mainly concern the adoption of a de-coupled detection head and visual attention module in the YOLO-V4, and an adaptive multi-scale feature fusion module AMFFM is used to enhance the textures and features of tiny music symbols in the deep convolution layers, and the gradient harmonized mechanism is utilized to address the inherent imbalance between music objects. We verified the R\_Staff\_Net and the improved YOLO-V4 model on the ICDAR/GREC staff line removal dataset and the MUSCIMA++ dataset, respectively. The experiments highlight that R\_Staff\_Net presents outstanding performance with an F-M score of 98.64\%, and our improved YOLO-V4 model is superior to other handwritten music symbol detection methods with a mean average precision (mAP) of 91.8\% when addressing page-level input. Although the experimental results of the detector show that the R\_Staff\_Net helps little to the overall mAP, the network is beneficial for symbols that are similar to staff lines or heavily overlap staff lines.},
	language = {en},
	urldate = {2023-01-27},
	journal = {Neural Comput \& Applic},
	author = {Zhang, Yusen and Huang, Zhiqing and Zhang, Yanxin and Ren, Keyan},
	month = jan,
	year = {2023},
	file = {Zhang et al_2023_A detector for page-level handwritten music object recognition based on deep.pdf:/home/ptorras/zotpapers/Zhang et al_2023_A detector for page-level handwritten music object recognition based on deep.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_end--end_2017,
	address = {Suzhou, China},
	title = {End-to-{End} {Optical} {Music} {Recognition} {Using} {Neural} {Networks}.},
	url = {https://zenodo.org/record/1418333},
	abstract = {[TODO] Add abstract here.},
	urldate = {2023-01-27},
	author = {Calvo-Zaragoza, Jorge and Valero-Mas, Jose J. and Pertusa, Antonio},
	month = oct,
	year = {2017},
	doi = {10.5281/zenodo.1418333},
	note = {ISBN: 9789811151798
Pages: 472-477
Publisher: ISMIR
Publication Title: Proceedings of the 18th International Society for Music Information Retrieval Conference},
	file = {Calvo-Zaragoza et al_2017_End-to-End Optical Music Recognition Using Neural Networks.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2017_End-to-End Optical Music Recognition Using Neural Networks.pdf:application/pdf},
}

@inproceedings{pacha_learning_2019,
	address = {Delft, The Netherlands},
	title = {Learning {Notation} {Graph} {Construction} for {Full}-{Pipeline} {Optical} {Music} {Recognition}},
	url = {https://zenodo.org/record/3527744},
	abstract = {Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2\% precision, 96.0\% recall, and an F-score of 95.6\% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2\% precision, 91.5\% recall and an F-score of 92.3\%.},
	urldate = {2023-01-27},
	author = {Pacha, Alexander and Calvo-Zaragoza, Jorge and Jan Hajič, jr.},
	month = nov,
	year = {2019},
	doi = {10.5281/zenodo.3527744},
	note = {Pages: 75-82
Publisher: ISMIR
Publication Title: Proceedings of the 20th International Society for Music Information Retrieval Conference},
	file = {Pacha et al_2019_Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition.pdf:/home/ptorras/zotpapers/Pacha et al_2019_Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition.pdf:application/pdf},
}

@article{selfridge-field_optical_1994,
	title = {Optical recognition of musical notation : {A} survey of current work},
	volume = {9},
	shorttitle = {Optical recognition of musical notation},
	url = {https://cir.nii.ac.jp/crid/1570009749907332736},
	urldate = {2023-01-27},
	journal = {Computing in Musicology},
	author = {Selfridge-Field},
	year = {1994},
	note = {Publisher: CCARH},
	pages = {109--145},
	file = {Snapshot:/home/ptorras/Zotero/storage/REMQ9AFC/1570009749907332736.html:text/html},
}

@misc{hajic_jr_detecting_2017,
	title = {Detecting {Noteheads} in {Handwritten} {Scores} with {ConvNets} and {Bounding} {Box} {Regression}},
	url = {http://arxiv.org/abs/1708.01806},
	doi = {10.48550/arXiv.1708.01806},
	abstract = {Noteheads are the interface between the written score and music. Each notehead on the page signifies one note to be played, and detecting noteheads is thus an unavoidable step for Optical Music Recognition. Noteheads are clearly distinct objects, however, the variety of music notation handwriting makes noteheads harder to identify, and while handwritten music notation symbol \{{\textbackslash}em classification\} is a well-studied task, symbol \{{\textbackslash}em detection\} has usually been limited to heuristics and rule-based systems instead of machine learning methods better suited to deal with the uncertainties in handwriting. We present ongoing work on a simple notehead detector using convolutional neural networks for pixel classification and bounding box regression that achieves a detection f-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not require staff removal, and is applicable to a variety of handwriting styles and levels of musical complexity.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Hajič jr., Jan and Pecina, Pavel},
	month = aug,
	year = {2017},
	note = {arXiv:1708.01806 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.7.5, I.5.4, I.4.6, I.5.1},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/4JA8GUCF/Hajič Jr. and Pecina - 2017 - Detecting Noteheads in Handwritten Scores with Con.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/UBL8CL2H/1708.html:text/html;Hajic Jr._Pecina_2017_Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box.pdf:/home/ptorras/zotpapers/Hajic Jr._Pecina_2017_Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box.pdf:application/pdf},
}

@inproceedings{dorfer_potential_2017,
	title = {On the {Potential} of {Fully} {Convolutional} {Neural} {Networks} for {Musical} {Symbol} {Detection}},
	volume = {02},
	doi = {10.1109/ICDAR.2017.274},
	abstract = {Musical symbol detection on the page is an outstanding Optical Music Recognition (OMR) subproblem. We propose using a fully convolutional segmentation network to produce high-quality pixel-wise symbol probability masks. Experiments on notehead detection show a very promising detection f-score of 0.98 with elementary detection methods.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Dorfer, Matthias and Hajič, Jan and Widmer, Gerhard},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical imaging, Optical Music Recognition, Image segmentation, Music, Biomedical optical imaging, Computer architecture, Convolutional Neural Networks, Detection, Integrated optics, Noteheads, OMR, Optical computing, Segmentation},
	pages = {53--54},
	file = {Dorfer et al_2017_On the Potential of Fully Convolutional Neural Networks for Musical Symbol.pdf:/home/ptorras/zotpapers/Dorfer et al_2017_On the Potential of Fully Convolutional Neural Networks for Musical Symbol.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/PE9N7Q5D/8270216.html:text/html},
}

@article{rossant_robust_2006,
	title = {Robust and {Adaptive} {OMR} {System} {Including} {Fuzzy} {Modeling}, {Fusion} of {Musical} {Rules}, and {Possible} {Error} {Detection}},
	volume = {2007},
	copyright = {2007 F. Rossant and I. Bloch.},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/81541},
	doi = {10.1155/2007/81541},
	abstract = {This paper describes a system for optical music recognition (OMR) in case of monophonic typeset scores. After clarifying the difficulties specific to this domain, we propose appropriate solutions at both image analysis level and high-level interpretation. Thus, a recognition and segmentation method is designed, that allows dealing with common printing defects and numerous symbol interconnections. Then, musical rules are modeled and integrated, in order to make a consistent decision. This high-level interpretation step relies on the fuzzy sets and possibility framework, since it allows dealing with symbol variability, flexibility, and imprecision of music rules, and merging all these heterogeneous pieces of information. Other innovative features are the indication of potential errors and the possibility of applying learning procedures, in order to gain in robustness. Experiments conducted on a large data base show that the proposed method constitutes an interesting contribution to OMR.},
	language = {en},
	number = {1},
	urldate = {2023-01-27},
	journal = {EURASIP J. Adv. Signal Process.},
	author = {Rossant, Florence and Bloch, Isabelle},
	month = dec,
	year = {2006},
	note = {Number: 1
Publisher: SpringerOpen},
	pages = {1--25},
	file = {Rossant_Bloch_2006_Robust and Adaptive OMR System Including Fuzzy Modeling, Fusion of Musical.pdf:/home/ptorras/zotpapers/Rossant_Bloch_2006_Robust and Adaptive OMR System Including Fuzzy Modeling, Fusion of Musical.pdf:application/pdf},
}

@inproceedings{tuggener_deep_2018,
	address = {Paris, France},
	title = {Deep {Watershed} {Detector} for {Music} {Object} {Recognition}},
	url = {https://zenodo.org/record/1492401},
	abstract = {Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as with handwritten music.},
	urldate = {2023-01-27},
	author = {Tuggener, Lukas and Elezi, Ismail and Schmidhuber, Jürgen and Stadelmann, Thilo},
	month = sep,
	year = {2018},
	doi = {10.5281/zenodo.1492401},
	note = {Pages: 271-278
Publisher: ISMIR
Publication Title: Proceedings of the 19th International Society for Music Information Retrieval Conference},
	file = {Tuggener et al_2018_Deep Watershed Detector for Music Object Recognition.pdf:/home/ptorras/zotpapers/Tuggener et al_2018_Deep Watershed Detector for Music Object Recognition.pdf:application/pdf},
}

@inproceedings{alfaro-contreras_omr-assisted_2019,
	address = {Online},
	title = {{OMR}-assisted transcription: a case study with early prints},
	shorttitle = {{OMR}-assisted transcription},
	url = {https://zenodo.org/record/5574607},
	abstract = {Most of the musical heritage is only available as physical documents, given that the engraving process was carried out by handwriting or typesetting until the end of the 20th century. Their mere availability as scanned images does not enable tasks such as indexing or editing unless they are transcribed into a structured digital format. Given the cost and time required for manual transcription, Optical Music Recognition (OMR) presents itself as a promising alternative. Quite often, OMR systems show acceptable but not perfect performance, which eventually leaves them out of the transcription process. On the assumption that OMR systems might always make some errors, it is essential that the user corrects the output. This paper contributes to a better understanding of how music transcription is improved by the assistance of OMR systems that include the end-user in the recognition process. For that, we have measured the transcription time of a printed early music work under two scenarios: a manual one and a state-of-the-art OMR-assisted one, with several alternatives each. Our results demonstrate that using OMR remarkably reduces users' effort, even when its performance is far optimal, compared to the fully manual option.},
	urldate = {2023-01-30},
	author = {Alfaro-Contreras, María and Rizo, David and Inesta, Jose M. and Calvo-Zaragoza, Jorge},
	month = nov,
	year = {2019},
	doi = {10.5281/zenodo.5574607},
	note = {Pages: 35-41
Publication Title: Proceedings of the 22nd International Society for Music Information Retrieval Conference
Publisher: ISMIR},
	file = {Alfaro-Contreras et al_2019_OMR-assisted transcription.pdf:/home/ptorras/zotpapers/Alfaro-Contreras et al_2019_OMR-assisted transcription.pdf:application/pdf},
}

@inproceedings{shah_sangctc-improving_2019,
	title = {{SangCTC}-{Improving} {Traditional} {CTC} {Loss} {Function} for {Optical} {Music} {Recognition} ({August} 2019)},
	doi = {10.1109/PuneCon46936.2019.9105720},
	abstract = {Optical Music Recognition (OMR) is a branch of AI analogous to Optical Character Recognition (OCR) in which we train the machine to interpret sheet music to produce a playableor editable form of Music. To solve this problem in an End-to-End manner, Convolutional Recurrent Neural Network (CRNN) architecture is used. It considers both spatial and sequential nature of this problem. CTC loss function is proved to be a favorable choice in these types of sequence problems as it trains the models directly from input images to their corresponding musical transcripts without the need for a frame-by-frame alignment between the image and the ground-truth thereby solving the purpose of End-to-End training. Though traditional CTC seems to solve a major chunk of the problem, it suffers from some limitations due to overfitting/underfitting. It tends overfit/underfit because of uneven frequency distribution of symbols in Datasets and also makes overconfident predictions leading to bad generalization of model. No attempt has been made to overcome the aforementioned limitations collectively. Hence in this paper we propose a method and analyze the solution in the form of SangCTC. SangCTC is an enhanced variation of traditional CTC which attempts to overcome these limitations of overfitting/underfitting simultaneously using the concepts of focal theory and entropy.},
	booktitle = {2019 {IEEE} {Pune} {Section} {International} {Conference} ({PuneCon})},
	author = {Shah, Jimit K. and Padte, Anuya S. and Ahirao, Purnima N.},
	month = dec,
	year = {2019},
	keywords = {Deep Learning, Optical Music Recognition, Connectionist Temporal Classification, End-to-End Training, Overfitting},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/MEWZQTVD/9105720.html:text/html;Shah et al. - 2019 - SangCTC-Improving Traditional CTC Loss Function fo.pdf:/home/ptorras/Downloads/Shah et al. - 2019 - SangCTC-Improving Traditional CTC Loss Function fo.pdf:application/pdf},
}

@inproceedings{baro_musigraph_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Musigraph: {Optical} {Music} {Recognition} {Through} {Object} {Detection} and {Graph} {Neural} {Network}},
	isbn = {978-3-031-21648-0},
	shorttitle = {Musigraph},
	doi = {10.1007/978-3-031-21648-0\_12},
	abstract = {During the last decades, the performance of optical music recognition has been increasingly improving. However, and despite the 2-dimensional nature of music notation (e.g. notes have rhythm and pitch), most works treat musical scores as a sequence of symbols in one dimension, which make their recognition still a challenge. Thus, in this work we explore the use of graph neural networks for musical score recognition. First, because graphs are suited for n-dimensional representations, and second, because the combination of graphs with deep learning has shown a great performance in similar applications. Our methodology consists of: First, we will detect each isolated/atomic symbols (those that can not be decomposed in more graphical primitives) and the primitives that form a musical symbol. Then, we will build the graph taking as root node the notehead and as leaves those primitives or symbols that modify the note’s rhythm (stem, beam, flag) or pitch (flat, sharp, natural). Finally, the graph is translated into a human-readable character sequence for a final transcription and evaluation. Our method has been tested on more than five thousand measures, showing promising results.},
	language = {en},
	booktitle = {Frontiers in {Handwriting} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Baró, Arnau and Riba, Pau and Fornés, Alicia},
	editor = {Porwal, Utkarsh and Fornés, Alicia and Shafait, Faisal},
	year = {2022},
	keywords = {Optical music recognition, Object detection, Graph neural network},
	pages = {171--184},
	file = {Baro et al_2022_Musigraph.pdf:/home/ptorras/zotpapers/Baro et al_2022_Musigraph.pdf:application/pdf},
}

@article{bainbridge_challenge_2001,
	title = {The {Challenge} of {Optical} {Music} {Recognition}},
	volume = {35},
	issn = {0010-4817},
	url = {https://www.jstor.org/stable/30204846},
	abstract = {This article describes the challenges posed by optical music recognition - a topic in computer science that aims to convert scanned pages of music into an on-line format. First, the problem is described; then a generalised framework for software is presented that emphasises key stages that must be solved: staff line identification, musical object location, musical feature classification, and musical semantics. Next, significant research projects in the area are reviewed, showing how each fits the generalised framework. The article concludes by discussing perhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps that help ease the task.},
	number = {2},
	urldate = {2023-02-15},
	journal = {Computers and the Humanities},
	author = {Bainbridge, David and Bell, Tim},
	year = {2001},
	note = {Publisher: Springer},
	keywords = {optical music recognition, document image analysis, to classify, musical data acquisition, pattern},
	pages = {95--121},
	file = {Bainbridge and Bell - The Challenge of Optical Music Recognition.pdf:/home/ptorras/Zotero/storage/Z76ZHKHI/Bainbridge and Bell - The Challenge of Optical Music Recognition.pdf:application/pdf},
}

@article{byrd_music_2003,
	title = {A {Music} {Representation} {Requirement} {Specification} for {Academia}},
	volume = {27},
	issn = {1531-5169},
	url = {https://muse.jhu.edu/pub/6/article/49604},
	number = {4},
	urldate = {2023-02-16},
	journal = {Computer Music Journal},
	author = {Byrd, Donald Alvin and Isaacson, Eric J.},
	year = {2003},
	note = {Publisher: The MIT Press},
	pages = {43--57},
	file = {Byrd and Isaacson - 2003 - A Music Representation Requirement Specification f.pdf:/home/ptorras/Downloads/Byrd and Isaacson - 2003 - A Music Representation Requirement Specification f.pdf:application/pdf},
}

@inproceedings{agarwal_meteor_2008,
	address = {Stroudsburg, PA, USA},
	title = {{METEOR}, {M}-{BLEU} and {M}-{TER}: {Evaluation} {Metrics} for {High}-correlation with {Human} {Rankings} of {Machine} {Translation} {Output}},
	isbn = {978-1-932432-09-1},
	url = {http://dl.acm.org/citation.cfm?id=1626394.1626406},
	booktitle = {Third {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Agarwal, Abhaya and Lavie, Alon},
	year = {2008},
	note = {event-place: Columbus, Ohio},
	pages = {115--118},
}

@article{akiyama_automated_1990,
	title = {Automated entry system for printed documents},
	volume = {23},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/003132039090112X},
	doi = {10.1016/0031-3203(90)90112-X},
	abstract = {This paper proposes a system for automatically reading either Japanese or English documents that have complex layout structures that include graphics. First, document image segmentation and character segmentation are carried out using three basic features and the knowledge of document layout rules. Next, multi-font character recognition is performed based on feature vector matching. Recognition experiments with a prototype system for a variety of complex printed documents shows that the proposed system is capable of reading different types of printed documents at an accuracy rate of 94.8–97.2\%.},
	number = {11},
	journal = {Pattern Recognition},
	author = {Akiyama, Teruo and Hagita, Norihiro},
	year = {1990},
	keywords = {Feature extraction, Image processing, Character recognition, Character segmentation, Document entry system, Document processing, Layout structure recognition},
	pages = {1141--1154},
}

@article{alvaro_integrated_2016,
	title = {An integrated grammar-based approach for mathematical expression recognition},
	volume = {51},
	doi = {10.1016/j.patcog.2015.09.013},
	journal = {Pattern Recognition},
	author = {Álvaro, Francisco and Sánchez, Joan-Andreu and Benedí, José-Miguel},
	year = {2016},
	pages = {135--147},
	file = {Alvaro et al_2016_An integrated grammar-based approach for mathematical expression recognition.pdf:/home/ptorras/zotpapers/Alvaro et al_2016_An integrated grammar-based approach for mathematical expression recognition.pdf:application/pdf},
}

@article{andre_parsimonious_2014,
	title = {A parsimonious oscillatory model of handwriting},
	volume = {108},
	doi = {10.1007/s00422-014-0600-z},
	number = {3},
	journal = {Biological Cybernetics},
	author = {André, Gaëtan and Kostrubiec, Viviane and Buisson, Jean-Christophe and Albaret, Jean-Michel and Zanone, Pier-Giorgio},
	year = {2014},
	note = {Publisher: Springer Science Business Media},
	pages = {321--336},
	file = {Andre et al_2014_A parsimonious oscillatory model of handwriting.pdf:/home/ptorras/zotpapers/Andre et al_2014_A parsimonious oscillatory model of handwriting.pdf:application/pdf},
}

@article{ann_efficient_2010,
	title = {Efficient algorithms for the block edit problems},
	volume = {208},
	doi = {10.1016/j.ic.2009.12.001},
	number = {3},
	journal = {Information and Computation},
	author = {Ann, Hsing-Yen and Yang, Chang-Biau and Peng, Yung-Hsing and Liaw, Bern-Cherng},
	year = {2010},
	note = {Publisher: Elsevier BV},
	pages = {221--229},
	file = {Ann et al_2010_Efficient algorithms for the block edit problems.pdf:/home/ptorras/zotpapers/Ann et al_2010_Efficient algorithms for the block edit problems.pdf:application/pdf},
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	volume = {abs/1409.0473},
	url = {http://arxiv.org/abs/1409.0473},
	journal = {Computing Research Repository},
	author = {Bahdanau, Dzmitry and Hyun, Cho Kyung and Bengio, Yoshua},
	year = {2014},
	file = {Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:/home/ptorras/zotpapers/Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf},
}

@inproceedings{balke_learning_2019,
	title = {Learning {Soft}-{Attention} {Models} for {Tempo}-{Invariant} {Audio}-{Sheet} {Music} {Retrieval}},
	url = {http://archives.ismir.net/ismir2019/paper/000024.pdf},
	booktitle = {20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Balke, Stefan and Dorfer, Matthias and Carvalho, Luis and Arzt, Andreas and Widmer, Gerhard},
	year = {2019},
	pages = {75--82},
	file = {Balke et al_2019_Learning Soft-Attention Models for Tempo-Invariant Audio-Sheet Music Retrieval.pdf:/home/ptorras/zotpapers/Balke et al_2019_Learning Soft-Attention Models for Tempo-Invariant Audio-Sheet Music Retrieval.pdf:application/pdf},
}

@article{barate_advanced_2018,
	title = {Advanced {Experience} of {Music} through {5G} {Technologies}},
	volume = {364},
	url = {http://stacks.iop.org/1757-899X/364/i=1/a=012021},
	abstract = {This paper focuses on new models to enjoy music that will be implementable in a near future thanks to 5G technology. In the last two decades, our research mainly focused on the comprehensive description of music information, where multiple aspects are integrated to provide the user with an advanced multi-layer environment to experience music content. In recent times, the advancements in network technologies allowed a web implementation of this approach through W3C-compliant languages. The last obstacle to the use of personal devices is currently posed by the characteristics of mobile networks, concerning bandwidth, reliability, and the density of devices in an area. Designed to meet the requirements of future technological challenges, such as the Internet of Things and self-driving vehicles, the advent of 5G networks will solve these problems, thus paving the way also for new music-oriented applications. The possibilities described in this work range from bringing archive materials and music cultural heritage to a new life to the implementation of immersive environments for live-show remote experience.},
	number = {1},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Baratè, Adriano and Haus, Goffredo and Ludovico, Luca A.},
	year = {2018},
	pages = {012021},
	file = {Barate et al_2018_Advanced Experience of Music through 5G Technologies.pdf:/home/ptorras/zotpapers/Barate et al_2018_Advanced Experience of Music through 5G Technologies.pdf:application/pdf},
}

@inproceedings{bay_evaluation_2009,
	address = {Kobe, Japan},
	title = {Evaluation of {Multiple}-{F0} {Estimation} and {Tracking} {Systems}},
	url = {http://ismir2009.ismir.net/proceedings/PS2-21.pdf},
	booktitle = {10th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Bay, Mert and Ehmann, Andreas F. and Downie, J. Stephen},
	year = {2009},
	pages = {315--320},
	file = {Bay et al_2009_Evaluation of Multiple-F0 Estimation and Tracking Systems.pdf:/home/ptorras/zotpapers/Bay et al_2009_Evaluation of Multiple-F0 Estimation and Tracking Systems.pdf:application/pdf},
}

@article{bellini_symbolic_2005,
	title = {Symbolic {Music} {Representation} in {MPEG}},
	volume = {12},
	doi = {10.1109/MMUL.2005.82},
	number = {4},
	journal = {IEEE MultiMedia},
	author = {Bellini, Pierfrancesco and Nesi, Paolo and Zoia, Giorgio},
	year = {2005},
	pages = {42--49},
	file = {Bellini et al_2005_Symbolic Music Representation in MPEG.pdf:/home/ptorras/zotpapers/Bellini et al_2005_Symbolic Music Representation in MPEG.pdf:application/pdf},
}

@article{benetos_automatic_2013,
	title = {Automatic music transcription: challenges and future directions},
	volume = {41},
	issn = {1573-7675},
	doi = {10.1007/s10844-013-0258-3},
	abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
	number = {3},
	journal = {Journal of Intelligent Information Systems},
	author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
	year = {2013},
	pages = {407--434},
	file = {Benetos et al_2013_Automatic music transcription.pdf:/home/ptorras/zotpapers/Benetos et al_2013_Automatic music transcription.pdf:application/pdf},
}

@inproceedings{bezine_generation_2004,
	title = {Generation and {Analysis} of {Handwriting} {Script} with the {Beta}-{Elliptic} {Model}},
	doi = {10.1109/iwfhr.2004.45},
	booktitle = {Ninth {International} {Workshop} on {Frontiers} in {Handwriting} {Recognition}},
	publisher = {Institute of Electrical \& Electronics Engineers (IEEE)},
	author = {Bezine, Hala and Alimi, Adel M. and Sherkat, Nasser},
	year = {2004},
}

@article{bezine_generation_2004-1,
	title = {Generation and {Analysis} of {Handwriting} {Script} {With} the {Beta}-{Elliptic} {Model}},
	volume = {8},
	issn = {1473-8031},
	url = {http://ijssst.info/Vol-08/No-2/paper6.pdf},
	number = {2},
	journal = {International Journal of Simulation},
	author = {Bezine, Hala and Alimi, Adel M. and Sherkat, Nasser},
	year = {2004},
	pages = {45--65},
	file = {Bezine et al_2004_Generation and Analysis of Handwriting Script With the Beta-Elliptic Model.pdf:/home/ptorras/zotpapers/Bezine et al_2004_Generation and Analysis of Handwriting Script With the Beta-Elliptic Model.pdf:application/pdf},
}

@inproceedings{bojar_grain_2011,
	address = {Edinburgh, Scotland},
	title = {A {Grain} of {Salt} for the {WMT} {Manual} {Evaluation}},
	isbn = {978-1-937284-12-1},
	url = {http://dl.acm.org/citation.cfm?id=2132960.2132962},
	booktitle = {Sixth {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Bojar, Ondřej and Ercegovčević, Miloš and Popel, Martin and Zaidan, Omar F.},
	year = {2011},
	pages = {1--11},
}

@inproceedings{bresler_using_2015,
	address = {Seggau, Austria},
	title = {Using {Agglomerative} {Clustering} of {Strokes} to {Perform} {Symbols} {Over}-segmentation within a {Diagram} {Recognition} {System}},
	isbn = {978-3-85125-388-7},
	url = {http://cmp.felk.cvut.cz/ftp/articles/bresler/Bresler-Prusa-Hlavac-CVWW-2015.pdf},
	booktitle = {20th {Computer} {Vision} {Winter} {Workshop}},
	publisher = {Graz University of Technology},
	author = {Bresler, Martin and Průša, Daniel and Hlaváč, V{\textbackslash}' aclav},
	editor = {Paul Wohlhart, Vincent Lepetit},
	year = {2015},
	keywords = {Artificial samples, Clustering, Diagram recognition, Finite automata, Flowcharts},
	pages = {67--74},
	file = {Bresler et al_2015_Using Agglomerative Clustering of Strokes to Perform Symbols Over-segmentation.pdf:/home/ptorras/zotpapers/Bresler et al_2015_Using Agglomerative Clustering of Strokes to Perform Symbols Over-segmentation.pdf:application/pdf},
}

@inproceedings{breuel_high-performance_2013,
	title = {High-{Performance} {OCR} for {Printed} {English} and {Fraktur} {Using} {LSTM} {Networks}},
	doi = {10.1109/ICDAR.2013.140},
	abstract = {Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6\% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64\%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15\% (Fontane) and 1.47\% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques.},
	booktitle = {2013 12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Breuel, Thomas M. and Ul-Hasan, Adnan and Al Azawi, Mayce and Shafait, Faisal},
	year = {2013},
	note = {ISSN: 1520-5363},
	keywords = {Hidden Markov models, Training, Optical character recognition software, Handwriting recognition, Error analysis, handwriting recognition, Recurrent neural networks, book digitization project, English text, Fraktur, Fraktur text, handwritten characters, high-performance OCR, long short term memory networks, LSTM networks, LSTM Networks, machine printed Fraktur recognition, machine printed Latin recognition, natural language processing, OCR, optical character recognition, printed English, printed OCR, recurrent networks, RNN, scanned pages, statistical analysis, statistical properties, text analysis, UW3 database},
	pages = {683--687},
}

@techreport{byrd_music_2006,
	title = {Music {Notation} and {Music} {Representation}},
	url = {http://music.informatics.indiana.edu/don_notation.html},
	institution = {Indiana University, School of informatics},
	author = {Byrd, Donald},
	year = {2006},
}

@inproceedings{callison-burch_meta-_2007,
	address = {Stroudsburg, PA, USA},
	title = {({Meta}-) {Evaluation} of {Machine} {Translation}},
	url = {http://dl.acm.org/citation.cfm?id=1626355.1626373},
	booktitle = {Second {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Callison-Burch, Chris and Fordyce, Cameron and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
	year = {2007},
	note = {event-place: Prague, Czech Republic},
	pages = {136--158},
}

@inproceedings{callison-burch_further_2008,
	address = {Stroudsburg, PA, USA},
	title = {Further {Meta}-evaluation of {Machine} {Translation}},
	isbn = {978-1-932432-09-1},
	url = {http://dl.acm.org/citation.cfm?id=1626394.1626403},
	booktitle = {Third {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Callison-Burch, Chris and Fordyce, Cameron and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
	year = {2008},
	note = {event-place: Columbus, Ohio},
	pages = {70--106},
}

@inproceedings{callison-burch_findings_2010,
	address = {Stroudsburg, PA, USA},
	title = {Findings of the 2010 {Joint} {Workshop} on {Statistical} {Machine} {Translation} and {Metrics} for {Machine} {Translation}},
	isbn = {978-1-932432-71-8},
	url = {http://dl.acm.org/citation.cfm?id=1868850.1868853},
	booktitle = {Joint {Fifth} {Workshop} on {Statistical} {Machine} {Translation} and {MetricsMATR}},
	publisher = {Association for Computational Linguistics},
	author = {Callison-Burch, Chris and Koehn, Philipp and Monz, Christof and Peterson, Kay and Przybocki, Mark and Zaidan, Omar F.},
	year = {2010},
	note = {event-place: Uppsala, Sweden},
	pages = {17--53},
}

@article{calvo-zaragoza_prototype_2016,
	title = {Prototype generation on structural data using dissimilarity space representation},
	issn = {1433-3058},
	doi = {10.1007/s00521-016-2278-8},
	abstract = {Data reduction techniques play a key role in instance-based classification to lower the amount of data to be processed. Among the different existing approaches, prototype selection (PS) and prototype generation (PG) are the most representative ones. These two families differ in the way the reduced set is obtained from the initial one: While the former aims at selecting the most representative elements from the set, the latter creates new data out of it. Although PG is considered to delimit more efficiently decision boundaries, the operations required are not so well defined in scenarios involving structural data such as strings, trees, or graphs. This work studies the possibility of using dissimilarity space (DS) methods as an intermediate process for mapping the initial structural representation to a statistical one, thereby allowing the use of PG methods. A comparative experiment over string data is carried out in which our proposal is faced to PS methods on the original space. Results show that the proposed strategy is able to achieve significantly similar results to PS in the initial space, thus standing as a clear alternative to the classic approach, with some additional advantages derived from the DS representation.},
	journal = {Neural Computing and Applications},
	author = {Calvo-Zaragoza, Jorge and Valero-Mas, Jose J. and Rico-Juan, Juan R.},
	year = {2016},
	pages = {1--10},
	file = {Calvo-Zaragoza et al_2016_Prototype generation on structural data using dissimilarity space representation.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2016_Prototype generation on structural data using dissimilarity space representation.pdf:application/pdf},
}

@article{calvo-zaragoza_efficient_2016,
	title = {An efficient approach for {Interactive} {Sequential} {Pattern} {Recognition}},
	volume = {64},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320316303569},
	doi = {10.1016/j.patcog.2016.11.006},
	abstract = {Abstract Interactive Pattern Recognition (IPR) is an emergent framework in which the user is involved actively in the recognition process by giving feedback to the system when an error is detected. Although this framework is expected to reduce the number of errors to correct, it may increase the time required to complete the task since the machine needs to recompute its proposal after each interaction. Therefore, a fast computation is required to make the interactive system profitable and user-friendly. This work presents an efficient approach to deal with IPR tasks when data has a sequential nature. Our approach includes some computation at the very beginning of the task but it then achieves a linear complexity after user corrections. We also show how these tasks can be effectively carried out if the solution space is defined with a Regular Language. This fact has indeed proven to be the most relevant factor to improve the efficiency of the approach. Several experiments are carried out in which our proposal is faced against a classical search. Results show a reduction in time in all experiments considered, solving efficiently some complex IPR tasks thanks to our proposals.},
	journal = {Pattern Recognition},
	author = {Calvo-Zaragoza, Jorge and Oncina, Jose},
	year = {2016},
	note = {Publisher: Elsevier},
	keywords = {Interactive Pattern Recognition},
	pages = {295--304},
	file = {Calvo-Zaragoza_Oncina_2016_An efficient approach for Interactive Sequential Pattern Recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza_Oncina_2016_An efficient approach for Interactive Sequential Pattern Recognition.pdf:application/pdf},
}

@article{cancino-chacon_computational_2018,
	title = {Computational {Models} of {Expressive} {Music} {Performance}: {A} {Comprehensive} and {Critical} {Review}},
	volume = {5},
	issn = {2297-2668},
	doi = {10.3389/fdigh.2018.00025},
	abstract = {Expressive performance is an indispensable part of music making. When playing a piece, expert performers shape various parameters (tempo, timing, dynamics, intonation, articulation, etc.) in ways that are not prescribed by the notated score, in this way producing an expressive rendition that brings out dramatic, affective, and emotional qualities that may engage and affect the listeners. Given the central importance of this skill for many kinds of music, expressive performance has become an important research topic for disciplines like musicology, music psychology, etc. This paper focuses on a specific thread of research: work on computational music performance models. Computational models are attempts at codifying hypotheses about expressive performance in terms of mathematical formulas or computer programs, so that they can be evaluated in systematic and quantitative ways. Such models can serve at least two main purposes: they permit us to systematically study certain hypotheses regarding performance; and they can be used as tools to generate automated or semi-automated performances, in artistic or educational contexts. The present article presents an up-to-date overview of the state of the art in this domain. We explore recent trends in the field, such as a strong focus on data-driven (machine learning); a growing interest in interactive expressive systems, such as conductor simulators and automatic accompaniment systems; and an increased interest in exploring cognitively plausible features and models. We provide an in-depth discussion of several important design choices in such computer models, and discuss a crucial (and still largely unsolved) problem that is hindering systematic progress: the question of how to evaluate such models in scientifically and musically meaningful ways. From all this, we finally derive some research directions that should be pursued with priority, in order to advance the field and our understanding of expressive music performance.},
	journal = {Frontiers in Digital Humanities},
	author = {Cancino-Chacón, Carlos E. and Grachten, Maarten and Goebl, Werner and Widmer, Gerhard},
	year = {2018},
	pages = {25},
	file = {Cancino-Chacon et al_2018_Computational Models of Expressive Music Performance.pdf:/home/ptorras/zotpapers/Cancino-Chacon et al_2018_Computational Models of Expressive Music Performance.pdf:application/pdf},
}

@article{chen_masklab_2017,
	title = {{MaskLab}: {Instance} {Segmentation} by {Refining} {Object} {Detection} with {Semantic} and {Direction} {Features}},
	volume = {abs/1712.04837},
	url = {http://arxiv.org/abs/1712.04837},
	journal = {CoRR},
	author = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
	year = {2017},
	file = {Chen et al_2017_MaskLab.pdf:/home/ptorras/zotpapers/Chen et al_2017_MaskLab.pdf:application/pdf},
}

@inproceedings{chiu_state---art_2018,
	title = {State-of-the-{Art} {Speech} {Recognition} with {Sequence}-to-{Sequence} {Models}},
	doi = {10.1109/ICASSP.2018.8462105},
	abstract = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-the-art ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2\% to 5.6\%, while the best conventional system achieves 6.7\%; on a dictation task our model achieves a WER of 4.1\% compared to 5\% for the conventional system.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Neural networks, Task analysis, Hidden Markov models, Training, Decoding, Acoustics, neural network, acoustic pronunciation and language model, ASR systems, automatic speech recognition system, decoding, dictation task, dictation tasks, encoder-decoder architectures, language model components, LAS model, listen attend and spell model, minimum word error rate optimization, multihead attention architecture, Optimization, recurrent neural nets, sequence-to-sequence models, single-head attention, speech recognition, streaming recognition, unidirectional LSTM encoder, voice search task, word piece models},
	pages = {4774--4778},
	file = {Chiu et al_2018_State-of-the-Art Speech Recognition with Sequence-to-Sequence Models.pdf:/home/ptorras/zotpapers/Chiu et al_2018_State-of-the-Art Speech Recognition with Sequence-to-Sequence Models.pdf:application/pdf},
}

@misc{chollet_keras_2017,
	title = {Keras},
	url = {https://github.com/keras-team/keras},
	publisher = {GitHub},
	author = {Chollet, François},
	year = {2017},
	note = {Publication Title: GitHub repository},
}

@inproceedings{chowdhury_efficient_2018,
	title = {An {Efficient} {End}-to-{End} {Neural} {Model} for {Handwritten} {Text} {Recognition}},
	url = {http://bmvc2018.org/contents/papers/0606.pdf},
	booktitle = {29th {British} {Machine} {Vision} {Conference}},
	author = {Chowdhury, Arindam and Vig, Lovekesh},
	year = {2018},
}

@inproceedings{clausner_aletheia_2011,
	address = {Beijing, China},
	title = {Aletheia - {An} {Advanced} {Document} {Layout} and {Text} {Ground}-{Truthing} {System} for {Production} {Environments}},
	url = {http://www.prima.cse.salford.ac.uk:8080/www/assets/papers/ICDAR2011_Clausner_Aletheia.pdf},
	doi = {10.1109/ICDAR.2011.19},
	booktitle = {2011 {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR}},
	publisher = {IEEE Computer Society},
	author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos},
	year = {2011},
	pages = {48--52},
}

@inproceedings{clausner_efficient_2014,
	title = {Efficient {OCR} {Training} {Data} {Generation} with {Aletheia}},
	url = {http://www.primaresearch.org/www/assets/papers/DAS2014_Clausner_OCRTrainingDataGeneration.pdf},
	booktitle = {Short {Paper} {Booklet} of the 11th {International} {Association} for {Pattern} {Recognition} ({IAPR}) {Workshop} on {Document} {Analysis} {Systems} ({DAS})},
	author = {Clausner, Christian and Pletschacher, Stefan and Antonacopoulos, Apostolos},
	year = {2014},
	file = {Clausner et al_2014_Efficient OCR Training Data Generation with Aletheia.pdf:/home/ptorras/zotpapers/Clausner et al_2014_Efficient OCR Training Data Generation with Aletheia.pdf:application/pdf},
}

@inproceedings{cont_evaluation_2007,
	address = {Vienna, Austria},
	title = {Evaluation of {Real}-{Time} {Audio}-to-{Score} {Alignment}},
	url = {https://hal.inria.fr/hal-00839068},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Cont, Arshia and Schwarz, Diemo and Schnell, Norbert and Raphael, Christopher},
	year = {2007},
	file = {Cont et al_2007_Evaluation of Real-Time Audio-to-Score Alignment.pdf:/home/ptorras/zotpapers/Cont et al_2007_Evaluation of Real-Time Audio-to-Score Alignment.pdf:application/pdf},
}

@article{cont_coupled_2010,
	title = {A {Coupled} {Duration}-{Focused} {Architecture} for {Real}-{Time} {Music}-to-{Score} {Alignment}},
	volume = {32},
	doi = {10.1109/TPAMI.2009.106},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cont, Arshia},
	year = {2010},
	pages = {974--987},
	file = {Cont_2010_A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment.pdf:/home/ptorras/zotpapers/Cont_2010_A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment.pdf:application/pdf},
}

@inproceedings{cordella_symbol_2000,
	address = {Berlin, Heidelberg},
	title = {Symbol and {Shape} {Recognition}},
	isbn = {978-3-540-40953-3},
	doi = {10.1007/3-540-40953-X_14},
	abstract = {The different aspects of a process for recognizing symbols in documents are considered and the techniques that have been most commonly used during the last ten years, in the different application fields, are reviewed. Methods used in the representation, description and classification phases are shortly discussed and the main recognition strategies are mentioned. Some of the problems that appear still open are proposed to the attention of the reader.},
	booktitle = {Graphics {Recognition} {Recent} {Advances}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cordella, L. P. and Vento, M.},
	editor = {Chhabra, Atul K. and Dori, Dov},
	year = {2000},
	pages = {167--182},
}

@misc{ornes_choral_1998,
	title = {Choral {Public} {Domain} {Library}},
	url = {http://cpdl.org},
	author = {Ornes, Rafael},
	year = {1998},
}

@inproceedings{cutter_towards_2015,
	title = {Towards {Mobile} {OCR}},
	doi = {10.1145/2682571.2797066},
	booktitle = {2015 {ACM} {Symposium} on {Document} {Engineering} - {DocEng}'15},
	publisher = {ACM Press},
	author = {Cutter, Michael P. and Manduchi, Roberto},
	year = {2015},
	file = {Cutter_Manduchi_2015_Towards Mobile OCR.pdf:/home/ptorras/zotpapers/Cutter_Manduchi_2015_Towards Mobile OCR.pdf:application/pdf},
}

@article{dawid_maximum_1979,
	title = {Maximum likelihood estimation of observer error-rates using the {EM} algorithm},
	url = {https://www.jstor.org/stable/2346806},
	journal = {Applied statistics},
	author = {Dawid, Alexander Philip and Skene, Allan M.},
	year = {1979},
	note = {Publisher: JSTOR},
	pages = {20--28},
}

@inproceedings{delakis_text_2008,
	title = {Text detection with convolutional neural networks},
	url = {https://www.researchgate.net/profile/Christophe_Garcia2/publication/221415287_text_Detection_with_Convolutional_Neural_Networks/links/545251a30cf2bccc49087299/text-Detection-with-Convolutional-Neural-Networks.pdf},
	booktitle = {International {Conference} on {Computer} {Vision} {Theory} and {Application}},
	author = {Delakis, Manolis and Garcia, Christophe},
	year = {2008},
	keywords = {CNN},
	pages = {290--294},
	file = {Delakis_Garcia_2008_Text detection with convolutional neural networks.pdf:/home/ptorras/zotpapers/Delakis_Garcia_2008_Text detection with convolutional neural networks.pdf:application/pdf},
}

@misc{bent_digital_1998,
	title = {Digital {Image} {Archive} of {Medieval} {Music}},
	url = {https://www.diamm.ac.uk},
	publisher = {University of Oxford},
	author = {Bent, Margaret and Wathey, Andrew},
	year = {1998},
}

@article{downie_music_2008,
	title = {The music information retrieval evaluation exchange (2005–2007): {A} window into music information retrieval research},
	volume = {29},
	doi = {10.1250/ast.29.247},
	number = {4},
	journal = {Acoust. Sci. \& Tech.},
	author = {Downie, J. Stephen},
	year = {2008},
	note = {Publisher: Acoustical Society of Japan},
	pages = {247--255},
	file = {Downie_2008_The music information retrieval evaluation exchange (2005–2007).pdf:/home/ptorras/zotpapers/Downie_2008_The music information retrieval evaluation exchange (2005–2007).pdf:application/pdf},
}

@incollection{downie_music_2010,
	address = {Berlin, Heidelberg},
	title = {The {Music} {Information} {Retrieval} {Evaluation} {eXchange}: {Some} {Observations} and {Insights}},
	isbn = {978-3-642-11674-2},
	url = {https://doi.org/10.1007/978-3-642-11674-2_5},
	abstract = {Advances in the science and technology of Music Information Retrieval (MIR) systems and algorithms are dependent on the development of rigorous measures of accuracy and performance such that meaningful comparisons among current and novel approaches can be made. This is the motivating principle driving the efforts of the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) and the annual Music Information Retrieval Evaluation eXchange (MIREX). Since it started in 2005, MIREX has fostered great advancements not only in many specific areas of MIR, but also in our general understanding of how MIR systems and algorithms are to be evaluated. This chapter outlines some of the major highlights of the past four years of MIREX evaluations, including its organizing principles, the selection of evaluation metrics, and the evolution of evaluation tasks. The chapter concludes with a brief introduction of how MIREX plans to expand into the future using a suite of Web 2.0 technologies to automated MIREX evaluations.},
	booktitle = {Advances in {Music} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Downie, J. Stephen and Ehmann, Andreas F. and Bay, Mert and Jones, M. Cameron},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2_5},
	pages = {93--115},
}

@inproceedings{droettboom_correcting_2003,
	title = {Correcting broken characters in the recognition of historical printed documents},
	doi = {10.1109/JCDL.2003.1204889},
	abstract = {We present a new technique for dealing with broken characters, one of the major challenges in the optical character recognition (OCR) of degraded historical printed documents. A technique based on graph combinatorics is used to rejoin the appropriate connected components. It has been applied to real data with successful results.},
	booktitle = {Joint {Conference} on {Digital} {Libraries}},
	author = {Droettboom, Michael},
	year = {2003},
	keywords = {Optical character recognition software, Shape, Robustness, Character recognition, OCR, optical character recognition, broken character correction, Business, Carbon capture and storage, character sets, Combinatorial mathematics, connected component, Degradation, document image processing, graph combinatorics, graph theory, historical printed document, history, Optical design, Printing},
	pages = {364--366},
}

@article{erickson_darms_1975,
	title = {“{The} {Darms} project”: {A} status report},
	volume = {9},
	issn = {1572-8412},
	doi = {10.1007/BF02396292},
	number = {6},
	journal = {Computers and the Humanities},
	author = {Erickson, Raymond F.},
	year = {1975},
	pages = {291--298},
}

@article{eskenazi_comprehensive_2017,
	title = {A comprehensive survey of mostly textual document segmentation algorithms since 2008},
	volume = {64},
	issn = {0031-3203},
	doi = {10.1016/j.patcog.2016.10.023},
	abstract = {In document image analysis, segmentation is the task that identifies the regions of a document. The increasing number of applications of document analysis requires a good knowledge of the available technologies. This survey highlights the variety of the approaches that have been proposed for document image segmentation since 2008. It provides a clear typology of documents and of document image segmentation algorithms. We also discuss the technical limitations of these algorithms, the way they are evaluated and the general trends of the community. © 2016 Elsevier Ltd},
	language = {English},
	journal = {Pattern Recognition},
	author = {Eskenazi, Sébastien and Gomez-Krämer, Petra and Ogier, Jean-Marc},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Image segmentation, Document image analysis, Digital image storage, Document, Document image segmentation, Evaluation, Surveying, Surveys, Technical limitations, Textual documents, Trends, Typology},
	pages = {1--14},
	file = {Eskenazi et al_2017_A comprehensive survey of mostly textual document segmentation algorithms since.pdf:/home/ptorras/zotpapers/Eskenazi et al_2017_A comprehensive survey of mostly textual document segmentation algorithms since.pdf:application/pdf},
}

@article{everingham_pascal_2015,
	title = {The {Pascal} {Visual} {Object} {Classes} {Challenge}: {A} {Retrospective}},
	volume = {111},
	doi = {10.1007/s11263-014-0733-5},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	year = {2015},
	pages = {98--136},
	file = {Everingham et al_2015_The Pascal Visual Object Classes Challenge.pdf:/home/ptorras/zotpapers/Everingham et al_2015_The Pascal Visual Object Classes Challenge.pdf:application/pdf},
}

@article{ewert_score-informed_2014,
	title = {Score-{Informed} {Source} {Separation} for {Musical} {Audio} {Recordings}: {An} overview},
	volume = {31},
	doi = {10.1109/MSP.2013.2296076},
	number = {3},
	journal = {IEEE Signal Process. Mag.},
	author = {Ewert, Sebastian and Pardo, Bryan and Müller, Meinard and Plumbley, Mark D.},
	year = {2014},
	pages = {116--124},
	file = {Ewert et al_2014_Score-Informed Source Separation for Musical Audio Recordings.pdf:/home/ptorras/zotpapers/Ewert et al_2014_Score-Informed Source Separation for Musical Audio Recordings.pdf:application/pdf},
}

@inproceedings{fahmy_survey_1992,
	title = {A survey of graph grammars: {Theory} and applications},
	url = {https://www.researchgate.net/publication/3513859_A_survey_of_graph_grammars_theory_and_applications},
	booktitle = {11th {IAPR} {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Fahmy, Hoda and Blostein, Dorothea},
	year = {1992},
	pages = {294--298},
	file = {Fahmy_Blostein_1992_A survey of graph grammars.pdf:/home/ptorras/zotpapers/Fahmy_Blostein_1992_A survey of graph grammars.pdf:application/pdf},
}

@article{fasanaro_dissociation_1990,
	title = {Dissociation in {Musical} {Reading}: {A} {Musician} {Affected} by {Alexia} without {Agraphia}},
	volume = {7},
	issn = {0730-7829},
	url = {http://mp.ucpress.edu/content/7/3/259},
	doi = {10.2307/40285464},
	abstract = {Previous works have postulated a similarity between music reading and text reading. Therefore it is interesting to evaluate both of these functions in an alexic subject. The patient investigated is a professional musician who had an ischemic lesion in the left temporoparieto-occipital region. Text reading showed pure alexia in which both the phonological and global routes were damaged. His ability to read correctly via matching tests showed that the word-form system was preserved. The reading of musical scores was damaged too and showed a dissociation between the reading of ideograms and rhythms (preserved) and the reading of notes (impaired). The results of note reading were analogous to those of word reading. Furthermore, the patient could read notes correctly via matching tests. On the basis of these findings, we propose a model of music reading where the reading of notes is based on a representational system analogous to that of words (the so-called internal language) whereas reading of ideograms and rhythms occurs via an internal representation unrelated to linguistic functions.},
	number = {3},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Fasanaro, A. M. and Spitaleri, D. L. A. and Valiani, R. and Grossi, D.},
	year = {1990},
	note = {Publisher: University of California Press Journals},
	pages = {259--272},
}

@book{feist_berklee_2017,
	title = {Berklee {Contemporary} {Music} {Notation}},
	isbn = {978-0-87639-178-5},
	abstract = {Learn the nuances of music notation, and create professional looking scores. This reference presents a comprehensive look at contemporary music notation. You will learn the meaning and stylistic practices for many types of notation that are currently in common use, from traditional staffs to lead sheets to guitar tablature. It discusses hundreds of notation symbols, as well as general guidelines for writing music. Berklee College of Music brings together teachers and students from all over the world, and we use notation in a great variety of ways. This book presents our perspectives on notation: what we have found to be the most commonly used practices in today's music industry, and what seems to be serving our community best. It includes a foreword by Matthew Nicholl, who was a long-time chair of Berklee's Contemporary Writing and Production Department. Whether you find yourself in a Nashville recording studio, Hollywood sound stage, grand concert hall, worship choir loft, or elementary school auditorium, this book will help you to create readable, professional, publication-quality notation. Beyond understanding the standard rules and definitions, you will learn to make appropriate choices for your own work, and generally how to achieve clarity and consistency in your notation so that it best serves your music.},
	publisher = {Berklee Press},
	author = {Feist, Jonathan},
	year = {2017},
}

@article{ferlaino_towards_2018,
	title = {Towards {Deep} {Cellular} {Phenotyping} in {Placental} {Histology}},
	url = {https://openreview.net/pdf?id=HJq5OGKsz},
	journal = {ArXiv e-prints},
	author = {Ferlaino, Michael and Glastonbury, Craig A. and Motta-Mejia, Carolina and Vatish, Manu and Granne, Ingrid and Kennedy, Stephen and Lindgren, Cecilia M. and Nellåker, Christoffer},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ferlaino et al_2018_Towards Deep Cellular Phenotyping in Placental Histology.pdf:/home/ptorras/zotpapers/Ferlaino et al_2018_Towards Deep Cellular Phenotyping in Placental Histology.pdf:application/pdf},
}

@article{fletcher_robust_1988,
	title = {A {Robust} {Algorithm} for {Text} {String} {Separation} from {Mixed} {Text}/{Graphics} {Images}},
	volume = {10},
	doi = {10.1109/34.9112},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Fletcher, Lloyd Alan and Kasturi, Rangachar},
	year = {1988},
	keywords = {document analysis, character recognition, computer graphics, computerised pattern recognition, computerised picture processing, computerized picture processing, graphics recognition, Hough transform, mixed text/graphics images, text string separation, transforms},
	pages = {910--918},
}

@inproceedings{gao_hidden_2003,
	title = {A hidden {Markov} model based approach to music segmentation and identification},
	doi = {10.1109/ICICS.2003.1292732},
	abstract = {Classification of musical segments is an interesting problem. It is a key technology in the development of content-based audio document indexing and retrieval. In this paper, we apply the feature extraction and modeling techniques commonly used in automatic speech recognition to solving the problem of segmentation and instrument identification of musical passages. The correlation among the different components in the feature space and the auto-correlation of each component are analyzed to demonstrate feasibility in musical signal analysis and instrument class modeling. Our experimental results are first evaluated on 3 instrument categories, i.e. vocal music, instrumental music, and their combinations. Furthermore each category is split into two individual cases to give a 6-class problem. Our results show that good performance could be obtained with simple features, such as mel-frequency cepstral coefficients and cepstral coefficients derived from linear prediction signal analysis. Even with a limited amount of training data, we could give an accuracy of 90.60\% in the case of three categories. A slightly worse accuracy of 90.38\% is obtained when we double the number of categories to six classes.},
	booktitle = {Joint {Fourth} {International} {Conference} on {Information}, {Communications} and {Signal} {Processing} and the {Fourth} {Pacific} {Rim} {Conference} on {Multimedia}},
	author = {Gao, Sheng and Maddage, Namunu Chinthaka and Lee, Chin-Hui},
	year = {2003},
	keywords = {Hidden Markov models, Feature extraction, hidden Markov models, Automatic speech recognition, Indexing, speech recognition, audio indexing, audio retrieval, audio signal processing, automatic speech recognition, cepstral analysis, Cepstral analysis, cepstral coefficients, component correlations, Content based retrieval, content-based retrieval, correlation methods, feature extraction, hidden Markov model, indexing, instrumental music, Instruments, linear prediction signal analysis, music, Music information retrieval, music segmentation, musical passage identification, musical segment classification, musical signal analysis, Signal analysis, signal classification, Space technology, time-varying signals, vocal music},
	pages = {1576--1580 vol.3},
}

@inproceedings{garfinkle_patternfinder_2017,
	address = {New York, NY, USA},
	title = {{PatternFinder}: {Content}-{Based} {Music} {Retrieval} with {Music21}},
	isbn = {978-1-4503-5347-2},
	doi = {10.1145/3144749.3144751},
	booktitle = {4th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Garfinkle, David and Arthur, Claire and Schubert, Peter and Cumming, Julie and Fujinaga, Ichiro},
	year = {2017},
	note = {event-place: Shanghai, China},
	keywords = {content-based music retrieval, imitation masses, music21, point-set similarity, polyphonic search, symbolic music similarity, time-scaled, time-warped, transposition-invariant},
	pages = {5--8},
}

@inproceedings{gatos_adaptive_2004,
	address = {Berlin, Heidelberg},
	title = {An {Adaptive} {Binarization} {Technique} for {Low} {Quality} {Historical} {Documents}},
	isbn = {978-3-540-28640-0},
	doi = {10.1007/978-3-540-28640-0_10},
	abstract = {Historical document collections are a valuable resource for human history. This paper proposes a novel digital image binarization scheme for low quality historical documents allowing further content exploitation in an efficient way. The proposed scheme consists of five distinct steps: a pre-processing procedure using a low-pass Wiener filter, a rough estimation of foreground regions using Niblack's approach, a background surface calculation by interpolating neighboring background intensities, a thresholding by combining the calculated background surface with the original image and finally a post-processing step in order to improve the quality of text regions and preserve stroke connectivity. The proposed methodology works with great success even in cases of historical manuscripts with poor quality, shadows, nonuniform illumination, low contrast, large signal- dependent noise, smear and strain. After testing the proposed method on numerous low quality historical manuscripts, it has turned out that our methodology performs better compared to current state-of-the-art adaptive thresholding techniques.},
	booktitle = {Document {Analysis} {Systems} {VI}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gatos, Basilios and Pratikakis, Ioannis and Perantonis, Stavros J.},
	editor = {Marinai, Simone and Dengel, Andreas R.},
	year = {2004},
	pages = {102--113},
	file = {Gatos et al_2004_An Adaptive Binarization Technique for Low Quality Historical Documents.pdf:/home/ptorras/zotpapers/Gatos et al_2004_An Adaptive Binarization Technique for Low Quality Historical Documents.pdf:application/pdf},
}

@article{gatos_adaptive_2006,
	title = {Adaptive degraded document image binarization},
	volume = {39},
	issn = {0031-3203},
	doi = {10.1016/j.patcog.2005.09.010},
	number = {3},
	journal = {Pattern Recognition},
	author = {Gatos, Basilios and Pratikakis, Ioannis and Perantonis, Stavros J.},
	year = {2006},
	keywords = {binarization, degraded document images, local adaptive binarization},
	pages = {317--327},
}

@inproceedings{gatos_icdar_2009,
	title = {{ICDAR} 2009 {Document} {Image} {Binarization} {Contest} ({DIBCO} 2009)},
	doi = {10.1109/ICDAR.2009.246},
	abstract = {DIBCO 2009 is the first International Document Image Binarization Contest organized in the context of ICDAR 2009 conference. The general objective of the contest is to identify current advances in document image binarization using established evaluation performance measures. This paper describes the contest details including the evaluation measures used as well as the performance of the 43 submitted methods along with a short description of each method.},
	booktitle = {2009 10th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Gatos, Basilios and Ntirogiannis, K. and Perantonis, Stavros J.},
	year = {2009},
	note = {ISSN: 1520-5363},
	keywords = {Text analysis, Image recognition, document image processing, Adaptive filters, Computational intelligence, Current measurement, document image analysis, document image binarization, evaluation, Filtering, Image analysis, image recognition, Informatics, international document image binarization contest, Iterative algorithms, Laboratories, performance measure},
	pages = {1375--1382},
	file = {Gatos et al_2009_ICDAR 2009 Document Image Binarization Contest (DIBCO 2009).pdf:/home/ptorras/zotpapers/Gatos et al_2009_ICDAR 2009 Document Image Binarization Contest (DIBCO 2009).pdf:application/pdf},
}

@article{george_computer_2014,
	title = {Computer analysis of similarities between albums in popular music},
	volume = {45},
	doi = {10.1016/j.patrec.2014.02.021},
	journal = {Pattern Recognition Letters},
	author = {George, Joe and Shamir, Lior},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {78--84},
}

@book{gerou_essential_2009,
	title = {Essential {Dicionary} of {Music} {Notation}},
	isbn = {0-8828284-768-6},
	url = {http://www.amazon.com/Essentials-Music-Notation-Alfred-Publishing/dp/073906083X},
	publisher = {Alfred Publishing Co., Inc.},
	author = {Gerou, Tom and Lusk, Linda},
	year = {2009},
	keywords = {music notation},
}

@article{giotis_survey_2017,
	title = {A survey of document image word spotting techniques},
	volume = {68},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320317300870},
	doi = {https://doi.org/10.1016/j.patcog.2017.02.023},
	abstract = {Vast collections of documents available in image format need to be indexed for information retrieval purposes. In this framework, word spotting is an alternative solution to optical character recognition (OCR), which is rather inefficient for recognizing text of degraded quality and unknown fonts usually appearing in printed text, or writing style variations in handwritten documents. Over the past decade there has been a growing interest in addressing document indexing using word spotting which is reflected by the continuously increasing number of approaches. However, there exist very few comprehensive studies which analyze the various aspects of a word spotting system. This work aims to review the recent approaches as well as fill the gaps in several topics with respect to the related works. The nature of texts and inherent challenges addressed by word spotting methods are thoroughly examined. After presenting the core steps which compose a word spotting system, we investigate the use of retrieval enhancement techniques based on relevance feedback which improve the retrieved results. Finally, we present the datasets which are widely used for word spotting, we describe the evaluation standards and measures applied for performance assessment and discuss the results achieved by the state of the art.},
	journal = {Pattern Recognition},
	author = {Giotis, Angelos P. and Sfikas, Giorgos and Gatos, Basilis and Nikou, Christophoros},
	year = {2017},
	keywords = {Document indexing, Features, Relevance feedback, Representation, Retrieval, Word spotting},
	pages = {310--332},
}

@inproceedings{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	doi = {10.1109/CVPR.2014.81},
	booktitle = {{IEEE} {Conference} {On} {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
	file = {Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:/home/ptorras/zotpapers/Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:application/pdf},
}

@inproceedings{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	doi = {10.1109/ICCV.2015.169},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Girshick, Ross},
	year = {2015},
	keywords = {Training, Feature extraction, Pipelines, Object detection, Proposals, Computer architecture, C++, Caffe, fast R-CNN, fast region-based convolutional network method, feedforward neural nets, object detection, Open source software, open-source MIT License, Python, VGG16 network},
	pages = {1440--1448},
}

@article{girshick_region-based_2016,
	title = {Region-{Based} {Convolutional} {Networks} for {Accurate} {Object} {Detection} and {Segmentation}},
	volume = {38},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2015.2437384},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2016},
	keywords = {deep learning, Training, Image segmentation, Feature extraction, Detectors, Object detection, Proposals, Support vector machines, object detection, canonical PASCAL VOC Challenge datasets, convolutional codes, convolutional networks, detection, high-capacity convolutional networks, image coding, image segmentation, mAP, mean average precision, Object recognition, object segmentation, region-based convolutional networks, semantic segmentation, source code, source coding, transfer learning},
	pages = {142--158},
}

@techreport{good_musicxml_2001,
	title = {{MusicXML}: {An} {Internet}-{Friendly} {Format} for {Sheet} {Music}},
	url = {https://pdfs.semanticscholar.org/5617/972667ff794da79a4cbb6b985e85f8487ddd.pdf},
	institution = {Recordare LLC},
	author = {Good, Michael},
	year = {2001},
}

@inproceedings{good_using_2003,
	title = {Using {MusicXML} for {File} {Interchange}},
	doi = {10.1109/WDM.2003.1233890},
	abstract = {The MusicXML format is designed to be a universal translator for programs that understand common Western musical notation. We have made significant progress towards this goal, with over a dozen programs supporting MusicXML as of June 2003. We describe some of the ways that MusicXML has been used for file interchange, and will demonstrate several scenarios.},
	booktitle = {Third {International} {Conference} on {WEB} {Delivering} of {Music}},
	author = {Good, Michael and Actor, Geri},
	year = {2003},
	keywords = {Displays, Prototypes, music, Application software, electronic data interchange, file interchange, IEEE news, Markup languages, MusicXML format, program interpreters, Software prototyping, Technological innovation, universal translator, Western musical notation, XML},
	pages = {153},
}

@article{goodfellow_multi-digit_2013,
	title = {Multi-digit {Number} {Recognition} from {Street} {View} {Imagery} using {Deep} {Convolutional} {Neural} {Networks}},
	volume = {abs/1312.6},
	url = {http://arxiv.org/abs/1312.6082},
	abstract = {Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96\% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving 97.84\% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90\% accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a 99.8\% accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.},
	journal = {Computing Research Repository},
	author = {Goodfellow, Ian J. and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
	year = {2013},
	keywords = {CNN},
	pages = {1--13},
	file = {Goodfellow et al_2013_Multi-digit Number Recognition from Street View Imagery using Deep.pdf:/home/ptorras/zotpapers/Goodfellow et al_2013_Multi-digit Number Recognition from Street View Imagery using Deep.pdf:application/pdf},
}

@article{goolsby_eye_1994,
	title = {Eye {Movement} in {Music} {Reading}: {Effects} of {Reading} {Ability}, {Notational} {Complexity}, and {Encounters}},
	volume = {12},
	issn = {0730-7829},
	url = {http://mp.ucpress.edu/content/12/1/77},
	doi = {10.2307/40285756},
	abstract = {Six types of eye movement were measured and recorded with an SRI Eyetracker: number of progressive and regressive fixations, durations of progressive and regressive fixations and lengths of progressive and regressive saccades. Twenty-four graduate music students were selected as skilled and less- skilled music readers. Eye position was measured every millisecond with a high degree of accuracy. The factorial design was 2 Groups x 4 Melodies x 3 Encounters (including a practice period). Results indicated that patterns of eye movement in the two groups were similar across melodies and encounters, but differed with notational complexity. Eye movement was reduced when performing melodies with more-concentrated visual information than when performing melodies with less- concentrated visual information. The main effect of encounters indicated that music readers used fewer but longer fixations after practicing the melodies. Results suggest that skilled music readers look farther ahead in the notation, and then back to the point of performance, when sightreading.},
	number = {1},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Goolsby, Thomas W.},
	year = {1994},
	note = {Publisher: University of California Press Journals},
	pages = {77--96},
}

@article{goolsby_profiles_1994,
	title = {Profiles of {Processing}: {Eye} {Movements} during {Sightreading}},
	volume = {12},
	issn = {0730-7829},
	url = {http://mp.ucpress.edu/content/12/1/97},
	doi = {10.2307/40285757},
	abstract = {Temporal and sequential components of the eye movement used by a skilled and a less-skilled sightreader were used to construct six profiles of processing. Each subject read three melodies of varying levels of concentration of visual detail. The profiles indicates the order, duration, and location of each fixation while the subjects sightread the melodies. Results indicate that music readers do not fixate on note stems or the bar lines that connect eighth notes when sightreading. The less-skilled music reader progressed through the melody virtually note-by-note using long fixations, whereas the skilled sightreader directed fixations to all areas of the notation (using more regressions than the less-skilled reader) to perform the music accurately. Results support earlier findings that skilled sightreaders look farther ahead in the notation, then back to the point of performance (Goolsby, 1994), and have a larger perceptual span than less-skilled sightreaders. Findings support Slobodans (1984) contention that music reading (i. e., sightreading) is indeed music perception, because music notation is processed before performance. Support was found for Sloboda's (1977, 1984, 1985, 1988) hypotheses on the effects of physical and structural boundaries on visual musical perception. The profiles indicate a number of differences between music perception from processing visual notation and perception resulting from language reading. These differences include: (1) opposite trends in the control of eye movement (i. e., the better music reader fixates in blank areas of the visual stimuli and not directly on each item of the information that was performed), (2) a perceptual span that is vertical as well as horizontal, (3) more eye movement associated with the better reader, and (4) greater attention used for processing language than for music, although the latter task requires an "exact realization."},
	number = {1},
	journal = {Music Perception: An Interdisciplinary Journal},
	author = {Goolsby, Thomas W.},
	year = {1994},
	note = {Publisher: University of California Press Journals},
	pages = {97--123},
}

@book{gould_behind_2011,
	title = {Behind {Bars}},
	isbn = {0-571-51456-1},
	url = {http://behindbarsnotation.co.uk/},
	publisher = {Faber Music},
	author = {Gould, Elaine},
	year = {2011},
}

@inproceedings{graves_unconstrained_2007,
	address = {USA},
	title = {Unconstrained {Online} {Handwriting} {Recognition} with {Recurrent} {Neural} {Networks}},
	isbn = {978-1-60560-352-0},
	url = {http://dl.acm.org/citation.cfm?id=2981562.2981635},
	booktitle = {20th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Graves, Alex and Fernández, Santiago and Liwicki, Marcus and Bunke, Horst and Schmidhuber, Jürgen},
	year = {2007},
	note = {event-place: Vancouver, British Columbia, Canada},
	pages = {577--584},
}

@inproceedings{graves_speech_2013,
	title = {Speech recognition with deep recurrent neural networks},
	doi = {10.1109/ICASSP.2013.6638947},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	year = {2013},
	note = {ISSN: 1520-6149},
	keywords = {Training, Noise, connectionist temporal classification, Recurrent neural networks, Speech recognition, Acoustics, recurrent neural networks, speech recognition, deep neural networks, deep recurrent neural networks, end-to-end training methods, long short-term memory RNN architecture, sequential data, Vectors},
	pages = {6645--6649},
	file = {Graves et al_2013_Speech recognition with deep recurrent neural networks.pdf:/home/ptorras/zotpapers/Graves et al_2013_Speech recognition with deep recurrent neural networks.pdf:application/pdf},
}

@inproceedings{graves_towards_2014,
	address = {Beijing, China},
	title = {Towards {End}-to-end {Speech} {Recognition} with {Recurrent} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3045089},
	booktitle = {31st {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Graves, Alex and Jaitly, Navdeep},
	year = {2014},
	pages = {1764--1772},
}

@article{grefenstette_learning_2015,
	title = {Learning to {Transduce} with {Unbounded} {Memory}},
	volume = {abs/1506.02516},
	url = {http://arxiv.org/abs/1506.02516},
	journal = {Computing Research Repository},
	author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
	year = {2015},
	file = {Grefenstette et al_2015_Learning to Transduce with Unbounded Memory.pdf:/home/ptorras/zotpapers/Grefenstette et al_2015_Learning to Transduce with Unbounded Memory.pdf:application/pdf},
}

@article{gregor_draw_2015,
	title = {{DRAW}: {A} {Recurrent} {Neural} {Network} {For} {Image} {Generation}},
	volume = {abs/1502.04623},
	url = {http://arxiv.org/abs/1502.04623},
	journal = {Computing Research Repository},
	author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
	year = {2015},
	file = {Gregor et al_2015_DRAW.pdf:/home/ptorras/zotpapers/Gregor et al_2015_DRAW.pdf:application/pdf},
}

@inproceedings{hankinson_music_2011,
	title = {The {Music} {Encoding} {Initiative} as a {Document}-{Encoding} {Framework}},
	url = {https://ismir2011.ismir.net/papers/OS3-1.pdf},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Hankinson, Andrew and Roland, Perry and Fujinaga, Ichiro},
	year = {2011},
	pages = {293--298},
	file = {Hankinson et al_2011_The Music Encoding Initiative as a Document-Encoding Framework.pdf:/home/ptorras/zotpapers/Hankinson et al_2011_The Music Encoding Initiative as a Document-Encoding Framework.pdf:application/pdf},
}

@book{harman_information_2011,
	edition = {1st},
	title = {Information {Retrieval} {Evaluation}},
	isbn = {1-59829-971-9 978-1-59829-971-7},
	publisher = {Morgan \& Claypool Publishers},
	author = {Harman, Donna},
	year = {2011},
}

@inproceedings{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf},
	booktitle = {The {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	month = oct,
	year = {2017},
	file = {He et al_2017_Mask R-CNN.pdf:/home/ptorras/zotpapers/He et al_2017_Mask R-CNN.pdf:application/pdf},
}

@book{heussenstamm_norton_1987,
	title = {The {Norton} {Manual} of {Music} {Notation}},
	isbn = {978-0-393-95526-2},
	publisher = {W. W. Norton \& Company},
	author = {Heussenstamm, George},
	year = {1987},
}

@article{hosang_what_2016,
	title = {What {Makes} for {Effective} {Detection} {Proposals}?},
	volume = {38},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2015.2465908},
	abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hosang, Jan and Benenson, Rodrigo and Dollár, Piotr and Schiele, Bernt},
	year = {2016},
	keywords = {Training, Image segmentation, Detectors, Object detection, Computer Vision, Measurement, Proposals, object detection, average recall, Computer vision, detection proposals, DPM detection performance, Fast R-CNN detection performance, ground truth annotation recall, Image edge detection, ImageNet, MS COCO, object localisation, PASCAL, proposal localisation accuracy, proposal repeatability},
	pages = {814--830},
	file = {Hosang et al_2016_What Makes for Effective Detection Proposals.pdf:/home/ptorras/zotpapers/Hosang et al_2016_What Makes for Effective Detection Proposals.pdf:application/pdf},
}

@article{hu_learning_2017,
	title = {Learning to {Segment} {Every} {Thing}},
	volume = {abs/1711.10370},
	url = {http://arxiv.org/abs/1711.10370},
	journal = {CoRR},
	author = {Hu, Ronghang and Dollár, Piotr and He, Kaiming and Darrell, Trevor and Girshick, Ross},
	year = {2017},
	file = {Hu et al_2017_Learning to Segment Every Thing.pdf:/home/ptorras/zotpapers/Hu et al_2017_Learning to Segment Every Thing.pdf:application/pdf},
}

@techreport{huang_deep_2016,
	title = {Deep {Learning} for {Music}},
	url = {https://arxiv.org/abs/1606.04930},
	institution = {Stanford University},
	author = {Huang, Allen and Wu, Raymond},
	year = {2016},
	note = {Backup Publisher: Stanford University},
	file = {Huang_Wu_2016_Deep Learning for Music.pdf:/home/ptorras/zotpapers/Huang_Wu_2016_Deep Learning for Music.pdf:application/pdf},
}

@inproceedings{huang_speedaccuracy_2017,
	title = {Speed/{Accuracy} {Trade}-{Offs} for {Modern} {Convolutional} {Object} {Detectors}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	year = {2017},
	file = {Huang et al_2017_Speed-Accuracy Trade-Offs for Modern Convolutional Object Detectors.pdf:/home/ptorras/zotpapers/Huang et al_2017_Speed-Accuracy Trade-Offs for Modern Convolutional Object Detectors.pdf:application/pdf},
}

@misc{project_petrucci_llc_international_2006,
	title = {International {Music} {Score} {Library} {Project}},
	url = {http://imslp.org},
	author = {{Project Petrucci LLC}},
	year = {2006},
}

@article{journet_doccreator_2017,
	title = {{DocCreator}: {A} {New} {Software} for {Creating} {Synthetic} {Ground}-{Truthed} {Document} {Images}},
	volume = {3},
	doi = {10.3390/jimaging3040062},
	number = {4},
	journal = {Journal of Imaging},
	author = {Journet, Nicholas and Visani, Muriel and Mansencal, Boris and Van-Cuong, Kieu and Billy, Antoine},
	year = {2017},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {62},
	file = {Journet et al_2017_DocCreator.pdf:/home/ptorras/zotpapers/Journet et al_2017_DocCreator.pdf:application/pdf},
}

@inproceedings{katayose_approach_1989,
	title = {An approach to an artificial music expert},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Katayose, Haruhiro and Kato, H. and Imai, M. and S., Inokuchi},
	year = {1989},
	pages = {139--146},
}

@article{katayose_kansei_1989,
	title = {The {Kansei} {Music} {System}},
	volume = {13},
	issn = {01489267, 15315169},
	url = {http://www.jstor.org/stable/3679555},
	number = {4},
	journal = {Computer Music Journal},
	author = {Katayose, Haruhiro and Inokuchi, Seiji},
	year = {1989},
	note = {Publisher: The MIT Press},
	pages = {72--77},
}

@article{keil_applications_2017,
	title = {Applications of {RISM} data in digital libraries and digital musicology},
	issn = {1432-1300},
	url = {https://doi.org/10.1007/s00799-016-0205-3},
	doi = {10.1007/s00799-016-0205-3},
	abstract = {Information about manuscripts and printed music indexed in RISM (Répertoire International des Sources Musicales), a large, international project that records and describes musical sources, was for decades available solely through book publications, CD-ROMs, or subscription services. Recent initiatives to make the data available on a wider scale have resulted in, most significantly, a freely accessible online database and the availability of its data as open data and linked open data. Previously, the task of increasing the amount of data was primarily carried out by RISM national groups and the Zentralredaktion (Central Office). The current opportunities available by linking to other freely accessible databases and importing data from other resources open new perspectives and prospects. This paper describes the RISM data and their applications for digital libraries and digital musicological projects. We discuss the possibilities and challenges in making available a large and growing quantity of data and how the data have been utilized in external library and musicological projects. Interactive functions in the RISM OPAC are planned for the future, as is closer collaboration with the projects that use RISM data. Ultimately, RISM would like to arrange a “take and give” system in which the RISM data are used in external projects, enhanced by the project participants, and then delivered back to the RISM Zentralredaktion.},
	journal = {International Journal on Digital Libraries},
	author = {Keil, Klaus and Ward, Jennifer A.},
	year = {2017},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	volume = {abs/1412.6980},
	url = {http://arxiv.org/abs/1412.6980},
	journal = {Computing Research Repository},
	author = {Kingma, Diederik P. and Ba, Jimmy Lei},
	year = {2014},
	file = {Kingma_Ba_2014_Adam.pdf:/home/ptorras/zotpapers/Kingma_Ba_2014_Adam.pdf:application/pdf},
}

@inproceedings{koehn_europarl_2005,
	title = {Europarl: {A} parallel corpus for statistical machine translation},
	url = {http://courses.washington.edu/ling473/Project5.pdf},
	booktitle = {{MT} summit},
	author = {Koehn, Philipp},
	year = {2005},
	pages = {79--86},
}

@inproceedings{kurth_automated_2007,
	address = {Vienna, Austria},
	title = {Automated synchronization of scanned sheet music with audio recordings},
	isbn = {978-3-85403-218},
	url = {http://ismir2007.ismir.net/proceedings/ISMIR2007_p261_kurth.pdf},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Kurth, Frank and Müller, Meinard and Fremerey, Christian and Chang, Yoon-ha and Clausen, Michael},
	year = {2007},
	keywords = {others},
	pages = {261--266},
	file = {Kurth et al_2007_Automated synchronization of scanned sheet music with audio recordings.pdf:/home/ptorras/zotpapers/Kurth et al_2007_Automated synchronization of scanned sheet music with audio recordings.pdf:application/pdf},
}

@incollection{lake_one-shot_2013,
	title = {One-shot learning by inverting a compositional causal process},
	url = {http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2526--2534},
	file = {Lake et al_2013_One-shot learning by inverting a compositional causal process.pdf:/home/ptorras/zotpapers/Lake et al_2013_One-shot learning by inverting a compositional causal process.pdf:application/pdf},
}

@inproceedings{lavie_meteor_2007,
	address = {Stroudsburg, PA, USA},
	title = {Meteor: {An} {Automatic} {Metric} for {MT} {Evaluation} with {High} {Levels} of {Correlation} with {Human} {Judgments}},
	url = {http://dl.acm.org/citation.cfm?id=1626355.1626389},
	booktitle = {Second {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Lavie, Alon and Agarwal, Abhaya},
	year = {2007},
	note = {event-place: Prague, Czech Republic},
	pages = {228--231},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	keywords = {Neural networks, Hidden Markov models, Optical character recognition software, Feature extraction, Machine learning, Character recognition, Pattern recognition, Optical computing, optical character recognition, 2D shape variability, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pages = {436--444},
}

@inproceedings{lee_comparison_2016,
	title = {Comparison of faster {R}-{CNN} models for object detection},
	doi = {10.1109/ICCAS.2016.7832305},
	abstract = {Object detection is one of the important problems for autonomous robots. Faster R-CNN, one of the state-of-the-art object detection methods, approaches real time application; nevertheless, computational time lies borderline of real time application, i.e. 5fps with VGG16 model in K40 GPU system in [1]. Moreover, computation time depends on model and image crop size, but precision is also affected; usually, time and precision have trade-off relation. By adjusting input image size in spite of downgrading performance, computation time meets criteria for one model. Therefore, selection of a model is one of the important problems when faster R-CNN based object detection system for an autonomous robot is constructed. In this paper, we convert several state-of-the-art models from convolution neural network (CNN) for image classification. Then, we compare converted models with several image crop size in terms of computation time and detection precision. We will utilize those comparison data for selecting a proper detection model in case a robot needs to perform an object detection task.},
	booktitle = {2016 16th {International} {Conference} on {Control}, {Automation} and {Systems} ({ICCAS})},
	author = {Lee, Chungkeun and Kim, H. Jin and Oh, Kyeong Won},
	year = {2016},
	keywords = {Data models, Feature extraction, Computational modeling, Object detection, object detection, convolution, Agriculture, autonomous robot, Convolution, convolution neural network, Convolution neural network, Faster R-CNN, image classification, mobile robots, neural nets, R-CNN model, Robots},
	pages = {107--110},
}

@article{lewis_rcv1_2004,
	title = {Rcv1: {A} new benchmark collection for text categorization research},
	volume = {5},
	url = {http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf},
	journal = {The Journal of Machine Learning Research},
	author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
	year = {2004},
	note = {Publisher: JMLR. org},
	pages = {361--397},
	file = {Lewis et al_2004_Rcv1.pdf:/home/ptorras/zotpapers/Lewis et al_2004_Rcv1.pdf:application/pdf},
}

@misc{the_lilypond_developement_team_lilypond_2014,
	title = {{LilyPond} - {Essay} on automated music engraving},
	url = {http://www.lilypond.org/},
	author = {{The LilyPond Developement Team}},
	year = {2014},
	keywords = {music notation},
}

@inproceedings{lin_microsoft_2014,
	address = {Cham},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	pages = {740--755},
	file = {Lin et al_2014_Microsoft COCO.pdf:/home/ptorras/zotpapers/Lin et al_2014_Microsoft COCO.pdf:application/pdf},
}

@article{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	volume = {abs/1708.02002},
	url = {http://arxiv.org/abs/1708.02002},
	journal = {CoRR},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	year = {2017},
	file = {Lin et al_2017_Focal Loss for Dense Object Detection.pdf:/home/ptorras/zotpapers/Lin et al_2017_Focal Loss for Dense Object Detection.pdf:application/pdf},
}

@inproceedings{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	file = {Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:/home/ptorras/zotpapers/Lin et al_2017_Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	isbn = {978-3-319-46448-0},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300x300 input, SSD achieves 74.3\% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512x512 input, SSD achieves 76.9\% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {21--37},
	file = {Liu et al_2016_SSD.pdf:/home/ptorras/zotpapers/Liu et al_2016_SSD.pdf:application/pdf},
}

@inproceedings{lopez_effects_2019,
	address = {Vienna, Austria},
	title = {The effects of translation between symbolic music formats: {A} case study with {Humdrum}, {Lilypond}, {MEI}, and {MusicXML}},
	url = {https://music-encoding.org/conference/2019/abstracts_mec2019/The%20effects%20of%20translation%20between%20the%20Humdrum%20%20Lilypond%20%20MEI%20%20and%20MusicXML.pdf},
	booktitle = {Music {Encoding} {Conference} 2019},
	author = {López, Néstor Nápoles and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2019},
}

@article{machacek_results_2014,
	title = {Results of the {WMT14} {Metrics} {Shared} {Task}},
	journal = {Ninth Workshop on Statistical Machine Translation},
	author = {Macháček, Matouš and Bojar, Ondřej},
	year = {2014},
	note = {ISBN: 978-1-941643-17-4
Place: Baltimore, MD, USA
Publisher: Association for Computational Linguistics},
	pages = {293--301},
}

@article{machacek_evaluating_2015,
	title = {Evaluating {Machine} {Translation} {Quality} {Using} {Short} {Segments} {Annotations}},
	volume = {103},
	doi = {10.1515/pralin-2015-0005},
	number = {1},
	journal = {The Prague Bulletin of Mathematical Linguistics},
	author = {Macháček, Matouš and Bojar, Ondřej},
	year = {2015},
	note = {Publisher: Walter de Gruyter GmbH},
	file = {Machacek_Bojar_2015_Evaluating Machine Translation Quality Using Short Segments Annotations.pdf:/home/ptorras/zotpapers/Machacek_Bojar_2015_Evaluating Machine Translation Quality Using Short Segments Annotations.pdf:application/pdf},
}

@article{mandt_stochastic_2017,
	title = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {Inference}},
	url = {https://arxiv.org/abs/1704.04289},
	journal = {ArXiv e-prints},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	year = {2017},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {Mandt et al_2017_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf:/home/ptorras/zotpapers/Mandt et al_2017_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf:application/pdf},
}

@book{manning_introduction_2008,
	title = {Introduction to {Information} {Retrieval}},
	isbn = {978-0-521-86571-5},
	url = {https://nlp.stanford.edu/IR-book/},
	publisher = {Cambridge University Press},
	author = {Manning, Chirstopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	year = {2008},
}

@book{marr_vision_2010,
	title = {Vision},
	publisher = {W.H. Freeman and Company San Francisco},
	author = {Marr, David},
	year = {2010},
	doi = {10.7551/mitpress/9780262514620.001.0001},
}

@article{marsden_interrogating_2012,
	title = {Interrogating {Melodic} {Similarity}: {A} {Definitive} {Phenomenon} or the {Product} of {Interpretation}?},
	volume = {41},
	number = {4},
	journal = {Journal of New Music Research},
	author = {Marsden, Alan},
	year = {2012},
	note = {Publisher: Routledge},
	pages = {323--335},
}

@book{mattheson_vollkommene_1739,
	title = {Der vollkommene {Capellmeister}},
	isbn = {978-3-7618-1413-0},
	url = {https://imslp.org/wiki/Der_vollkommene_Capellmeister_(Mattheson,_Johann)},
	publisher = {Herold, Christian, Hamburg},
	author = {Mattheson, Johann},
	year = {1739},
}

@inproceedings{mckay_automatic_2004,
	address = {Barcelona, Spain},
	title = {Automatic genre classification using large high-level musical feature sets},
	url = {http://ismir2004.ismir.net/proceedings/p095-page-525-paper240.pdf},
	booktitle = {5th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {McKay, Cory and Fujinaga, Ichiro},
	year = {2004},
}

@inproceedings{mckay_jsymbolic_2006,
	address = {New Orleans, LA},
	title = {{jSymbolic}: {A} feature extractor for {MIDI} files},
	url = {https://www.music.mcgill.ca/~cmckay/papers/musictech/McKay_ICMC_06_jSymbolic.pdf},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {McKay, Cory and Fujinaga, Ichiro},
	year = {2006},
	pages = {302--305},
	file = {McKay_Fujinaga_2006_jSymbolic.pdf:/home/ptorras/zotpapers/McKay_Fujinaga_2006_jSymbolic.pdf:application/pdf},
}

@inproceedings{merchan_sanchez-jara_e-score_2015,
	title = {e-{Score}; {Impact}, {Perception} and {Uses} in {Music} {Educational} {Institutions}},
	isbn = {978-1-4503-3442-6},
	doi = {10.1145/2808580.2808647},
	abstract = {Parallel to the very first music expressions as a human activity, developed under some kind of guidelines and precepts, there have arisen relentless efforts and attempts to achieve the long-lasting and stable representation of the musical products being created, in a way that would allow to recreate them as closely as posible to the original event.from the very beginning until the present days, have been many notation systems, formats, and expressions developed with this purpose in mind. With the incursion and implementation of the new digital technologies in the musical activity spheres, a new paradigm emerge in the representation and transmission of the musical work, culminating nowadays with the born of a new music document typology tha may be called digital music score or e-Score. The aim of this paper is lay down and approach to this new type of object for music texts reading, editing and transmission through a quantitive research, being the scope of the study to analyze and understand how electronic music scores (e-Score) are received, used and perceived by music students and teachers in any of the music learning centers in our country. The main purpose is to provide scientific evidence, through statistical methods, of the level of implementation and use of these new support type for music knowledge transmission, as well as the most influential aspects implied in perception of therm amongst musical community in Spain.},
	language = {English},
	booktitle = {3rd {International} {Conference} on {Technological} {Ecosystems} for {Enhancing} {Multiculturality}},
	publisher = {Association for Computing Machinery},
	author = {Merchán Sánchez-Jara, Javier},
	editor = {Felgueiras M.C., Alves G.R.},
	year = {2015},
	keywords = {Computer music, Digital sheet music, Digital technologies, E-learning, Ecology, Ecosystems, Education, Educational institutions, Electronic editions, Human activities, Music education, Music readers, Scientific evidence, Teaching},
	pages = {449--454},
}

@misc{hao_real-time_2019,
	title = {Real-time {Audio} to {Score} {Alignment} (a.k.a {Score} {Following})},
	url = {https://www.music-ir.org/mirex/wiki/2019:Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following)},
	author = {Hao, Yun},
	year = {2019},
}

@article{mnih_recurrent_2014,
	title = {Recurrent {Models} of {Visual} {Attention}},
	volume = {abs/1406.6247},
	url = {http://arxiv.org/abs/1406.6247},
	journal = {Computing Research Repository},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
	year = {2014},
	file = {Mnih et al_2014_Recurrent Models of Visual Attention.pdf:/home/ptorras/zotpapers/Mnih et al_2014_Recurrent Models of Visual Attention.pdf:application/pdf},
}

@inproceedings{muge_automatic_2000,
	address = {Berlin, Heidelberg},
	title = {Automatic {Feature} {Extraction} and {Recognition} for {Digital} {Access} of {Books} of the {Renaissance}},
	isbn = {978-3-540-45268-3},
	doi = {10.1007/3-540-45268-0_1},
	abstract = {Antique printed books constitute a heritage that should be preserved and used. With novel digitising techniques is now possible to have these books stored in digital format and accessible to a wider public. However it remains the problem of how to use them. DEBORA (Digital accEss to BOoks of the RenAissance) is a European project that aims to develop a system to interact with these books through world-wide networks. The main issue is to build a database accessible through client computers. That will require to built accompanying metadata that should characterise different components of the books as illuminated letters, banners, figures and key words in order to simplify and speed up the remote access. To solve these problems, digital image analysis algorithms regarding filtering, segmentation, separation of text from non-text, lines and word segmentation and word recognition were developed. Some novel ideas are presented and illustrated through examples.},
	booktitle = {Research and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer Berlin Heidelberg},
	author = {Muge, F. and Granado, I. and Mengucci, M. and Pina, P. and Ramos, V. and Sirakov, N. and Caldas Pinto, J. R. and Marcolino, A. and Ramalho, Mário and Vieira, P. and Maia do Amaral, A.},
	editor = {Borbinha, José and Baker, Thomas},
	year = {2000},
	pages = {1--13},
}

@book{ng_interactive_2008,
	title = {Interactive {Multimedia} {Music} {Technologies}},
	isbn = {1-59904-152-9},
	abstract = {Many multimedia music content owners and distributors are converting their archives of music scores from paper into digital images, and to machine readable symbolic notation in order to survive in the business world. Interactive Multimedia Music Technologies discusses relevant state-of-the-art technologies and consists of analysis, knowledge, and application scenarios as surveyed, analyzed, and evaluated by industry professionals. Interactive Multimedia Music Technologies exemplifies the newest functionalities of multimedia interactive music to be used for valorizing cultural heritage, content and archives that are not currently distributed due to lack of safety, suitable coding models, and conversion technologies. Interactive Multimedia Music Technologies explains new and innovative methods of promoting music and products for entertainment, distance teaching, valorizing archives, and commercial and non-commercial purposes, and provides new services for those connected via personal computers, mobile and other devices, for both sighted and print-impaired consumers.},
	publisher = {IGI Global},
	author = {Ng, Kia and Nesi, Paolo},
	year = {2008},
	doi = {10.4018/978-1-59904-150-6},
	keywords = {others},
	file = {Ng_Nesi_2008_Interactive Multimedia Music Technologies.pdf:/home/ptorras/zotpapers/Ng_Nesi_2008_Interactive Multimedia Music Technologies.pdf:application/pdf},
}

@techreport{nielsen_statistical_2009,
	address = {Copenhagen, Denmark},
	title = {Statistical {Analysis} of {Music} {Corpora}},
	url = {https://johanbrinch.com/static/papers/brinchj-2008_music.pdf},
	institution = {University of Copenhagen},
	author = {Nielsen, Johan},
	year = {2009},
	note = {Backup Publisher: Dept. of Computer Science, University of Copenhagen},
	file = {Nielsen_2009_Statistical Analysis of Music Corpora.pdf:/home/ptorras/zotpapers/Nielsen_2009_Statistical Analysis of Music Corpora.pdf:application/pdf},
}

@inproceedings{nivre_pseudo-projective_2005,
	address = {Stroudsburg, PA, USA},
	title = {Pseudo-projective {Dependency} {Parsing}},
	doi = {10.3115/1219840.1219853},
	booktitle = {43rd {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nivre, Joakim and Nilsson, Jens},
	year = {2005},
	note = {event-place: Ann Arbor, Michigan},
	pages = {99--106},
}

@article{ntirogiannis_performance_2013,
	title = {Performance evaluation methodology for historical document image binarization},
	volume = {22},
	issn = {1057-7149},
	doi = {10.1109/TIP.2012.2219550},
	abstract = {Document image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process. The evaluation of a binarization method aids in studying its algorithmic behavior, as well as verifying its effectiveness, by providing qualitative and quantitative indication of its performance. This paper addresses a pixel-based binarization evaluation methodology for historical handwritten/machine-printed document images. In the proposed evaluation scheme, the recall and precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias. Additional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text, false alarms, background noise, character enlargement, and merging. Several experiments conducted in comparison with other pixel-based evaluation measures demonstrate the validity of the proposed evaluation scheme.},
	number = {2},
	journal = {IEEE Transactions on Image Processing},
	author = {Ntirogiannis, Konstantinos and Gatos, Basilis and Pratikakis, Ioannis},
	year = {2013},
	pmid = {23008259},
	keywords = {binarization, Document image binarization, ground truth, performance evaluation},
	pages = {595--609},
}

@book{palmer_vision_1999,
	title = {Vision science: {Photons} to phenomenology},
	url = {https://mitpress.mit.edu/books/vision-science},
	publisher = {MIT press},
	author = {Palmer, Stephen E.},
	year = {1999},
}

@inproceedings{papineni_bleu_2002,
	address = {Stroudsburg, PA, USA},
	title = {{BLEU}: {A} {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	doi = {10.3115/1073083.1073135},
	booktitle = {40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	note = {event-place: Philadelphia, Pennsylvania},
	pages = {311--318},
}

@inproceedings{phon-amnuaisuk_estimating_2009,
	address = {Berlin, Heidelberg},
	title = {Estimating {HMM} {Parameters} {Using} {Particle} {Swarm} {Optimisation}},
	isbn = {978-3-642-01129-0},
	abstract = {A Hidden Markov Model (HMM) is a powerful model in describing temporal sequences. The HMM parameters are usually estimated using Baum-Welch algorithm. However, it is well known that the Baum-Welch algorithm tends to arrive at local optimal points. In this report, we investigate the potential of the Particle Swarm Optimisation (PSO) as an alternative method for HMM parameters estimation. The domain in this study is the recognition of handwritten music notations. Three observables: (i) sequence of ink patterns, (ii) stroke information and (iii) spatial information associated with eight musical symbols were recorded. Sixteen HMM models were built from the data. Eight HMM models for eight musical symbols were built from the parameters estimated using the Baum-Welch algorithm and the other eight models were built from the parameters estimated using PSO. The experiment shows that the performances of HMM models, using parameters estimated from PSO and Baum-Welch approach, are comparable. We suggest that PSO or a combination of PSO and Baum-Welch algorithm could be alternative approaches for the HMM parameters estimation.},
	booktitle = {Applications of {Evolutionary} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Phon-Amnuaisuk, Somnuk},
	editor = {Giacobini, Mario and Brabazon, Anthony and Cagnoni, Stefano and Di Caro, Gianni A. and Ekárt, Anikó and Esparcia-Alcázar, Anna Isabel and Farooq, Muddassar and Fink, Andreas and Machado, Penousal},
	year = {2009},
	pages = {625--634},
}

@misc{presgurvic_songbook_2005,
	title = {Songbook {Romeo} \& {Julia}},
	url = {https://www.musicalvienna.at/de/souvenirs/12/ANDERE-MUSICALS/10/Songbook-Romeo-und-Julia},
	publisher = {Universal Music Publishing Limited, 8-9 Frith Street, London W1D 3JB},
	author = {Presgurvic, Gérard},
	editor = {Piano, German translation by Michaela Ronzoni; Arrangement Christian Kolonovits; and Gattringer, vocal arrangement Wolfgang},
	year = {2005},
	note = {Backup Publisher: Vereinigte Bühnen Wien
Place: Vienna, Austria},
}

@inproceedings{rashid_table_2017,
	title = {Table {Recognition} in {Heterogeneous} {Documents} {Using} {Machine} {Learning}},
	doi = {10.1109/ICDAR.2017.132},
	abstract = {Tables are an easy way to represent information in a structural form. Table recognition is important for the extraction of such information from document images. Usually, modern OCR systems provide textual information coming from tables without recognizing actual table structure. However, recognition of table structure is important to get the contextual meaning of the contents. Table structure recognition in heterogeneous documents is challenging due to a variety of table layouts. It becomes harder where no physical rulings are present in a table. This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images. Textual contents of documents are classified as table or non-table elements using a pre-trained neural network model. The output of the neural network is further enhanced by applying a contextual post processing on each element to correct the classifications errors if any. The system is trained using a subset of UNLV and UW3 document images and depicted more than 97\% accuracy on a test set in detection of table and non-table elements.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Rashid, Sheikh Faisal and Akmal, Abdullah and Adnan, Muhammad and Aslam, Ali Adnan and Dengel, Andreas},
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Neural networks, Training, Optical character recognition software, Feature extraction, Image recognition, Character recognition, optical character recognition, document image processing, neural nets, heterogeneous document images, heterogeneous documents, Layout, learning (artificial intelligence), modern OCR systems, non-table elements, nontable elements, table contents, table layouts, table recognition, table structure recognition},
	pages = {777--782},
}

@inproceedings{redmon_yolo9000_2017,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	doi = {10.1109/CVPR.2017.690},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Farhadi, Ali},
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Training, Feature extraction, Detectors, Object detection, object detection, image classification, COCO detection dataset, Image resolution, ImageNet classification dataset, ImageNet detection task, object classification, object detection system, PASCAL VOC, Real-time systems, YOLO detection method, YOLO9000, YOLOv2 model},
	pages = {6517--6525},
	file = {Redmon_Farhadi_2017_YOLO9000.pdf:/home/ptorras/zotpapers/Redmon_Farhadi_2017_YOLO9000.pdf:application/pdf},
}

@incollection{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {91--99},
	file = {Ren et al_2015_Faster R-CNN.pdf:/home/ptorras/zotpapers/Ren et al_2015_Faster R-CNN.pdf:application/pdf},
}

@inproceedings{rettinghaus_building_2021,
	address = {University of Alicante, Spain},
	title = {Building a {Comprehensive} {Sheet} {Music} {Library} {Application}},
	url = {https://hcommons.org/deposits/item/hc:45961/},
	doi = {10.17613/fc1c-mx52},
	abstract = {Digital symbolic music scores offer many benefits compared to paper-based scores, such as a flexible dynamic layout that allows adjustments of size and style, intelligent navigation features, automatic page-turning, onthe-fly modifications of the score including transposition into a different key, and rule-based annotations that can save hours of manual work by automatically highlighting relevant aspects in the score. However, most musicians still rely on paper because they don’t have access to a digital version of their sheet music, or their digital solution does not provide a satisfying experience. To bring digital scores to millions of musicians, we at Enote are building a mobile application that offers a comprehensive digital library of sheet music. These scores are obtained by a large-scale Optical Music Recognition process, combined with metadata collection and curation. Our material is stored in the MEI format and we rely on Verovio as a central component of our app to present scores and parts dynamically on mobile devices. This combination of the expressiveness of MEI with the beautiful engraving of Verovio allows us to create a flexible, mobile solution that we believe to be a powerful and true alternative to paper scores with practical features like smart annotations or instant transpositions. We also invest heavily into the open-source development of Verovio to make it the gold standard for rendering beautiful digital sheet music.},
	booktitle = {Music {Encoding} {Conference} {Proceedings} 2021},
	author = {Rettinghaus, Klaus and Pacha, Alexander},
	editor = {Münnich, Stefan and Rizo, David},
	year = {2021},
	file = {Rettinghaus_Pacha_2021_Building a Comprehensive Sheet Music Library Application.pdf:/home/ptorras/zotpapers/Rettinghaus_Pacha_2021_Building a Comprehensive Sheet Music Library Application.pdf:application/pdf},
}

@inproceedings{roman_and_2018,
	address = {Paris, France},
	title = {And {End}-{To}-{End} {Framework} for {Audio}-to-{Score} {Music} {Transcription} on {Monophonic} {Excerpts}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Román, Miguel A. and Pertusa, Antonio and Calvo-Zaragoza, Jorge},
	year = {2018},
	pages = {34--41},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Munich, Germany},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	booktitle = {18th {International} {Conference} on {Medical} {Image} {Computing} and {Computer}-{Assisted} {Intervention} ({MICCAI})},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	pages = {234--241},
	file = {Ronneberger et al_2015_U-Net.pdf:/home/ptorras/zotpapers/Ronneberger et al_2015_U-Net.pdf:application/pdf},
}

@book{sampson_writing_1985,
	title = {Writing {Systems}: {A} {Linguistic} {Introduction}},
	isbn = {978-0-8047-1756-4},
	url = {https://books.google.cz/books?id=tVcdNRvwoDkC},
	publisher = {Stanford University Press},
	author = {Sampson, Geoffrey},
	year = {1985},
	lccn = {84040708},
}

@incollection{sanchez_handwritten_2018,
	title = {Handwritten {Text} {Recognition} {Competitions} {With} the {tranScriptorium} {Dataset}},
	abstract = {The following sections are included: Introduction The Bentham Datasets Used in the HTR Contests Contest Protocol and Evaluation Methodology Existing State-of-the-Art Approaches HTR Approaches Employed in the Contests Presentation and Analysis of HTR Contest Results Remarks and Conclusions References},
	booktitle = {Document {Analysis} and {Text} {Recognition}},
	publisher = {World Scientific},
	author = {Sánchez, Joan Andreu and Romero, Verónica and Toselli, Alejandro H. and Vidal, Enrique},
	year = {2018},
	doi = {10.1142/9789813229273_0008},
	note = {Section: Chapter 8},
	pages = {213--239},
}

@article{sauvola_adaptive_2000,
	title = {Adaptive document image binarization},
	volume = {33},
	issn = {0031-3203},
	doi = {10.1016/S0031-3203(99)00055-2},
	abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.},
	number = {2},
	journal = {Pattern Recognition},
	author = {Sauvola, J. and Pietikäinen, M.},
	year = {2000},
	pmid = {84262800005},
	note = {ISBN: 0818678984},
	keywords = {document analysis, binarization, adaptive binarization, document segmentation, document understanding, soft decision},
	pages = {225--236},
}

@article{saxena_niblacks_2017,
	title = {Niblack's binarization method and its modifications to real-time applications: a review},
	doi = {10.1007/s10462-017-9574-2},
	journal = {Artificial Intelligence Review},
	author = {Saxena, Lalit Prakash},
	year = {2017},
	note = {Publisher: Springer Nature},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	url = {https://arxiv.org/pdf/1404.7828.pdf},
	journal = {Neural networks},
	author = {Schmidhuber, Jürgen},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {85--117},
	file = {Schmidhuber_2015_Deep learning in neural networks.pdf:/home/ptorras/zotpapers/Schmidhuber_2015_Deep learning in neural networks.pdf:application/pdf},
}

@book{schobrun_everything_2005,
	series = {Everything series},
	title = {The {Everything} {Reading} {Music} {Book}: {A} {Step}-{By}-{Step} {Introduction} to {Understanding} {Music} {Notation} {And} {Theory}},
	isbn = {1-59337-324-4},
	abstract = {This easy-to-follow guide helps you master the fundamentals of music notation and theory in no time.},
	publisher = {Adams Media},
	author = {Schobrun, Marc},
	year = {2005},
}

@book{selfridge-field_beyond_1997,
	address = {Cambridge, MA, USA},
	title = {Beyond {MIDI}: {The} {Handbook} of {Musical} {Codes}},
	isbn = {0-262-19394-9},
	url = {https://mitpress.mit.edu/books/beyond-midi},
	publisher = {MIT Press},
	author = {Selfridge-Field, Eleanor},
	year = {1997},
}

@article{sermanet_overfeat_2013,
	title = {Overfeat: {Integrated} recognition, localization and detection using convolutional networks},
	volume = {abs/1312.6229},
	url = {http://arxiv.org/abs/1312.6229},
	journal = {CoRR},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michaël and Fergus, Rob and LeCun, Yann},
	year = {2013},
	file = {Sermanet et al_2013_Overfeat.pdf:/home/ptorras/zotpapers/Sermanet et al_2013_Overfeat.pdf:application/pdf},
}

@article{serra_computational_2017,
	title = {The computational study of a musical culture through its digital traces},
	journal = {Acta Musicologica. 2017; 89 (1): 24-44.},
	author = {Serra, Xavier},
	year = {2017},
	note = {Publisher: International Musicological Society},
}

@inproceedings{shahab_open_2010,
	address = {Boston, Massachusetts, USA},
	title = {An {Open} {Approach} {Towards} the {Benchmarking} of {Table} {Structure} {Recognition} {Systems}},
	isbn = {978-1-60558-773-8},
	url = {http://doi.acm.org/10.1145/1815330.1815345},
	doi = {10.1145/1815330.1815345},
	booktitle = {9th {International} {Workshop} on {Document} {Analysis} {Systems}},
	publisher = {ACM},
	author = {Shahab, Asif and Shafait, Faisal and Kieninger, Thomas and Dengel, Andreas},
	year = {2010},
	keywords = {image segmentation, performance evaluation, table structure recognition, benchmarking, ground truth preparation, performance measures},
	pages = {113--120},
}

@article{siagian_rapid_2007,
	title = {Rapid {Biologically}-{Inspired} {Scene} {Classification} {Using} {Features} {Shared} with {Visual} {Attention}},
	volume = {29},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2007.40},
	abstract = {We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of early-visual features, which capture the "gist" of the scene into a low-dimensional signature vector. Distinct from previous approaches, the algorithm presents the advantage of being biologically plausible and of having low-computational complexity, sharing its low-level features with a model for visual attention that may operate concurrently on a robot. We compare classification accuracy using scenes filmed at three outdoor sites on campus (13,965 to 34,711 frames per site). Dividing each site into nine segments, we obtain segment classification rates between 84.21 percent and 88.62 percent. Combining scenes from all sites (75,073 frames in total) yields 86.45 percent correct classification, demonstrating the generalization and scalability of the approach},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Siagian, Christian and Itti, Laurent},
	year = {2007},
	keywords = {Image segmentation, Attention, Robustness, Mobile robots, image recognition, Object recognition, Computer vision, image classification, mobile robots, Educational institutions, Layout, Algorithms, Artificial Intelligence, Automated, biologically-inspired scene classification, Biomimetics, college campus, computational complexity, computational neuroscience, Computer Systems, Computer-Assisted, context-based scene recognition, Gist of a scene, Image Enhancement, Image Interpretation, image statistics, Imaging, Information Storage and Retrieval, mobile robotics, outdoor scenes, Pattern Recognition, robot localization., Robot sensing systems, robot vision, Robot vision systems, Robotics, saliency, scene recognition, Sonar navigation, Three-Dimensional, Visual Perception},
	pages = {300--312},
}

@article{sigtia_end--end_2015,
	title = {An {End}-to-{End} {Neural} {Network} for {Polyphonic} {Piano} {Music} {Transcription}},
	volume = {abs/1508.01774},
	url = {http://arxiv.org/abs/1508.01774},
	journal = {Computing Research Repository},
	author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
	year = {2015},
	file = {Sigtia et al_2015_An End-to-End Neural Network for Polyphonic Piano Music Transcription.pdf:/home/ptorras/zotpapers/Sigtia et al_2015_An End-to-End Neural Network for Polyphonic Piano Music Transcription.pdf:application/pdf},
}

@article{simard_best_2003,
	title = {Best practices for convolutional neural networks applied to visual document analysis},
	doi = {10.1109/ICDAR.2003.1227801},
	abstract = {Not Available},
	journal = {Seventh International Conference on Document Analysis and Recognition},
	author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John C.},
	year = {2003},
	note = {ISBN: 0-7695-1960-1},
	keywords = {Neural networks, Handwriting recognition, Text analysis, Support vector machines, CNN, Convolution, Best practices, Concrete, Industrial training, Information processing, Performance analysis},
	pages = {958--963},
}

@book{sloboda_exploring_2005,
	title = {Exploring the musical mind},
	url = {https://books.google.at/books?id=xNK6LTIW3D8C&printsec=frontcover#v=onepage&q&f=false},
	publisher = {Oxford University Press},
	author = {Sloboda, John},
	year = {2005},
	doi = {10.1093/acprof:oso/9780198530121.001.0001},
	doi = {10.1093/acprof:oso/9780198530121.001.0001},
	note = {Publication Title: Cognition, emotion, ability, function},
}

@misc{sachsische_landesbibliothek_staats-_2007,
	title = {Staats- und {Universitätsbibliothek} {Dresden}},
	url = {https://www.slub-dresden.de},
	author = {{Sächsische Landesbibliothek}},
	year = {2007},
}

@misc{smith_score_1987,
	title = {{SCORE}},
	url = {https://en.wikipedia.org/wiki/SCORE_(software)},
	author = {Smith, Leland},
	year = {1987},
}

@inproceedings{socher_parsing_2011,
	title = {Parsing {Natural} {Scenes} and {Natural} {Language} with {Recursive} {Neural} {Networks}},
	url = {http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf},
	booktitle = {28th {International} {Conference} on {Machine} {Learning}},
	author = {Socher, Richard and Lin, Cliff Chiung-Yu and Ng, Andrew Y. and Manning, Christopher D.},
	year = {2011},
	file = {Socher et al_2011_Parsing Natural Scenes and Natural Language with Recursive Neural Networks.pdf:/home/ptorras/zotpapers/Socher et al_2011_Parsing Natural Scenes and Natural Language with Recursive Neural Networks.pdf:application/pdf},
}

@inproceedings{sordo_analysis_2015,
	title = {Analysis of the {Evolution} of {Research} {Groups} and {Topics} in the {ISMIR} {Conference}},
	isbn = {978-84-606-8853-2},
	booktitle = {16th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Sordo, Mohamed and Ogihara, Mitsunori and Wuchty, Stefan},
	year = {2015},
}

@inproceedings{spreadbury_standard_2015,
	address = {Paris, France},
	title = {Standard {Music} {Font} {Layout} ({SMuFL})},
	isbn = {978-2-9552905-0-7},
	url = {http://tenor2015.tenor-conference.org/papers/24-Spreadbury-SMuFL.pdf},
	booktitle = {First {International} {Conference} on {Technologies} for {Music} {Notation} and {Representation} - {TENOR2015}},
	publisher = {Institut de Recherche en Musicologie},
	author = {Spreadbury, Daniel and Piéchaud, Robert},
	editor = {Battier, Marc and Bresson, Jean and Couprie, Pierre and Davy-Rigaux, Cécile and Fober, Dominique and Geslin, Yann and Genevois, Hugues and Picard, François and Tacaille, Alice},
	year = {2015},
	pages = {146--153},
}

@article{stadelmann_deep_2018,
	title = {Deep {Learning} in the {Wild}},
	url = {https://arxiv.org/abs/1807.04950},
	journal = {ArXiv e-prints},
	author = {Stadelmann, Thilo and Amirian, Mohammadreza and Arabaci, Ismail and Arnold, Marek and Duivesteijn, Gilbert François and Elezi, Isamil and Geiger, Melanie and Lörwald, Stefan and Meier, Benjamin Bruno and Rombach, Katharina and Tuggener, Lukas},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Stadelmann et al_2018_Deep Learning in the Wild.pdf:/home/ptorras/zotpapers/Stadelmann et al_2018_Deep Learning in the Wild.pdf:application/pdf},
}

@inproceedings{stamatopoulos_icdar2013_2013,
	title = {{ICDAR2013} {Handwriting} {Segmentation} {Contest}},
	doi = {10.1109/ICDAR.2013.283},
	booktitle = {2013 12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Stamatopoulos, Nikolaos and Gatos, Basilis and Louloudis, Georgios and Pal, Umpada and Alaei, Alireza},
	year = {2013},
}

@article{stauffer_keyword_2018,
	title = {Keyword spotting in historical handwritten documents based on graph matching},
	volume = {81},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320318301274},
	doi = {https://doi.org/10.1016/j.patcog.2018.04.001},
	abstract = {In the last decades historical handwritten documents have become increasingly available in digital form. Yet, the accessibility to these documents with respect to browsing and searching remained limited as full automatic transcription is often not possible or not sufficiently accurate. This paper proposes a novel reliable approach for template-based keyword spotting in historical handwritten documents. In particular, our framework makes use of different graph representations for segmented word images and a sophisticated matching procedure. Moreover, we extend our method to a spotting ensemble. In an exhaustive experimental evaluation on four widely used benchmark datasets we show that the proposed approach is able to keep up or even outperform several state-of-the-art methods for template- and learning-based keyword spotting.},
	journal = {Pattern Recognition},
	author = {Stauffer, Michael and Fischer, Andreas and Riesen, Kaspar},
	year = {2018},
	keywords = {Graph representation, Bipartite graph matching, Ensemble methods, Handwritten keyword spotting},
	pages = {240--253},
	file = {Stauffer et al_2018_Keyword spotting in historical handwritten documents based on graph matching.pdf:/home/ptorras/zotpapers/Stauffer et al_2018_Keyword spotting in historical handwritten documents based on graph matching.pdf:application/pdf},
}

@book{stone_music_1996,
	title = {Music {Notation} in the {Twentieth} {Century}, {A} {Practical} {Guidebook}},
	publisher = {W. W. Norton \& Company},
	author = {Stone, Kurt},
	year = {1996},
	keywords = {music notation},
}

@article{su_robust_2013,
	title = {Robust document image binarization technique for degraded document images},
	volume = {22},
	issn = {1057-7149},
	doi = {10.1109/TIP.2012.2231089},
	abstract = {Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 \& 2011 and handwritten-DIBCO 2010 and achieves accuracies of 93.5\%, 87.8\%, and 92.03\%, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Su, Bolan and Lu, Shijian and Tan, Chew Lim},
	year = {2013},
	pmid = {23221822},
	keywords = {document analysis, document image processing, binarization, Adaptive image contrast, degraded document image binarization, pixel classification},
	pages = {1408--1417},
}

@inproceedings{szegedy_inception-v4_2017,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14806},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI}-17)},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
	year = {2017},
}

@article{szeto_roles_2018,
	title = {The {Roles} of {Academic} {Libraries} in {Shaping} {Music} {Publishing} in the {Digital} {Age}},
	volume = {67},
	doi = {10.1353/lib.2018.0038},
	abstract = {Libraries are positioned at the nexus of creative production, music publishing, performance, and research. The academic library community has the potential to play an influential leadership role in shaping the music publishing life cycle, making scores more readily discoverable and accessible, and establishing itself as a force that empowers a wide range of creativity and scholarship. Yet the music publishing industry has been slow to capitalize on the digital market, and academic libraries have been slow to integrate electronic music scores into their collections. In this paper, I will discuss the historical, technical, and human factors that have contributed to this moment, and the critical next steps the academic library community can take in response to the booming digital music publishing market to make a lasting impact through setting technological standards and best practices, developing education in these technologies and related intellectual property issues, and becoming an active partner in digital music publishing and in innovative research and creative possibilities.},
	number = {2},
	journal = {Johns Hopkins University Press},
	author = {Szeto, Kimmy},
	year = {2018},
	pages = {303--318},
	file = {Szeto_2018_The Roles of Academic Libraries in Shaping Music Publishing in the Digital Age.pdf:/home/ptorras/zotpapers/Szeto_2018_The Roles of Academic Libraries in Shaping Music Publishing in the Digital Age.pdf:application/pdf},
}

@misc{toiviainen_midi_2016,
	title = {{MIDI} toolbox 1.1},
	url = {https://github.com/miditoolbox/},
	publisher = {GitHub},
	author = {Toiviainen, P. and Eerola, T.},
	year = {2016},
	note = {Publication Title: GitHub repository},
}

@article{trier_goal-directed_1995,
	title = {Goal-directed evaluation of binarization methods},
	volume = {17},
	issn = {0162-8828},
	doi = {10.1109/34.476511},
	abstract = {This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Trier, O. D. and Jain, A. K.},
	year = {1995},
	keywords = {Image segmentation, Text analysis, Image processing, Character recognition, Performance evaluation, document image processing, binarization, Image analysis, image segmentation, Computer vision, character recognition, performance evaluation, character recognition module, Data mining, digit image recognition, document handling, goal-directed evaluation, Humans, low-level image analysis, Machine vision, Niblack's method, scanned gray scale images, segmentation, two-level thresholding},
	pages = {1191--1201},
}

@article{trier_evaluation_1995,
	title = {Evaluation of binarization methods for utility map images},
	volume = {2},
	issn = {0162-8828},
	doi = {10.1109/ICIP.1994.413515},
	abstract = {This paper presents an evaluation of locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Such low quality occurs frequently in utility maps and excludes the use of global binarization methods. Only robust locally adaptive binarization methods with no need for on-line tuning of the parameters were considered since the gray scale images of utility maps often consist of a billion (10$^{\textrm{9}}$) pixels or more. Eight locally adaptive binarization methods were tested on five different images. The postprocessing step (PS) of Yanowitz and Bruckstein's (1989) method improved all the other best binarization methods. Niblack's (1986) method with PS gave the best performance. Eikvil, Taxt and Moen's (1991) method with PS, and Yanowitz and Bruckstein's method did almost as well. Comparison was also made on the CPU requirement},
	journal = {1st International Conference on Image Processing},
	author = {Trier, Øivind Due and Taxt, Torfinn},
	year = {1995},
	note = {ISBN: 0-8186-6952-7},
	keywords = {binarization, evaluation, document images, locally adaptive binarization, thresholding, utility maps},
	pages = {31--36},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/ uijlings/SelectiveSearch.html ).},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	year = {2013},
	pages = {154--171},
	file = {Uijlings et al_2013_Selective Search for Object Recognition.pdf:/home/ptorras/zotpapers/Uijlings et al_2013_Selective Search for Object Recognition.pdf:application/pdf},
}

@techreport{urbano_mirex_2010,
	title = {{MIREX} 2010 {Symbolic} {Melodic} {Similarity}: {Local} {Alignment} with {Geometric} {Representations}},
	url = {https://julian-urbano.info/files/publications/013-mirex-2010-symbolic-melodic-similarity-local-alignment-geometric-representations.pdf},
	institution = {Music Information Retrieval Evaluation eXchange},
	author = {Urbano, Julián and Lloréns, Juan and Morato, Jorge and Sánchez-Cuadrado, Sonia},
	year = {2010},
	file = {Urbano et al_2010_MIREX 2010 Symbolic Melodic Similarity.pdf:/home/ptorras/zotpapers/Urbano et al_2010_MIREX 2010 Symbolic Melodic Similarity.pdf:application/pdf},
}

@techreport{urbano_mirex_2011,
	title = {{MIREX} 2011 {Symbolic} {Melodic} {Similarity}: {Sequence} {Alignment} with {Geometric} {Representations}},
	url = {https://julian-urbano.info/files/publications/034-mirex-2011-symbolic-melodic-similarity-sequence-alignment-geometric-representations.pdf},
	institution = {Music Information Retrieval Evaluation eXchange},
	author = {Urbano, Julián and Lloréns, Juan and Morato, Jorge and Sánchez-Cuadrado, Sonia},
	year = {2011},
	file = {Urbano et al_2011_MIREX 2011 Symbolic Melodic Similarity.pdf:/home/ptorras/zotpapers/Urbano et al_2011_MIREX 2011 Symbolic Melodic Similarity.pdf:application/pdf},
}

@techreport{urbano_mirex_2012,
	title = {{MIREX} 2012 {Symbolic} {Melodic} {Similarity}: {Hybrid} {Sequence} {Alignment} with {Geometric} {Representations}},
	url = {https://julian-urbano.info/files/publications/049-mirex-2012-symbolic-melodic-similarity-hybrid-sequence-alignment-geometric-representations.pdf},
	institution = {Music Information Retrieval Evaluation eXchange},
	author = {Urbano, Julián and Lloréns, Juan and Morato, Jorge and Sánchez-Cuadrado, Sonia},
	year = {2012},
	file = {Urbano et al_2012_MIREX 2012 Symbolic Melodic Similarity.pdf:/home/ptorras/zotpapers/Urbano et al_2012_MIREX 2012 Symbolic Melodic Similarity.pdf:application/pdf},
}

@techreport{urbano_mirex_2013,
	title = {{MIREX} 2013 {Symbolic} {Melodic} {Similarity}: {A} {Geometric} {Model} supported with {Hybrid} {Sequence} {Alignment}},
	url = {http://hdl.handle.net/10230/27527},
	institution = {Music Information Retrieval Evaluation eXchange},
	author = {Urbano, Julián},
	year = {2013},
	file = {Urbano_2013_MIREX 2013 Symbolic Melodic Similarity.pdf:/home/ptorras/zotpapers/Urbano_2013_MIREX 2013 Symbolic Melodic Similarity.pdf:application/pdf},
}

@article{vajda_semi-automatic_2015,
	title = {Semi-automatic ground truth generation using unsupervised clustering and limited manual labeling: {Application} to handwritten character recognition},
	volume = {58},
	doi = {10.1016/j.patrec.2015.02.001},
	journal = {Pattern Recognition Letters},
	author = {Vajda, Szilárd and Rangoni, Yves and Cecotti, Hubert},
	year = {2015},
	note = {Publisher: Elsevier BV},
	pages = {23--28},
	file = {Vajda et al_2015_Semi-automatic ground truth generation using unsupervised clustering and.pdf:/home/ptorras/zotpapers/Vajda et al_2015_Semi-automatic ground truth generation using unsupervised clustering and.pdf:application/pdf},
}

@inproceedings{villegas_optical_2015,
	title = {Optical modelling and language modelling trade-off for {Handwritten} {Text} {Recognition}},
	doi = {10.1109/ICDAR.2015.7333878},
	abstract = {Training the models needed for Automatic Handwritten Text Recognition of historical documents generally requires a significant amount of human effort. This is mainly due to the great differences that often exist between collections and to the lack of linguistic resources from the period when the documents were written, which results in a need of manual data labelling effort. This paper presents a study on the reuse of models trained with data from a different collection, focusing on the contribution that the language model and the optical models have on the performance. An empirical evaluation is performed using data from Jeremy Bentham manuscripts with the aim of recognising a manuscript about a very different topic written by Jane Austen.},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Villegas, Mauricio and Sánchez, Joan Andreu and Vidal, Enrique},
	year = {2015},
	keywords = {Adaptation models, Integrated optics, optical character recognition, document image processing, history, Bentham Jeremy manuscript, handwritten text recognition, Handwritten Text Recognition, Integrated circuits, language modelling, Language Models, linguistic resource, linguistics, manual data labelling effort, Model Retraining, optical modelling, Optical Models, text detection},
	pages = {831--835},
}

@article{vonikakis_robust_2011,
	title = {Robust document binarization with {OFF} center-surround cells},
	volume = {14},
	issn = {1433-7541},
	doi = {10.1007/s10044-011-0214-1},
	abstract = {This paper presents a new method for degraded-document binarization, inspired by the attributes of the Human Visual System (HVS). It can deal with various types of degradations, such as uneven illumination, shadows, low contrast, smears, and heavy noise densities. The proposed algorithm combines the characteristics of the OFF center-surround cells of the HVS with the classic Otsu binarization technique. Cells of two different scales are combined, increasing the efficiency of the algorithm and reducing the extracted noise in the final output. A new response function, which regulates the output of the cell according to the local contrast and the local lighting conditions is also introduced. The Otsu technique is used to binarize the outputs of the OFF center-surround cells. Quantitative experiments performed on a set of various computer-generated degradations, such as noise, shadow, and low contrast demonstrate the superior performance of the proposed method against six other well-established techniques. Qualitative and OCR comparisons also confirm these results.},
	number = {3},
	journal = {Pattern Analysis and Applications},
	author = {Vonikakis, V. and Andreadis, I. and Papamarkos, N.},
	year = {2011},
	note = {ISBN: 3025410795},
	keywords = {binarization, Document binarization, Human visual system, OFF center-surround cells, Thresholding},
	pages = {219--234},
}

@article{vos_ascending_1989,
	title = {Ascending and {Descending} {Melodic} {Intervals}: {Statistical} {Findings} and their {Perceptual} {Relevance}},
	volume = {6},
	doi = {10.2307/40285439},
	number = {4},
	journal = {Music Perception},
	author = {Vos, Piet G. and Troost, Jim M.},
	year = {1989},
	pages = {383--396},
}

@inproceedings{wauthier_efficient_2013,
	title = {Efficient ranking from pairwise comparisons},
	booktitle = {30th {International} {Conference} on {Machine} {Learning}},
	author = {Wauthier, Fabian and Jordan, Michael and Jojic, Nebojsa},
	year = {2013},
	pages = {109--117},
}

@inproceedings{weigl_readwrite_2020,
	address = {New York, NY, USA},
	series = {{DLfM} 2020},
	title = {Read/{Write} {Digital} {Libraries} for {Musicology}},
	isbn = {978-1-4503-8760-6},
	url = {https://doi.org/10.1145/3424911.3425519},
	doi = {10.1145/3424911.3425519},
	abstract = {The Web and other digital technologies have democratised music creation, reception, and analysis, putting music in the hands, ears, and minds of billions of users. Music digital libraries typically focus on an essential subset of this deluge—commercial and academic publications, and historical materials—but neglect to incorporate contributions by scholars, performers, and enthusiasts, such as annotations or performed interpretations of these artifacts, despite their potential utility for many types of users. In this paper we consider means by which digital libraries for musicology may incorporate such contributions into their collections, adhering to principles of FAIR data management and respecting contributor rights as outlined in the EU’s General Data Protection Regulation. We present an overview of centralised and decentralised approaches to this problem, and propose hybrid solutions in which contributions reside in a) user-controlled personal online datastores, b) decentralised file storage, and c) are published and aggregated into digital library collections. We outline the implementation of these ideas using Solid, a Web decentralisation project building on W3C standard technologies to facilitate publication and control over Linked Data. We demonstrate the feasibility of this approach by implementing prototypes supporting two types of contribution: Web Annotations describing or analysing musical elements in score encodings and music recordings; and, music performances and associated metadata supporting performance analyses across many renditions of a given piece. Finally, we situate these ideas within a wider conception of enriched, decentralised, and interconnected online music repositories.},
	booktitle = {7th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Weigl, David M. and Goebl, Werner and Hofmann, Alex and Crawford, Tim and Zubani, Federico and Liem, Cynthia C. S. and Porter, Alastair},
	year = {2020},
	note = {event-place: Montréal, QC, Canada},
	keywords = {contribution, decentralisation, IPFS, Music Digital Libraries, Solid},
	pages = {48--52},
	file = {Weigl et al_2020_Read-Write Digital Libraries for Musicology.pdf:/home/ptorras/zotpapers/Weigl et al_2020_Read-Write Digital Libraries for Musicology.pdf:application/pdf},
}

@article{wilson_general_2003,
	title = {The general inefficiency of batch training for gradient descent learning},
	volume = {16},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608003001382},
	doi = {10.1016/S0893-6080(03)00138-2},
	abstract = {Abstract Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more ‘correct’ than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training—often orders of magnitude slower—especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy.},
	number = {10},
	journal = {Neural Networks},
	author = {Wilson, D. Randall and Martinez, Tony R.},
	year = {2003},
	keywords = {Batch training},
	pages = {1429--1451},
	file = {Wilson_Martinez_2003_The general inefficiency of batch training for gradient descent learning.pdf:/home/ptorras/zotpapers/Wilson_Martinez_2003_The general inefficiency of batch training for gradient descent learning.pdf:application/pdf},
}

@article{wittlich_system_1978,
	title = {A system for interactive encoding of music scores under computer control},
	volume = {12},
	issn = {1572-8412},
	url = {https://doi.org/10.1007/BF02400103},
	doi = {10.1007/BF02400103},
	number = {4},
	journal = {Computers and the Humanities},
	author = {Wittlich, Gary and Byrd, Donald and Nerheim, Rosalee},
	year = {1978},
	pages = {309--319},
}

@misc{wu_detectron2_2019,
	title = {Detectron2},
	url = {https://github.com/facebookresearch/detectron2},
	author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
	year = {2019},
}

@inproceedings{xia_improvised_2017,
	address = {Aalborg University Copenhagen, Denmark},
	title = {Improvised {Duet} {Interaction}: {Learning} {Improvisation} {Techniques} for {Automatic} {Accompaniment}},
	url = {http://homes.create.aau.dk/dano/nime17/papers/0022/paper0022.pdf},
	booktitle = {New {Interfaces} for {Musical} {Expression}},
	author = {Xia, Gus G. and Dannenberg, Roger B.},
	editor = {Erkut, Cumhur},
	year = {2017},
	file = {Xia_Dannenberg_2017_Improvised Duet Interaction.pdf:/home/ptorras/zotpapers/Xia_Dannenberg_2017_Improvised Duet Interaction.pdf:application/pdf},
}

@phdthesis{zhai_non-numerical_2010,
	address = {Hamilton, Ontario},
	type = {{PhD} {Thesis}},
	title = {Non-{Numerical} {Ranking} {Based} on {Pairwise} {Comparisons}},
	url = {http://hdl.handle.net/11375/19476},
	school = {McMaster University},
	author = {Zhai, Yun},
	year = {2010},
	file = {Zhai_2010_Non-Numerical Ranking Based on Pairwise Comparisons.pdf:/home/ptorras/zotpapers/Zhai_2010_Non-Numerical Ranking Based on Pairwise Comparisons.pdf:application/pdf},
}

@inproceedings{zhai_modeling_2012,
	title = {Modeling {Images} using {Transformed} {Indian} {Buffet} {Processes}},
	url = {http://icml.cc/2012/papers/750.pdf},
	booktitle = {29th {International} {Conference} on {Machine} {Learning}},
	publisher = {icml.cc / Omnipress},
	author = {Zhai, Ke and Hu, Yuening and Boyd-Graber, Jordan L. and Williamson, Sinead},
	year = {2012},
	file = {Zhai et al_2012_Modeling Images using Transformed Indian Buffet Processes.pdf:/home/ptorras/zotpapers/Zhai et al_2012_Modeling Images using Transformed Indian Buffet Processes.pdf:application/pdf},
}

@inproceedings{zhang_top-down_2016,
	address = {San Diego, California},
	title = {Top-down {Tree} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {http://www.aclweb.org/anthology/N16-1035},
	booktitle = {2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
	year = {2016},
	pages = {310--320},
	file = {Zhang et al_2016_Top-down Tree Long Short-Term Memory Networks.pdf:/home/ptorras/zotpapers/Zhang et al_2016_Top-down Tree Long Short-Term Memory Networks.pdf:application/pdf},
}

@article{zhang_watch_2017,
	title = {Watch, {Attend} and {Parse}: {An} {End}-to-end {Neural} {Network} {Based} {Approach} to {Handwritten} {Mathematical} {Expression} {Recognition}},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320317302376},
	doi = {10.1016/j.patcog.2017.06.017},
	abstract = {Abstract Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize \{HMEs\} in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes \{HME\} images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the \{CROHME\} international competition. Using the official training dataset, \{WAP\} significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55\% on \{CROHME\} 2014 and 44.55\% on \{CROHME\} 2016.},
	journal = {Pattern Recognition},
	author = {Zhang, Jianshu and Du, Jun and Zhang, Shiliang and Liu, Dan and Hu, Yulong and Hu, Jinshui and Wei, Si and Dai, Lirong},
	year = {2017},
	keywords = {Handwritten Mathematical Expression Recognition},
}

@inproceedings{zitnick_edge_2014,
	title = {Edge {Boxes}: {Locating} {Object} {Proposals} from {Edges}},
	url = {https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/},
	abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96\% object recall at overlap threshold of 0.5 and over 75\% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
	booktitle = {{ECCV}},
	publisher = {European Conference on Computer Vision},
	author = {Zitnick, Larry and Dollar, Piotr},
	year = {2014},
	file = {Zitnick_Dollar_2014_Edge Boxes.pdf:/home/ptorras/zotpapers/Zitnick_Dollar_2014_Edge Boxes.pdf:application/pdf},
}

@inproceedings{achankunju_music_2018,
	address = {Paris, France},
	title = {Music {Search} {Engine} from {Noisy} {OMR} {Data}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Achankunju, Sanu Pulimootil},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {23--24},
}

@inproceedings{adamska_mobile_2015,
	address = {Cham},
	title = {Mobile {System} for {Optical} {Music} {Recognition} and {Music} {Sound} {Generation}},
	isbn = {978-3-319-24369-6},
	doi = {10.1007/978-3-319-24369-6_48},
	abstract = {The paper presents a mobile system for generating a melody based on a photo of a musical score. The client-server architecture was applied. The client role is designated to a mobile application responsible for taking a photo of a score, sending it to the server for further processing and playing mp3 file received from the server. The server role is to recognize notes from the image, generate mp3 file and send it to the client application. The key element of the system is the program realizing the algorithm of notes recognition. It is based on the decision trees and characteristics of the individual symbols extracted from the image. The system is implemented in the Windows Phone 8 framework and uses a cloud operating system Microsoft Azure. It enables easy archivization of photos, recognized notes in the Music XML format and generated mp3 files. An easy transition to other mobile operating systems is possible as well as processing multiple music collections scans.},
	booktitle = {Computer {Information} {Systems} and {Industrial} {Management}},
	publisher = {Springer International Publishing},
	author = {Adamska, Julia and Piecuch, Mateusz and Podgórski, Mateusz and Walkiewicz, Piotr and Lukasik, Ewa},
	editor = {Saeed, Khalid and Homenda, Wladyslaw},
	year = {2015},
	pages = {571--582},
	file = {Adamska et al_2015_Mobile System for Optical Music Recognition and Music Sound Generation.pdf:/home/ptorras/zotpapers/Adamska et al_2015_Mobile System for Optical Music Recognition and Music Sound Generation.pdf:application/pdf},
}

@techreport{alfaro-contreras_reconocimiento_2020,
	title = {Reconocimiento holístico de partituras musicales},
	url = {https://rua.ua.es/dspace/bitstream/10045/108270/1/Reconocimiento_holistico_de_partituras_musicales.pdf},
	language = {Spanish},
	institution = {Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain},
	author = {Alfaro-Contreras, María and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	year = {2020},
	file = {Alfaro-Contreras et al_2020_Reconocimiento holistico de partituras musicales.pdf:/home/ptorras/zotpapers/Alfaro-Contreras et al_2020_Reconocimiento holistico de partituras musicales.pdf:application/pdf},
}

@inproceedings{alfaro-contreras_neural_2021,
	address = {Alicante, Spain},
	title = {Neural architectures for exploiting the components of {Agnostic} {Notation} in {Optical} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Alfaro-Contreras, María and Valero-Mas, Jose J. and Iñesta, José Manuel},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {33--37},
}

@article{alirezazadeh_effective_2014,
	title = {Effective staff line detection, restoration and removal approach for different quality of scanned handwritten music sheets},
	volume = {3},
	doi = {10.14419/jacst.v3i2.3196},
	number = {2},
	journal = {Journal of Advanced Computer Science \& Technology},
	author = {Alirezazadeh, Fatemeh and Ahmadzadeh, Mohammad Reza},
	year = {2014},
	note = {Publisher: Science Publishing Corporation},
	pages = {136--142},
	file = {Alirezazadeh_Ahmadzadeh_2014_Effective staff line detection, restoration and removal approach for different.pdf:/home/ptorras/zotpapers/Alirezazadeh_Ahmadzadeh_2014_Effective staff line detection, restoration and removal approach for different.pdf:application/pdf},
}

@inproceedings{andronico_automatic_1982,
	address = {Venice, Italy},
	title = {On {Automatic} {Pattern} {Recognition} and {Acquisition} of {Printed} {Music}},
	url = {http://hdl.handle.net/2027/spo.bbp2372.1982.024},
	booktitle = {International {Computer} {Music} {Conference}},
	publisher = {Michigan Publishing},
	author = {Andronico, Alfio and Ciampa, Alberto},
	year = {1982},
}

@inproceedings{anquetil_symbol_2000,
	address = {Berlin, Heidelberg},
	title = {A {Symbol} {Classifier} {Able} to {Reject} {Wrong} {Shapes} for {Document} {Recognition} {Systems}},
	isbn = {978-3-540-40953-3},
	url = {https://link.springer.com/chapter/10.1007%2F3-540-40953-X_17},
	abstract = {We propose in this paper a new framework to develop a transparent classifier able to deal with reject notions. The generated classifier can be characterized by a strong reliability without loosing good properties in generalization. We show on a musical scores recognition system that this classifier is very well suited to develop a complete document recognition system. Indeed this classifier allows them firstly to extract known symbols in a document (text for example) and secondly to validate segmentation hypotheses. Tests had been successfully performed on musical and digit symbols databases.},
	booktitle = {Graphics {Recognition} {Recent} {Advances}},
	publisher = {Springer Berlin Heidelberg},
	author = {Anquetil, Éric and Coüasnon, Bertrand and Dambreville, Frédéric},
	editor = {Chhabra, Atul K. and Dori, Dov},
	year = {2000},
	pages = {209--218},
}

@inproceedings{anstice_design_1996,
	title = {The design of a pen-based musical input system},
	doi = {10.1109/OZCHI.1996.560019},
	abstract = {Computerising the task of music editing can avoid a considerable amount of tedious work for musicians, particularly for tasks such as key transposition, part extraction, and layout. However the task of getting the music onto the computer can still be time consuming and is usually done with the help of bulky equipment. This paper reports on the design of a pen-based input system that uses easily-learned gestures to facilitate fast input, particularly if the system must be portable. The design is based on observations of musicians writing music by hand, and an analysis of the symbols in samples of music. A preliminary evaluation of the system is presented, and the speed is compared with the alternatives of handwriting, synthesiser keyboard input, and optical music recognition. Evaluations suggest that the gesture-based system could be approximately three times as fast as other methods of music data entry reported in the literature.},
	booktitle = {6th {Australian} {Conference} on {Computer}-{Human} {Interaction}},
	author = {Anstice, Jamie and Bell, Tim and Cockburn, Andy and Setchell, Martin},
	year = {1996},
	keywords = {Writing, Computer science, Handwriting recognition, Mice, optical music recognition, Proposals, Content based retrieval, Music information retrieval, gesture interface, handwriting, key transposition, Keyboards, light pens, Liquid crystal displays, music data entry, music editing, music layout, part extraction, pen-based musical input system, Portable computers, symbols, synthesiser keyboard input, time consuming},
	pages = {260--267},
}

@inproceedings{armand_musical_1993,
	title = {Musical score recognition: {A} hierarchical and recursive approach},
	doi = {10.1109/ICDAR.1993.395590},
	abstract = {Musical scores for live music show specific characteristics: large format, orchestral score, bad quality of (photo) copies. Moreover such music is generally handwritten. The author addresses the music recognition problem for such scores, and show a dedicated filtering that has been developed, both for segmentation and correction of copy defects. Recognition process involves geometrical and topographical parameters evaluation. The whole process (filtering + recognition) is recursively applied on images and sub-images, in a knowledge-based way.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {2nd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Armand, Jean-Pierre},
	year = {1993},
	keywords = {Image segmentation, Music, Image recognition, Head, music, Filtering, image recognition, segmentation, Classification algorithms, copy defects, correction, dedicated filtering, Digital filters, Electronic mail, EMP radiation effects, filtering, geometrical evaluation, handwritten, hierarchical, images, knowledge-based, large format, live music show specific characteristics, Morphology, music recognition, musical score recognition, orchestral score, recognition, recursive, sub-images, topographical parameters evaluation},
	pages = {906--909},
}

@misc{bitteur_audiveris_2004,
	title = {Audiveris},
	url = {https://github.com/audiveris},
	author = {Bitteur, Hervé},
	year = {2004},
}

@inproceedings{baba_gocen_2012,
	address = {New York, USA},
	title = {Gocen: {A} {Handwritten} {Notational} {Interface} for {Musical} {Performance} and {Learning} {Music}},
	isbn = {978-1-4503-1680-4},
	doi = {10.1145/2343456.2343465},
	booktitle = {{ACM} {SIGGRAPH} 2012 {Emerging} {Technologies}},
	publisher = {ACM},
	author = {Baba, Tetsuaki and Kikukawa, Yuya and Yoshiike, Toshiki and Suzuki, Tatsuhiko and Shoji, Rika and Kushiyama, Kumiko and Aoki, Makoto},
	year = {2012},
	pages = {9--9},
}

@article{bacon_recognising_1988,
	title = {Recognising music automatically},
	volume = {39},
	url = {http://stacks.iop.org/0031-9112/39/i=7/a=013},
	abstract = {Recognising characters typed in at a keyboard is a familiar task to most computers and one at which they excel, except that they (usually) insist on recognising what we have typed, rather than what we meant to type. A number of programs now on the market, however, go rather beyond merely recognising keystrokes on a keyboard, to actually recognising printed words on paper.},
	number = {7},
	journal = {Physics Bulletin},
	author = {Bacon, Richard A. and Carter, Nicholas Paul},
	year = {1988},
	pages = {265},
}

@misc{bainbridge_preliminary_1991,
	title = {Preliminary experiments in musical score recognition},
	publisher = {University of Edinburgh},
	author = {Bainbridge, David},
	year = {1991},
	note = {Place: Edinburgh, Scotland},
}

@techreport{bainbridge_complete_1994,
	title = {A complete optical music recognition system: {Looking} to the future},
	url = {https://ir.canterbury.ac.nz/handle/10092/14874},
	institution = {University of Canterbury},
	author = {Bainbridge, David},
	year = {1994},
	file = {Bainbridge_1994_A complete optical music recognition system.pdf:/home/ptorras/zotpapers/Bainbridge_1994_A complete optical music recognition system.pdf:application/pdf},
}

@techreport{bainbridge_optical_1994,
	title = {Optical music recognition: {Progress} report 1},
	url = {http://hdl.handle.net/10092/9670},
	institution = {Department of Computer Science, University of Canterbury},
	author = {Bainbridge, David},
	year = {1994},
	file = {Bainbridge_1994_Optical music recognition.pdf:/home/ptorras/zotpapers/Bainbridge_1994_Optical music recognition.pdf:application/pdf},
}

@article{bainbridge_extensible_1996,
	title = {An extensible optical music recognition system},
	volume = {18},
	url = {http://www.cs.waikato.ac.nz/~davidb/publications/acsc96/final.html},
	journal = {The Nineteenth Australasian computer science conference},
	author = {Bainbridge, David and Bell, Tim},
	year = {1996},
	note = {Publisher: University of Canterbury},
	pages = {308--317},
}

@inproceedings{bainbridge_dealing_1997,
	title = {Dealing with {Superimposed} {Objects} in {Optical} {Music} {Recognition}},
	isbn = {0 85296 692 X},
	doi = {10.1049/cp:19970997},
	abstract = {Optical music recognition (OMR) involves identifying musical symbols on a scanned sheet of music, and interpreting them so that the music can either be played by the computer, or put into a music editor. Applications include providing an automatic accompaniment, transposing or extracting parts for individual instruments, and performing an automated musicological analysis of the music. A key problem with music recognition, compared with character recognition, is that symbols very often overlap on the page. The most significant form of this problem is that the symbols are superimposed on a five-line staff. Although the staff provides valuable positional information, it creates ambiguity because it is difficult to determine whether a pixel would be black or white if the staff line was not there. The other main difference between music recognition and character recognition is the set of permissible symbols. In text, the alphabet size is fixed. Conversely, in music notation there is no standard "alphabet" of shapes, with composers inventing new notation where necessary, and music for particular instruments using specialised notation where appropriate. The focus of this paper is on techniques we have developed to deal with superimposed objects (6 Refs.) recognition},
	booktitle = {6th {International} {Conference} on {Image} {Processing} and its {Applications}},
	author = {Bainbridge, David and Bell, Tim},
	year = {1997},
	note = {ISSN: 0537-9989
Issue: 443},
	keywords = {optical music recognition, m, superimposed objects, to classify},
	pages = {756--760},
}

@phdthesis{bainbridge_extensible_1997,
	type = {{PhD} {Thesis}},
	title = {Extensible optical music recognition},
	url = {http://hdl.handle.net/10092/9420},
	school = {University of Canterbury},
	author = {Bainbridge, David},
	year = {1997},
	file = {Bainbridge_1997_Extensible optical music recognition.pdf:/home/ptorras/zotpapers/Bainbridge_1997_Extensible optical music recognition.pdf:application/pdf;Full Text PDF:/home/ptorras/Zotero/storage/BLXELAIZ/Bainbridge - 1997 - Extensible optical music recognition.pdf:application/pdf},
}

@incollection{bainbridge_automatic_1997,
	address = {Singapore},
	title = {Automatic reading of music notation},
	abstract = {The aim of Optical Music Recognition (OMR) is to convert optically scanned pages of music into a machine-readable format. In this tutorial level discussion of the topic, an historical background of work is presented, followed by a detailed explanation of the four key stages to an OMR system: stave line identification, musical object location, symbol identification, and musical understanding. The chapter also shows how recent work has addressed the issues of touching and fragmented objects—objectives that must be solved in a practical OMR system. The report concludes by discussing remaining problems, including measuring accuracy.},
	booktitle = {Handbook of {Character} {Recognition} and {Document} {Image} {Analysis}},
	publisher = {World Scientific},
	author = {Bainbridge, David and Carter, Nicholas Paul},
	editor = {Bunke, H. and Wang, P.},
	year = {1997},
	doi = {10.1142/9789812830968_0022},
	pages = {583--603},
}

@inproceedings{bainbridge_musical_1998,
	title = {Musical image compression},
	doi = {10.1109/DCC.1998.672149},
	abstract = {Optical music recognition aims to convert the vast repositories of sheet music in the world into an on-line digital format. In the near future it will be possible to assimilate music into digital libraries and users will be able to perform searches based on a sung melody in addition to typical text-based searching. An important requirement for such a system is the ability to reproduce the original score as accurately as possible. Due to the huge amount of sheet music available, the efficient storage of musical images is an important topic of study. This paper investigates whether the "knowledge" extracted from the optical music recognition (OMR) process can be exploited to gain higher compression than the JBIG international standard for bi-level image compression. We present a hybrid approach where the primitive shapes of music extracted by the optical music recognition process-note heads, note stems, staff lines and so forth-are fed into a graphical symbol based compression scheme originally designed for images containing mainly printed text. Using this hybrid approach the average compression rate for a single page is improved by 3.5\% over JBIG. When multiple pages with similar typography are processed in sequence, the file size is decreased by 4-8\%. The relevant background to both optical music recognition and textual image compression is presented. Experiments performed on 66 test images are described, outlining the combinations of parameters that were examined to give the best results.},
	booktitle = {Data {Compression} {Conference}},
	author = {Bainbridge, David and Inglis, Stuart},
	year = {1998},
	note = {ISSN: 1068-0314},
	keywords = {Image coding, Text recognition, Software libraries, Shape, Image recognition, optical music recognition, Ordinary magnetoresistance, digital libraries, Head, optical character recognition, Optical design, music, image coding, average compression rate, bi-level image compression, data compression, experiments, file size, graphical symbol based compression, hybrid approach, Image converters, Image storage, JBIG international standard, music score, musical image compression, musical image storage, note heads, note stems, on-line digital format, primitive shapes, printed text, sheet music, staff lines, text-based searching, textual image compression},
	pages = {209--218},
	file = {Bainbridge_Inglis_1998_Musical image compression.pdf:/home/ptorras/zotpapers/Bainbridge_Inglis_1998_Musical image compression.pdf:application/pdf},
}

@inproceedings{bainbridge_bulk_1999,
	title = {Bulk processing of optically scanned music},
	url = {http://digital-library.theiet.org/content/conferences/10.1049/cp_19990367},
	doi = {10.1049/cp:19990367},
	abstract = {For many years now optical music recognition (OMR) has been advocated as the leading methodology for transferring the vast repositories of music notation from paper to digital database. Other techniques exist for acquiring music on-line; however, these methods require operators with musical and computer skills. The notion, therefore, of an entirely automated process through OMR is highly attractive. It has been an active area of research since its inception in 1966 (Pruslin), and even though there has been the development of many systems with impressively high accuracy rates it is surprising to note that there is little evidence of large collections being processed with the technology-work by Carter (1994) and Bainbridge and Carter (1997) being the only known notable exception. This paper outlines some of the insights gained, and algorithms implemented, through the practical experience of converting collections in excess of 400 pages. In doing so, the work demonstrates that there are additional factors not currently considered by other research centres that are necessary for OMR to reach its full potential.},
	booktitle = {7th {International} {Conference} on {Image} {Processing} and its {Applications}},
	publisher = {Institution of Engineering and Technology},
	author = {Bainbridge, David and Wijaya, K.},
	year = {1999},
	keywords = {optical music recognition, OMR, music notation, bulk processing, digital database, optically scanned music},
	pages = {474--478},
}

@inproceedings{bainbridge_digital_2001,
	address = {Roanoke, Virginia, USA},
	title = {Digital {Music} {Libraries} — {Research} and {Development}},
	doi = {10.1145/379437.379765},
	booktitle = {1st {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}},
	author = {Bainbridge, David and Bernbom, Gerry and Davidson, Mary Wallace and Dillon, Andrew P. and Dovey, Matthey and Dunn, Jon W. and Fingerhut, Michael and Fujinaga, Ichiro and Isaacson, Eric J.},
	year = {2001},
	pages = {446--448},
	file = {Bainbridge et al_2001_Digital Music Libraries — Research and Development.pdf:/home/ptorras/zotpapers/Bainbridge et al_2001_Digital Music Libraries — Research and Development.pdf:application/pdf},
}

@article{bainbridge_music_2003,
	title = {A music notation construction engine for optical music recognition},
	volume = {33},
	issn = {1097-024X},
	doi = {10.1002/spe.502},
	abstract = {Optical music recognition (OMR) systems are used to convert music scanned from paper into a format suitable for playing or editing on a computer. These systems generally have two phases: recognizing the graphical symbols (such as note-heads and lines) and determining the musical meaning and relationships of the symbols (such as the pitch and rhythm of the notes). In this paper we explore the second phase and give a two-step approach that admits an economical representation of the parsing rules for the system. The approach is flexible and allows the system to be extended to new notations with little effort—the current system can parse common music notation, Sacred Harp notation and plainsong. It is based on a string grammar and a customizable graph that specifies relationships between musical objects. We observe that this graph can be related to printing as well as recognizing music notation, bringing the opportunity for cross-fertilization between the two areas of research. Copyright © 2003 John Wiley \& Sons, Ltd.},
	number = {2},
	journal = {Software: Practice and Experience},
	author = {Bainbridge, David and Bell, Tim},
	year = {2003},
	note = {Publisher: John Wiley \& Sons, Ltd.},
	keywords = {optical music recognition, definite clause grammars, graph traversal, music notation construction},
	pages = {173--200},
}

@inproceedings{bainbridge_identifying_2006,
	address = {Victoria, Canada},
	title = {Identifying music documents in a collection of images},
	url = {http://hdl.handle.net/10092/141},
	abstract = {Digital libraries and search engines are now well-equipped to find images of documents based on queries. Many images of music scores are now available, often mixed up with textual documents and images. For example, using the Google “images” search feature, a search for “Beethoven” will return a number of scores and manuscripts as well as pictures of the composer. In this paper we report on an investigation into methods to mechanically determine if a particular document is indeed a score, so that the user can specify that only musical scores should be returned. The goal is to find a minimal set of features that can be used as a quick test that will be applied to large numbers of documents. A variety of filters were considered, and two promising ones (run-length ratios and Hough transform) were evaluated. We found that a method based around run-lengths in vertical scans (RL) that out-performs a comparable algorithm using the Hough transform (HT). On a test set of 1030 images, RL achieved recall and precision of 97.8\% and 88.4\% respectively while HT achieved 97.8\% and 73.5\%. In terms of processor time, RL was more than five times as fast as HT.},
	booktitle = {7th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Bainbridge, David and Bell, Tim},
	year = {2006},
	pages = {47--52},
	file = {Bainbridge_Bell_2006_Identifying music documents in a collection of images.pdf:/home/ptorras/zotpapers/Bainbridge_Bell_2006_Identifying music documents in a collection of images.pdf:application/pdf},
}

@inproceedings{bainbridge_musical_2014,
	title = {A {Musical} {Progression} with {Greenstone}: {How} {Music} {Content} {Analysis} and {Linked} {Data} is {Helping} {Redefine} the {Boundaries} to a {Music} {Digital} {Library}},
	isbn = {978-1-4503-3002-2},
	doi = {10.1145/2660168.2660170},
	abstract = {Despite the recasting of the web's technical capabilities through Web 2.0, conventional digital library software architectures-from which many of our leading Music Digital Libraries (MDLs) are formed-result in digital resources that are, surprisingly, disconnected from other online sources of information, and embody a "read-only" mindset. Leveraging from Music Information Retrieval (MIR) techniques and Linked Open Data (LOD), in this paper we demonstrate a new form of music digital library that encompasses management, discovery, delivery, and analysis of the musical content it contains. Utilizing open source tools such as Greenstone, audioDB, Meandre, and Apache Jena we present a series of transformations to a musical digital library sourced from audio files that steadily increases the level of support provided to the user for musicological study. While the seed for this work was motivated by better supporting musicologists in a digital library, the developed software architecture alters the boundaries to what is conventionally thought of as a digital library- and in doing so challenges core assumptions made in mainstream digital library software design. Copyright 2014 ACM.},
	booktitle = {1st {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Bainbridge, David and Hu, Xiao and Downie, J. Stephen},
	year = {2014},
	keywords = {Music information retrieval, Open source software, Audio acoustics, Digital libraries, Digital music libraries, Embedded workflow, Linked open data (LOD), Music content analysis, Music digital libraries, Musicology analysis, Software architecture, Software design, Technical capabilities},
}

@inproceedings{balke_matching_2015,
	title = {Matching {Musical} {Themes} {Based} on {Noisy} {OCR} and {OMR} {Input}},
	isbn = {978-1-4673-6997-8},
	doi = {10.1109/ICASSP.2015.7178060},
	abstract = {In the year 1948, Barlow and Morgenstern published the book 'A Dictionary of Musical Themes', which contains 9803 important musical themes from the Western classical music literature. In this paper, we deal with the problem of automatically matching these themes to other digitally available sources. To this end, we introduce a processing pipeline that automatically extracts from the scanned pages of the printed book textual metadata using Optical Character Recognition (OCR) as well as symbolic note information using Optical Music Recognition (OMR). Due to the poor printing quality of the book, the OCR and OMR results are quite noisy containing numerous extraction errors. As one main contribution, we adjust alignment techniques for matching musical themes based on the OCR and OMR input. In particular, we show how the matching quality can be substantially improved by fusing the OCR- and OMR-based matching results. Finally, we report on our experiments within the challenging Barlow and Morgenstern scenario, which also indicates the potential of our techniques when considering other sources of musical themes such as digital music archives and the world wide web.},
	booktitle = {International {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Balke, Stefan and Achankunju, Sanu Pulimootil and Müller, Meinard},
	year = {2015},
	note = {ISSN: 1520-6149},
	keywords = {others},
	pages = {703--707},
}

@article{balke_bridging_2018,
	title = {Bridging the {Gap}: {Enriching} {YouTube} {Videos} with {Jazz} {Music} {Annotations}},
	volume = {5},
	issn = {2297-2668},
	doi = {10.3389/fdigh.2018.00001},
	abstract = {Web services allow permanent access to music from all over the world. Especially in the case of web services with user-supplied content, e.g., YouTube(TM), the available metadata is often incomplete or erroneous. On the other hand, a vast amount of high-quality and musically relevant metadata has been annotated in research areas such as Music Information Retrieval (MIR). Although they have great potential, these musical annotations are ofter inaccessible to users outside the academic world. With our contribution, we want to bridge this gap by enriching publicly available multimedia content with musical annotations available in research corpora, while maintaining easy access to the underlying data. Our web-based tools offer researchers and music lovers novel possibilities to interact with and navigate through the content. In this paper, we consider a research corpus called the Weimar Jazz Database (WJD) as an illustrating example scenario. The WJD contains various annotations related to famous jazz solos. First, we establish a link between the WJD annotations and corresponding YouTube videos employing existing retrieval techniques. With these techniques, we were able to identify 988 corresponding YouTube videos for 329 solos out of 456 solos contained in the WJD. We then embed the retrieved videos in a recently developed web-based platform and enrich the videos with solo transcriptions that are part of the WJD. Furthermore, we integrate publicly available data resources from the Semantic Web in order to extend the presented information, for example, with a detailed discography or artists-related information. Our contribution illustrates the potential of modern web-based technologies for the digital humanities, and novel ways for improving access and interaction with digitized multimedia content.},
	journal = {Frontiers in Digital Humanities},
	author = {Balke, Stefan and Dittmar, Christian and Abeßer, Jakob and Frieler, Klaus and Pfleiderer, Martin and Müller, Meinard},
	year = {2018},
	pages = {1--11},
	file = {Balke et al_2018_Bridging the Gap.pdf:/home/ptorras/zotpapers/Balke et al_2018_Bridging the Gap.pdf:application/pdf},
}

@inproceedings{baro_optical_2017,
	address = {Kyoto, Japan},
	title = {Optical {Music} {Recognition} by {Recurrent} {Neural} {Networks}},
	doi = {10.1109/ICDAR.2017.260},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE},
	author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
	year = {2017},
	note = {ISSN: 2379-2140},
	pages = {25--26},
}

@inproceedings{baro_starting_2018,
	address = {Paris, France},
	title = {A {Starting} {Point} for {Handwritten} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Baró, Arnau and Riba, Pau and Fornés, Alicia},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {5--6},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/ENRT58PY/Baró et al. - 2018 - A Starting Point for Handwritten Music Recognition.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/K56FSSFX/forum.html:text/html},
}

@inproceedings{baro_handwritten_2021,
	address = {Alicante, Spain},
	title = {Handwritten {Historical} {Music} {Recognition} through {Sequence}-to-{Sequence} with {Attention} {Mechanism}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Baró, Arnau and Badal, Carles and Torras, Pau and Fornés, Alicia},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {55--59},
}

@mastersthesis{baro-mas_optical_2017,
	title = {Optical {Music} {Recognition} by {Long} {Short}-{Term} {Memory} {Recurrent} {Neural} {Networks}},
	url = {http://www.cvc.uab.es/people/afornes/students/Master_ABaro2017.pdf},
	school = {Universitat Autònoma de Barcelona},
	author = {Baró-Mas, Arnau},
	year = {2017},
}

@inproceedings{barton_neumes_2002,
	title = {The {NEUMES} {Project}: digital transcription of medieval chant manuscripts},
	doi = {10.1109/WDM.2002.1176213},
	abstract = {This paper introduces the NEUMES Project from a top-down perspective. The purpose of the project is to design a software infrastructure for digital transcription of medieval chant manuscripts, such that transcriptions can be interoperable across many types of applications programs. Existing software for modern music does not provide an effective solution. A distributed library of chant document resources for the Web is proposed, to encompass photographic images, transcriptions, and searchable databases of manuscript descriptions. The NEUMES encoding scheme for chant transcription is presented, with NeumesXML serving as a 'wrapper' for transmission, storage, and editorial markup of transcription data. A scenario of use is given and future directions for the project are briefly discussed.},
	booktitle = {2nd {International} {Conference} on {Web} {Delivering} of {Music}},
	author = {Barton, Louis W. G.},
	year = {2002},
	keywords = {Encoding, Writing, Shape, history, music, Books, chant document resources, data structures, digital transcription, distributed library, editorial markup, encoding scheme, hypermedia markup languages, Inspection, Internet, interoperability, Libraries, Lighting, medieval chant manuscripts, NEUMES Project, NeumesXML, open systems, photographic images, Scholarships, searchable databases, software infrastructure, storage, transmission, Uncertainty, Web},
	pages = {211--218},
}

@inproceedings{barton_e-library_2005,
	address = {Denver, CO, USA},
	title = {E-library of {Medieval} {Chant} {Manuscript} {Transcriptions}},
	isbn = {1-58113-876-8},
	doi = {10.1145/1065385.1065458},
	booktitle = {5th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {ACM},
	author = {Barton, Louis W. G. and Caldwell, John A. and Jeavons, Peter G.},
	year = {2005},
	keywords = {digital libraries, XML, chant, comparison, data representation, medieval manuscripts, musical notation, search, transcription},
	pages = {320--329},
}

@incollection{baumann_transforming_1992,
	title = {Transforming {Printed} {Piano} {Music} into {MIDI}},
	abstract = {This paper decribes a recognition system for transforming printed piano music into the international standard MIDI for acoustic output generation. Because of the system is adapted for processing musical scores, it follows a top-down strategy in order to take advantage of the hierarchical structuring. Applying a decision tree classifier and various musical rules, the system comes up with a recognition rate of 80 to 100\% depending on the musical complexity of the input. The resulting symbolic representation in terms of so called MIDI-EVENTs can be easily understood by musical devices such as synthesizers, expanders, or keyboards.},
	booktitle = {Advances in {Structural} and {Syntactic} {Pattern} {Recognition}},
	publisher = {World Scientific},
	author = {Baumann, Stephan and Dengel, Andreas},
	year = {1992},
	doi = {10.1142/9789812797919_0030},
	pages = {363--372},
}

@techreport{baumann_document_1993,
	title = {Document recognition of printed scores and transformation into {MIDI}},
	institution = {Deutsches Forschungszentrum für Künstliche Intelligenz GmbH},
	author = {Baumann, Stephan},
	year = {1993},
	doi = {10.22028/D291-24925},
	file = {Baumann_1993_Document recognition of printed scores and transformation into MIDI.pdf:/home/ptorras/zotpapers/Baumann_1993_Document recognition of printed scores and transformation into MIDI.pdf:application/pdf},
}

@inproceedings{baumann_simplified_1995,
	title = {A {Simplified} {Attributed} {Graph} {Grammar} for {High}-{Level} {Music} {Recognition}},
	isbn = {0-8186-7128-9},
	doi = {10.1109/ICDAR.1995.602096},
	booktitle = {3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE},
	author = {Baumann, Stephan},
	year = {1995},
	keywords = {Writing, Artificial intelligence, Handwriting recognition, Music, Shape, Acoustic applications, Circuit testing, Multiple signal classification, Ordinary magnetoresistance, System testing},
	pages = {1080--1083},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/7LKZ7C2A/602096.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/H6WQ7DTK/Baumann - 1995 - A simplified attributed graph grammar for high-lev.pdf:application/pdf},
}

@inproceedings{baumann_report_1995,
	title = {Report of the line drawing and music recognition working group},
	doi = {10.1142/9789812797933},
	booktitle = {Document {Analysis} {Systems}},
	author = {Baumann, Stephan and Tombre, Karl},
	editor = {Spitz, A. Lawrence and Dengel, Andreas},
	year = {1995},
	pages = {1080--1083},
}

@inproceedings{bellini_optical_2001,
	title = {Optical music sheet segmentation},
	isbn = {0-7695-1284-4},
	doi = {10.1109/wdm.2001.990175},
	abstract = {The optical music recognition problem has been addressed in several ways, obtaining suitable results only when simple music constructs are processed. The most critical phase of the optical music recognition process is the first analysis of the image sheet. The first analysis consists of segmenting the acquired sheet into smaller parts which may be processed to recognize the basic symbols. The segmentation module of the O$^{\textrm{3}}$ MR system (Object Oriented Optical Music Recognition) system is presented. The proposed approach is based on the adoption of projections for the extraction of basic symbols that constitute a graphic element of the music notation. A set of examples is also included.},
	booktitle = {1st {International} {Conference} on {WEB} {Delivering} of {Music}},
	publisher = {Institute of Electrical \& Electronics Engineers (IEEE)},
	author = {Bellini, Pierfrancesco and Bruno, Ivan and Nesi, Paolo},
	year = {2001},
	pages = {183--190},
}

@incollection{bellini_off-line_2004,
	title = {An {Off}-{Line} {Optical} {Music} {Sheet} {Recognition}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IGI Global},
	author = {Bellini, Pierfrancesco and Bruno, Ivan and Nesi, Paolo},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch002},
	pages = {40--77},
}

@article{bellini_assessing_2007,
	title = {Assessing {Optical} {Music} {Recognition} {Tools}},
	volume = {31},
	doi = {10.1162/comj.2007.31.1.68},
	number = {1},
	journal = {Computer Music Journal},
	author = {Bellini, Pierfrancesco and Bruno, Ivan and Nesi, Paolo},
	year = {2007},
	note = {Publisher: MIT Press},
	pages = {68--93},
}

@incollection{bellini_optical_2008,
	address = {Hershey, PA, USA},
	title = {Optical {Music} {Recognition}: {Architecture} and {Algorithms}},
	url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-59904-150-6.ch005},
	abstract = {Optical music recognition is a key problem for coding western music sheets in the digital world. This problem has been addressed in several manners obtaining suitable results only when simple music constructs are processed. To this end, several different strategies have been followed, to pass from the simple music sheet image to a complete and consistent representation of music notation symbols (symbolic music notation or representation). Typically, image processing, pattern recognition and symbolic reconstruction are the technologies that have to be considered and applied in several manners the architecture of the so called OMR (Optical Music Recognition) systems. In this chapter, the O3MR (Object Oriented Optical Music Recognition) system is presented. It allows producing from the image of a music sheet the symbolic representation and save it in XML format (WEDELMUSIC XML and MUSICXML). The algorithms used in this process are those of the image processing, image segmentation, neural network pattern recognition, and symbolic reconstruction and reasoning. Most of the solutions can be applied in other field of image understanding. The development of the O3MR solution with all its algorithms has been partially supported by the European Commission, in the IMUTUS Research and Development project, while the related music notation editor has been partially funded by the research and development WEDELMUSIC project of the European Commission. The paper also includes a methodology for the assessment of other OMR systems. The set of metrics proposed has been used to assess the quality of results produce by the O3MR with respect the best OMR on market.},
	booktitle = {Interactive {Multimedia} {Music} {Technologies}},
	publisher = {IGI Global},
	author = {Bellini, Pierfrancesco and Bruno, Ivan and Nesi, Paolo},
	editor = {Ng, Kia and Nesi, Paolo},
	year = {2008},
	note = {ISSN: 9781599041506
Journal Abbreviation: Interactive Multimedia Music Technologies},
	pages = {80--110},
}

@inproceedings{beran_recognition_1999,
	title = {Recognition of {Printed} {Music} {Score}},
	isbn = {978-3-540-48097-6},
	doi = {10.1007/3-540-48097-8_14},
	abstract = {This article describes our implementation of the Optical Music Recognition System (OMR). The system implemented in our project is based on the binary neural network ADAM. ADAM has been used for recognition of music symbols. Preprocessing was implemented by conventional techniques. We decomposed the OMR process into several phases. The results of these phases are summarized.},
	booktitle = {Machine {Learning} and {Data} {Mining} in {Pattern} {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Beran, Tomáš and Macek, Tomáš},
	editor = {Perner, Petra and Petrou, Maria},
	year = {1999},
	pages = {174--179},
}

@inproceedings{blostein_template_1990,
	title = {Template matching for rhythmic analysis of music keyboard input},
	doi = {10.1109/ICPR.1990.118213},
	abstract = {A system that recognizes common rhythmic patterns through template matching is described. The use of template matching gives the user the unusual ability to modify the set of templates used for analysis. This modification effects a tradeoff between the temporal accuracy required of the input and the complexity of the recognizable rhythm patterns that happen to be common in a particular piece of music. The evolving implementation of this algorithm has received heavy use over a six-year period and has proven itself as a practical and reliable input method for fast music transcription. It is concluded that templates demonstrably provide the necessary temporal context for accurate rhythm recognition.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {10th {International} {Conference} on {Pattern} {Recognition}},
	author = {Blostein, Dorothea and Haken, Lippold},
	year = {1990},
	keywords = {Music, Rhythm, Multiple signal classification, Pattern recognition, music, Laboratories, computerised pattern recognition, Keyboards, acoustic signal processing, Computer errors, computerised signal processing, Councils, Information science, music keyboard input, music transcription, rhythmic pattern recognition, template matching, Timing},
	pages = {767--770},
}

@article{blostein_justification_1991,
	title = {Justification of {Printed} {Music}},
	volume = {34},
	issn = {0001-0782},
	doi = {10.1145/102868.102874},
	number = {3},
	journal = {Communications of the ACM},
	author = {Blostein, Dorothea and Haken, Lippold},
	year = {1991},
	note = {Place: New York, NY, USA
Publisher: ACM},
	pages = {88--99},
}

@incollection{blostein_critical_1992,
	title = {A {Critical} {Survey} of {Music} {Image} {Analysis}},
	isbn = {978-3-642-77281-8},
	abstract = {The research literature concerning the automatic analysis of images of printed and handwritten music notation, for the period 1966 through 1990, is surveyed and critically examined.},
	booktitle = {Structured {Document} {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blostein, Dorothea and Baird, Henry S.},
	year = {1992},
	doi = {10.1007/978-3-642-77281-8_19},
	pages = {405--434},
}

@incollection{blostein_recognition_1992,
	title = {Recognition of {Music} {Notation}: {SSPR}'90 {Working} {Group} {Report}},
	isbn = {978-3-642-77281-8},
	url = {https://doi.org/10.1007/978-3-642-77281-8_32},
	abstract = {This report summarizes the discussions of the Working Group on the Recognition of Music Notation, of the IAPR 1990 Workshop on Syntactic and Structural Pattern Recognition, Murray Hill, NJ, 13–15 June 1990. The participants were: D. Blostein, N. Carter, R. Haralick, T. Itagaki, H. Kato, H. Nishida, and R. Siromoney. The discussion was moderated by Nicholas Carter and recorded by Dorothea Blostein.},
	booktitle = {Structured {Document} {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blostein, Dorothea and Carter, Nicholas Paul},
	year = {1992},
	doi = {10.1007/978-3-642-77281-8_32},
	pages = {573--574},
}

@article{blostein_using_1999,
	title = {Using diagram generation software to improve diagram recognition: a case study of music notation},
	volume = {21},
	issn = {0162-8828},
	doi = {10.1109/34.809106},
	abstract = {Diagrams are widely used in society to transmit information such as circuit designs, music, mathematical formulae, architectural plans, and molecular structure. Computers must process diagrams both as images (marks on paper) and as information. A diagram recognizer translates from image to information and a diagram generator translates from information to image. Current technology for diagram generation is ahead of the technology for diagram recognition. Diagram generators have extensive knowledge of notational conventions which relate to readability and aesthetics, whereas current diagram recognizers focus on the hard constraints of the notation. To create a recognizer capable of exploiting layout information, it is expedient to reuse the expertise in existing diagram generators. In particular, we discuss the use of Lime (our editor and generator for music notation) to proofread and correct the raw output of MIDIScan (a third-party commercial recognizer for music notation). Over the past several years, this combination of software has been distributed to thousands of users.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Blostein, Dorothea and Haken, Lippold},
	year = {1999},
	keywords = {Mathematics, Image recognition, Multiple signal classification, Character recognition, document image processing, music, Image analysis, image recognition, music notation, correction, Computer errors, aesthetics, Circuit synthesis, Computer aided software engineering, diagram generation software, diagram recognition, diagrams, Error correction, layout information, Lime, MIDIScan, notational conventions, proofreading, raw output, readability, Software systems},
	pages = {1121--1136},
}

@inproceedings{bonnici_automatic_2018,
	address = {Halifax, NS, Canada},
	title = {Automatic {Ornament} {Localisation}, {Recognition} and {Expression} from {Music} {Sheets}},
	isbn = {978-1-4503-5769-2},
	url = {http://doi.acm.org/10.1145/3209280.3209536},
	doi = {10.1145/3209280.3209536},
	booktitle = {{ACM} {Symposium} on {Document} {Engineering}},
	publisher = {ACM},
	author = {Bonnici, Alexandra and Abela, Julian and Zammit, Nicholas and Azzopardi, George},
	year = {2018},
	pages = {25:1--25:11},
	file = {Bonnici et al_2018_Automatic Ornament Localisation, Recognition and Expression from Music Sheets.pdf:/home/ptorras/zotpapers/Bonnici et al_2018_Automatic Ornament Localisation, Recognition and Expression from Music Sheets.pdf:application/pdf},
}

@inproceedings{bountouridis_towards_2017,
	address = {Cham},
	title = {Towards {Polyphony} {Reconstruction} {Using} {Multidimensional} {Multiple} {Sequence} {Alignment}},
	isbn = {978-3-319-55750-2},
	doi = {10.1007/978-3-319-55750-2_3},
	abstract = {The digitization of printed music scores through the process of optical music recognition is imperfect. In polyphonic scores, with two or more simultaneous voices, errors of duration or position can lead to badly aligned and inharmonious digital transcriptions. We adapt biological sequence analysis tools as a post-processing step to correct the alignment of voices. Our multiple sequence alignment approach works on multiple musical dimensions and we investigate the contribution of each dimension to the correct alignment. Structural information, such musical phrase boundaries, is of major importance; therefore, we propose the use of the popular bioinformatics aligner Mafft which can incorporate such information while being robust to temporal noise. Our experiments show that a harmony-aware Mafft outperforms sophisticated, multidimensional alignment approaches and can achieve near-perfect polyphony reconstruction.},
	booktitle = {Computational {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
	publisher = {Springer International Publishing},
	author = {Bountouridis, Dimitrios and Wiering, Frans and Brown, Dan and Veltkamp, Remco C.},
	editor = {Correia, João and Ciesielski, Vic and Liapis, Antonios},
	year = {2017},
	pages = {33--48},
}

@inproceedings{bruder_towards_2003,
	address = {Berlin, Heidelberg},
	title = {Towards a {Digital} {Document} {Archive} for {Historical} {Handwritten} {Music} {Scores}},
	isbn = {978-3-540-24594-0},
	doi = {10.1007/978-3-540-24594-0_41},
	abstract = {Contemporary digital libraries and archives of music scores focus mainly on providing efficient storage and access methods for their data. However, digital archives of historical music scores can enable musicologists not only to easily store and access research material, but also to derive new knowledge from existing data. In this paper we present the first steps in building a digital archive of historical music scores from the 17th and 18th century. Along with the architectural and accessibility aspects of the system, we describe an integrated approach for classification and identification of the scribes of music scores.},
	booktitle = {Digital {Libraries}: {Technology} and {Management} of {Indigenous} {Knowledge} for {Global} {Access}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bruder, Ilvio and Finger, Andreas and Heuer, Andreas and Ignatova, Temenushka},
	editor = {Sembok, Tengku Mohd Tengku and Zaman, Halimah Badioze and Chen, Hsinchun and Urs, Shalini R. and Myaeng, Sung-Hyon},
	year = {2003},
	pages = {411--414},
}

@inproceedings{bugge_using_2011,
	title = {Using {Sequence} {Alignment} and {Voting} {To} {Improve} {Optical} {Music} {Recognition} {From} {Multiple} {Recognizers}},
	isbn = {978-0-615-54865-4},
	url = {http://www.ismir2011.ismir.net/papers/PS3-9.pdf},
	abstract = {Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third partyOMRtools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is sig- nificantly better than only 2 of the 4 commercial recogniz- ers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial im- provements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Bugge, Esben Paul and Juncher, Kim Lundsteen and Mathiasen, Brian Soborg and Simonsen, Jakob Grue},
	year = {2011},
	pages = {405--410},
}

@inproceedings{bui_staff_2014,
	title = {Staff {Line} {Removal} {Using} {Line} {Adjacency} {Graph} and {Staff} {Line} {Skeleton} for {Camera}-{Based} {Printed} {Music} {Scores}},
	doi = {10.1109/ICPR.2014.480},
	abstract = {On camera-based music scores, curved and uneven staff-lines tend to incur more frequently, and with the loss in performance of binarization methods, line thickness variation and space variation between lines are inevitable. We propose a novel and effective staff-line removal method based on following 3 main ideas. First, the state-of-the-art staff-line detection method, Stable Path, is used to extract staff-line skeletons of the music score. Second, a line adjacency graph (LAG) model is exploited in a different manner of over segmentation to cluster pixel runs generated from the run-length encoding (RLE) of the image. Third, a two-pass staff-line removal pipeline called filament filtering is applied to remove clusters lying on the staff-line. Our method shows impressive results on music score images captured from cameras, and gives high performance when applied to the ICDAR/GREC 2013 database.},
	booktitle = {22nd {International} {Conference} on {Pattern} {Recognition}},
	author = {Bui, Hoang-Nam and Na, Iin-Seop and Kim, Soo-Hyung},
	year = {2014},
	note = {ISSN: 1051-4651},
	keywords = {music score images, Text analysis, Databases, Music, optical music recognition, graph theory, music, image coding, image segmentation, Educational institutions, binarization methods, camera-based printed music scores, Cameras, cluster pixel, filament filtering, filtering theory, ICDAR-GREC 2013 database, image denoising, image RLE, LAG model, line adjacency graph, line thickness variation, music score recognition, over segmentation, run-length encoding, runlength codes, Skeleton, space variation, stable path, staff line skeleton, staff-line, staff-line detection method, two-pass staff-line removal pipeline, visual databases},
	pages = {2787--2789},
}

@inproceedings{bulis_computerized_1992,
	title = {Computerized recognition of hand-written musical notes},
	url = {http://hdl.handle.net/2027/spo.bbp2372.1992.029},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Bulis, Alex and Almog, Roy and Gerner, Moti and Shimony, Uri},
	year = {1992},
	pages = {110--112},
}

@article{bullen_bringing_2008,
	title = {Bringing {Sheet} {Music} to {Life}: {My} {Experiences} with {OMR}},
	volume = {3},
	issn = {1940-5758},
	url = {http://journal.code4lib.org/articles/84},
	number = {84},
	journal = {code4lib Journal},
	author = {Bullen, Andrew H.},
	year = {2008},
}

@inproceedings{burgoyne_comparative_2007,
	title = {A {Comparative} {Survey} of {Image} {Binarisation} {Algorithms} for {Optical} {Recognition} on {Degraded} {Musical} {Sources}},
	url = {http://ismir2007.ismir.net/proceedings/ISMIR2007_p509_burgoyne.pdf},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Burgoyne, John Ashley and Pugin, Laurent and Eustace, Greg and Fujinaga, Ichiro},
	year = {2007},
	file = {Burgoyne et al_2007_A Comparative Survey of Image Binarisation Algorithms for Optical Recognition.pdf:/home/ptorras/zotpapers/Burgoyne et al_2007_A Comparative Survey of Image Binarisation Algorithms for Optical Recognition.pdf:application/pdf},
}

@inproceedings{burgoyne_enhanced_2008,
	address = {Philadelphia, PA},
	title = {Enhanced {Bleedthrough} {Correction} for {Early} {Music} {Documents} with {Recto}-{Verso} {Registration}},
	url = {http://www.ismir2008.ismir.net/papers/ISMIR2008_221.pdf},
	booktitle = {9th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Burgoyne, John Ashley and Devaney, Johanna and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2008},
	pages = {407--412},
}

@inproceedings{burgoyne_lyric_2009,
	address = {Kobe, Japan},
	title = {Lyric {Extraction} and {Recognition} on {Digital} {Images} of {Early} {Music} {Sources}},
	url = {http://ismir2009.ismir.net/proceedings/OS8-3.pdf},
	booktitle = {10th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Burgoyne, John Ashley and Ouyang, Yue and Himmelman, Tristan and Devaney, Johanna and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2009},
	pages = {723--727},
	file = {Burgoyne et al_2009_Lyric Extraction and Recognition on Digital Images of Early Music Sources.pdf:/home/ptorras/zotpapers/Burgoyne et al_2009_Lyric Extraction and Recognition on Digital Images of Early Music Sources.pdf:application/pdf},
}

@incollection{burgoyne_music_2015,
	title = {Music {Information} {Retrieval}},
	isbn = {978-1-118-68060-5},
	abstract = {Music information retrieval (MIR) is "a multidisciplinary research endeavor that strives to develop innovative content-based searching schemes, novel interfaces, and evolving networked delivery mechanisms in an effort to make the world's vast store of music accessible to all." MIR was born from computational musicology in the 1960s and has since grown to have links with music cognition and audio engineering, a dedicated annual conference (ISMIR) and an annual evaluation campaign (MIREX). MIR combines machine learning with expert human knowledge to use digital music data - images of music scores, "symbolic" data such as MIDI files, audio, and metadata about musical items - for information retrieval, classification and estimation, or sequence labeling. This chapter gives a brief history of MIR, introduces classical MIR tasks from optical music recognition to music recommendation systems, and outlines some of the key questions and directions for future developments in MIR. © 2016 John Wiley \& Sons, Ltd.},
	booktitle = {A {New} {Companion} to {Digital} {Humanities}},
	publisher = {Wiley Blackwell},
	author = {Burgoyne, John Ashley and Fujinaga, Ichiro and Downie, J. Stephen},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	year = {2015},
	doi = {10.1002/9781118680605.ch15},
	note = {Journal Abbreviation: A New Companion to Digital Humanities},
	pages = {213--228},
}

@inproceedings{burlet_neonjs_2012,
	address = {Porto, Portugal},
	title = {Neon.js: {Neume} {Editor} {Online}},
	url = {http://ismir2012.ismir.net/event/papers/121_ISMIR_2012.pdf},
	booktitle = {13th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Burlet, Gregory and Porter, Alastair and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2012},
	pages = {121--126},
	file = {Burlet et al_2012_Neon.pdf:/home/ptorras/zotpapers/Burlet et al_2012_Neon.pdf:application/pdf},
}

@phdthesis{byrd_music_1984,
	type = {{PhD} {Thesis}},
	title = {Music {Notation} by {Computer}},
	url = {https://dl.acm.org/citation.cfm?id=911809},
	school = {Indiana University},
	author = {Byrd, Donald},
	year = {1984},
	keywords = {music notation},
}

@inproceedings{byrd_prospects_2006,
	title = {Prospects for {Improving} {OMR} with {Multiple} {Recognizers}},
	isbn = {1-55058-349-2},
	url = {http://ismir2006.ismir.net/PAPERS/ISMIR06155_Paper.pdf},
	booktitle = {7th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Byrd, Donald and Schindele, Megan},
	year = {2006},
	keywords = {classifier, omr, optical music recognition, to classify},
	pages = {41--46},
	file = {Byrd_Schindele_2006_Prospects for Improving OMR with Multiple Recognizers.pdf:/home/ptorras/zotpapers/Byrd_Schindele_2006_Prospects for Improving OMR with Multiple Recognizers2.pdf:application/pdf},
}

@inproceedings{byrd_studying_2009,
	address = {Wadern, Germany},
	series = {Dagstuhl {Seminar} {Proceedings}},
	title = {Studying {Music} is {Difficult} and {Important}: {Challenges} of {Music} {Knowledge} {Representation}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2009/1987},
	booktitle = {Knowledge representation for intelligent music processing},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany},
	author = {Byrd, Donald},
	editor = {Selfridge-Field, Eleanor and Wiering, Frans and Wiggins, Geraint A.},
	year = {2009},
	note = {Backup Publisher: Leibniz-Center for Informatics
ISSN: 1862-4405
Issue: 09051},
	file = {Byrd_2009_Studying Music is Difficult and Important.pdf:/home/ptorras/zotpapers/Byrd_2009_Studying Music is Difficult and Important.pdf:application/pdf},
}

@techreport{byrd_omr_2010,
	address = {Bloomington, IN, USA},
	title = {{OMR} {Evaluation} and {Prospects} for {Improved} {OMR} via {Multiple} {Recognizers}},
	url = {http://homes.soic.indiana.edu/donbyrd/MROMR2010Pap/OMREvaluation+Prospects4MROMR.doc},
	institution = {Indiana University},
	author = {Byrd, Donald and Guerin, William and Schindele, Megan and Knopke, Ian},
	year = {2010},
	note = {Backup Publisher: Indiana University},
}

@techreport{byrd_music_2016,
	title = {A {Music} {Representation} {Requirement} {Specification} for {Academia}},
	url = {http://homes.sice.indiana.edu/donbyrd/Papers/MusicRepReqForAcad.doc},
	institution = {Indiana University, Bloomington},
	author = {Byrd, Donald and Isaacson, Eric},
	year = {2016},
}

@inproceedings{calvo-zaragoza_recognition_2014,
	title = {Recognition of {Pen}-{Based} {Music} {Notation}: {The} {HOMUS} {Dataset}},
	doi = {10.1109/ICPR.2014.524},
	abstract = {A profitable way of digitizing a new musical composition is by using a pen-based (online) system, in which the score is created with the sole effort of the composition itself. However, the development of such systems is still largely unexplored. Some studies have been carried out but the use of particular little datasets has led to avoid objective comparisons between different approaches. To solve this situation, this work presents the Handwritten Online Musical Symbols (HOMUS) dataset, which consists of 15200 samples of 32 types of musical symbols from 100 different musicians. Several alternatives of recognition for the two modalities -online, using the strokes drawn by the pen, and offline, using the image generated after drawing the symbol- are also presented. Some experiments are included aimed to draw main conclusions about the recognition of these data. It is expected that this work can establish a binding point in the field of recognition of online handwritten music notation and serve as a baseline for future developments.},
	booktitle = {22nd {International} {Conference} on {Pattern} {Recognition}},
	publisher = {Institute of Electrical \& Electronics Engineers (IEEE)},
	author = {Calvo-Zaragoza, Jorge and Oncina, Jose},
	year = {2014},
	note = {ISSN: 1051-4651},
	keywords = {Hidden Markov models, Handwriting recognition, Music, Error analysis, Support vector machines, music, image recognition, handwritten character recognition, light pens, data recognition, FCC, handwritten online musical symbols dataset, HOMUS dataset, image generation, information retrieval, Kernel, musical composition digitization, online handwritten music notation recognition, online modality recognition, pen-based music notation recognition, symbol drawing},
	pages = {3038--3043},
	file = {Calvo-Zaragoza_Oncina_2014_Recognition of Pen-Based Music Notation.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza_Oncina_2014_Recognition of Pen-Based Music Notation.pdf:application/pdf},
}

@article{calvo-zaragoza_avoiding_2015,
	title = {Avoiding staff removal stage in optical music recognition: application to scores written in white mensural notation},
	volume = {18},
	issn = {1433-755X},
	doi = {10.1007/s10044-014-0415-5},
	abstract = {Staff detection and removal is one of the most important issues in optical music recognition (OMR) tasks since common approaches for symbol detection and classification are based on this process. Due to its complexity, staff detection and removal is often inaccurate, leading to a great number of errors in posterior stages. For this reason, a new approach that avoids this stage is proposed in this paper, which is expected to overcome these drawbacks. Our approach is put into practice in a case of study focused on scores written in white mensural notation. Symbol detection is performed by using the vertical projection of the staves. The cross-correlation operator for template matching is used at the classification stage. The goodness of our proposal is shown in an experiment in which our proposal attains an extraction rate of 96 \% and a classification rate of 92 \%, on average. The results found have reinforced the idea of pursuing a new research line in OMR systems without the need of the removal of staff lines.},
	number = {4},
	journal = {Pattern Analysis and Applications},
	author = {Calvo-Zaragoza, Jorge and Barbancho, Isabel and Tardón, Lorenzo J. and Barbancho, Ana M.},
	year = {2015},
	pages = {933--943},
	file = {Calvo-Zaragoza et al_2015_Avoiding staff removal stage in optical music recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2015_Avoiding staff removal stage in optical music recognition.pdf:application/pdf},
}

@article{calvo-zaragoza_clustering_2015,
	title = {Clustering of strokes from pen-based music notation: {An} experimental study},
	volume = {9117},
	issn = {0302-9743},
	doi = {10.1007/978-3-319-19390-8_71},
	abstract = {A comfortable way of digitizing a new music composition is by using a pen-based recognition system, in which the digital score is created with the sole effort of the composition itself. In this kind of systems, the input consist of a set of pen strokes. However, it is hitherto unclear the different types of strokes that must be considered for this task. This paper presents an experimental study on automatic labeling of these strokes using the well-known k-medoids algorithm. Since recognition of pen-based music scores is highly related to stroke recognition, it may be profitable to repeat the process when new data is received through user interaction. Therefore, our intention is not to propose some stroke labeling but to show which stroke dissimilarities perform better within the clustering process. Results show that there can be found good methods in the trade-off between cluster complexity and classification accuracy, whereas others offer a very poor performance. © Springer International Publishing Switzerland 2015.},
	journal = {Lecture Notes in Computer Science},
	author = {Calvo-Zaragoza, Jorge and Oncina, Jose},
	editor = {Cardoso J.S., Paredes R., Pardo X.M.},
	year = {2015},
	note = {ISBN: 9783319193892
Publisher: Springer Verlag},
	keywords = {Pattern recognition, Image analysis, Automatic labeling, Classification accuracy, Clustering algorithms, Clustering process, Economic and social effects, Graphical user interfaces, K-medoids algorithms, Music composition, Poor performance, Recognition systems, Stroke recognition},
	pages = {633--640},
}

@inproceedings{calvo-zaragoza_document_2016,
	address = {New York, USA},
	title = {Document {Analysis} for {Music} {Scores} via {Machine} {Learning}},
	isbn = {978-1-4503-4751-8},
	doi = {10.1145/2970044.2970047},
	booktitle = {3rd {International} workshop on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2016},
	note = {Backup Publisher: ACM},
	keywords = {Machine Learning, Optical Music Recognition, Document Analysis},
	pages = {37--40},
}

@inproceedings{calvo-zaragoza_two_2016,
	address = {New York City},
	title = {Two (note) heads are better than one: pen-based multimodal interaction with music scores},
	isbn = {978-0-692-75506-8},
	url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/006_Paper.pdf},
	booktitle = {17th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Calvo-Zaragoza, Jorge and Rizo, David and Iñesta, José Manuel},
	editor = {Devaney, J. et al.},
	year = {2016},
	pages = {509--514},
	file = {Calvo-Zaragoza et al_2016_Two (note) heads are better than one.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2016_Two (note) heads are better than one.pdf:application/pdf},
}

@article{calvo-zaragoza_music_2016,
	title = {Music staff removal with supervised pixel classification},
	volume = {19},
	doi = {10.1007/s10032-016-0266-2},
	abstract = {This work presents a novel approach to tackle the music staff removal. This task is devoted to removing the staff lines from an image of a music score while maintaining the symbol information. It represents a key step in the performance of most optical music recognition systems. In the literature, staff removal is usually solved by means of image processing procedures based on the intrinsics of music scores. However, we propose to model the problem as a supervised learning classification task. Surprisingly, although there is a strong background and a vast amount of research concerning machine learning, the classification approach has remained unexplored for this purpose. In this context, each foreground pixel is labelled as either staff or symbol. We use pairs of scores with and without staff lines to train classification algorithms. We test our proposal with several well-known classification techniques. Moreover, in our experiments no attempt of tuning the classification algorithms has been made, but the parameters were set to the default setting provided by the classification software libraries. The aim of this choice is to show that, even with this straightforward procedure, results are competitive with state-of-the-art algorithms. In addition, we also discuss several advantages of this approach for which conventional methods are not applicable such as its high adaptability to any type of music score.},
	number = {3},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Calvo-Zaragoza, Jorge and Micó, Luisa and Oncina, Jose},
	year = {2016},
	note = {Publisher: Springer},
	keywords = {Optical music recognition, Artificial intelligence, Image processing, Supervised learning, Classification algorithm, Classification approach, Classification tasks, Classification technique, Conventional methods, Learning systems, Optical data processing, Pixel classification, Pixels, State-of-the-art algorithms},
	pages = {211--219},
	file = {Calvo-Zaragoza et al_2016_Music staff removal with supervised pixel classification.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2016_Music staff removal with supervised pixel classification.pdf:application/pdf},
}

@article{calvo-zaragoza_staff-line_2017,
	title = {Staff-line detection and removal using a convolutional neural network},
	issn = {1432-1769},
	doi = {10.1007/s00138-017-0844-4},
	abstract = {Staff-line removal is an important preprocessing stage for most optical music recognition systems. Common procedures to solve this task involve image processing techniques. In contrast to these traditional methods based on hand-engineered transformations, the problem can also be approached as a classification task in which each pixel is labeled as either staff or symbol, so that only those that belong to symbols are kept in the image. In order to perform this classification, we propose the use of convolutional neural networks, which have demonstrated an outstanding performance in image retrieval tasks. The initial features of each pixel consist of a square patch from the input image centered at that pixel. The proposed network is trained by using a dataset which contains pairs of scores with and without the staff lines. Our results in both binary and grayscale images show that the proposed technique is very accurate, outperforming both other classifiers and the state-of-the-art strategies considered. In addition, several advantages of the presented methodology with respect to traditional procedures proposed so far are discussed.},
	journal = {Machine Vision and Applications},
	author = {Calvo-Zaragoza, Jorge and Pertusa, Antonio and Oncina, Jose},
	year = {2017},
	pages = {1--10},
	file = {Calvo-Zaragoza et al_2017_Staff-line detection and removal using a convolutional neural network.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2017_Staff-line detection and removal using a convolutional neural network.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_one-step_2017,
	address = {Suzhou, China},
	title = {One-step detection of background, staff lines, and symbols in medieval music manuscripts with convolutional neural networks},
	isbn = {978-981-11-5179-8},
	url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/162_Paper.pdf},
	booktitle = {18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
}

@inproceedings{calvo-zaragoza_machine_2017,
	address = {A Coruña, Spain},
	title = {A machine learning framework for the categorization of elements in images of musical documents},
	url = {http://www.udc.es/grupos/ln/tenor2017/sections/node/5-unified_categorization.pdf},
	booktitle = {3rd {International} {Conference} on {Technologies} for {Music} {Notation} and {Representation}},
	publisher = {University of A Coruña},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	file = {Calvo-Zaragoza et al_2017_A machine learning framework for the categorization of elements in images of.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2017_A machine learning framework for the categorization of elements in images of.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_recognition_2017,
	address = {Kyoto, Japan},
	title = {Recognition of {Handwritten} {Music} {Symbols} with {Convolutional} {Neural} {Codes}},
	doi = {10.1109/ICDAR.2017.118},
	abstract = {There are large collections of music manuscripts preserved over the centuries. In order to analyze these documents it is necessary to transcribe them into a machine-readable format. This process can be done automatically using Optical Music Recognition (OMR) systems, which typically consider segmentation plus classification workflows. This work is focused on the latter stage, presenting a comprehensive study for classification of handwritten musical symbols using Convolutional Neural Networks (CNN). The power of these models lies in their ability to transform the input into a meaningful representation for the task at hand, and that is why we study the use of these models to extract features (Neural Codes) for other classifiers. For the evaluation we consider four datasets containing different configurations and notation styles, along with a number of network models, different image preprocessing techniques and several supervised learning classifiers. Our results show that a remarkable accuracy can be achieved using the proposed framework, which significantly outperforms the state of the art in all datasets considered.},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Calvo-Zaragoza, Jorge and Gallego, Antonio-Javier and Pertusa, Antonio},
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical Music Recognition, Task analysis, Convolutional neural networks, Training, Feature extraction, Music, Support vector machines, Convolutional Neural Networks, optical character recognition, feature extraction, music, image segmentation, convolution, handwritten character recognition, image classification, neural nets, learning (artificial intelligence), machine-readable format, Convolutional codes, Handwritten Music Symbols, handwritten musical symbols, image representation, music manuscripts, Neural Codes, Optical Music Recognition systems},
	pages = {691--696},
}

@inproceedings{calvo-zaragoza_pixelwise_2017,
	title = {Pixelwise classification for music document analysis},
	doi = {10.1109/IPTA.2017.8310134},
	abstract = {Content within musical documents not only contains music symbol but also include different elements such as staff lines, text, or frontispieces. Before attempting to automatically recognize components in these layers, it is necessary to perform an analysis of the musical documents in order to detect and classify each of these constituent parts. The obstacle for this analysis is the high heterogeneity amongst music collections, especially with ancient documents, which makes it difficult to devise methods that can be generalizable to a broader range of sources. In this paper we propose a data-driven document analysis framework based on machine learning that focuses on classifying regions of interest at pixel level. For that, we make use of Convolutional Neural Networks trained to infer the category of each pixel. The main advantage of this approach is that it can be applied regardless of the type of document provided, as long as training data is available. Since this work represents first efforts in that direction, our experimentation focuses on reporting a baseline classification using our framework. The experiments show promising performance, achieving an accuracy around 90\% in two corpora of old music documents.},
	booktitle = {7th {International} {Conference} on {Image} {Processing} {Theory}, {Tools} and {Applications}},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	keywords = {Optical Music Recognition, Task analysis, Convolutional neural networks, Training, Text analysis, Music, Convolutional Neural Network, Layout, Pixel classification, Document Image Analysis, Microsoft Windows, Music Archives},
	pages = {1--6},
}

@inproceedings{calvo-zaragoza_pixel-wise_2017,
	title = {Pixel-wise binarization of musical documents with convolutional neural networks},
	doi = {10.23919/MVA.2017.7986876},
	booktitle = {15th {International} {Conference} on {Machine} {Vision} {Applications}},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	pages = {362--365},
}

@article{calvo-zaragoza_recognition_2017-1,
	title = {Recognition of pen-based music notation with finite-state machines},
	volume = {72},
	issn = {0957-4174},
	doi = {10.1016/j.eswa.2016.10.041},
	abstract = {This work presents a statistical model to recognize pen-based music compositions using stroke recognition algorithms and finite-state machines. The series of strokes received as input is mapped onto a stochastic representation, which is combined with a formal language that describes musical symbols in terms of stroke primitives. Then, a Probabilistic Finite-State Automaton is obtained, which defines probabilities over the set of musical sequences. This model is eventually crossed with a semantic language to avoid sequences that does not make musical sense. Finally, a decoding strategy is applied in order to output a hypothesis about the musical sequence actually written. Comprehensive experimentation with several decoding algorithms, stroke similarity measures and probability density estimators are tested and evaluated following different metrics of interest. Results found have shown the goodness of the proposed model, obtaining competitive performances in all metrics and scenarios considered.},
	journal = {Expert Systems with Applications},
	author = {Calvo-Zaragoza, Jorge and Oncina, Jose},
	year = {2017},
	keywords = {Optical music recognition, Finite-State machines, Pen-based recognition},
	pages = {395--406},
	file = {Calvo-Zaragoza_Oncina_2017_Recognition of pen-based music notation with finite-state machines.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza_Oncina_2017_Recognition of pen-based music notation with finite-state machines.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_staff-line_2017-1,
	address = {Cham},
	title = {Staff-{Line} {Detection} on {Grayscale} {Images} with {Pixel} {Classification}},
	isbn = {978-3-319-58838-4},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-319-58838-4_31},
	abstract = {Staff-line detection and removal are important processing steps in most Optical Music Recognition systems. Traditional methods make use of heuristic strategies based on image processing techniques with binary images. However, binarization is a complex process for which it is difficult to achieve perfect results. In this paper we describe a novel staff-line detection and removal method that deals with grayscale images directly. Our approach uses supervised learning to classify each pixel of the image as symbol, staff, or background. This classification is achieved by means of Convolutional Neural Networks. The features of each pixel consist of a square window from the input image centered at the pixel to be classified. As a case of study, we performed experiments with the CVC-Muscima dataset. Our approach showed promising performance, outperforming state-of-the-art algorithms for staff-line removal.},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	editor = {Alexandre, Luís A. and Salvador Sánchez, José and Rodrigues, João M. F.},
	year = {2017},
	pages = {279--286},
}

@inproceedings{calvo-zaragoza_music_2017,
	title = {Music {Document} {Layout} {Analysis} through {Machine} {Learning} and {Human} {Feedback}},
	volume = {02},
	doi = {10.1109/ICDAR.2017.259},
	abstract = {Music documents often include musical symbols as well as other relevant elements such as staff lines, text, and decorations. To detect and separate these constituent elements, we propose a layout analysis framework based on machine learning that focuses on pixel-level classification of the image. For that, we make use of supervised learning classifiers trained to infer the category of each pixel. In addition, our scenario considers a human-aided computing approach in which the user is part of the recognition loop, providing feedback where relevant errors are made.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, Jorge and Zhang, Ké and Saleh, Zeyad and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Machine Learning, Optical Music Recognition, Task analysis, Text analysis, Music, Multiple signal classification, document image processing, music, image recognition, image classification, Layout, learning (artificial intelligence), staff lines, Algorithm design and analysis, feedback, human feedback, Human-aided computing, human-aided computing approach, layout analysis framework, machine learning, Machine learning algorithms, music document layout analysis, Music Document Layout Analysis, music documents, musical symbols, pixel-level image classification, supervised learning classifiers},
	pages = {23--24},
}

@article{calvo-zaragoza_deep_2018,
	title = {Deep {Neural} {Networks} for {Document} {Processing} of {Music} {Score} {Images}},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/5/654},
	doi = {10.3390/app8050654},
	abstract = {There is an increasing interest in the automatic digitization of medieval music documents. Despite efforts in this field, the detection of the different layers of information on these documents still poses difficulties. The use of Deep Neural Networks techniques has reported outstanding results in many areas related to computer vision. Consequently, in this paper, we study the so-called Convolutional Neural Networks (CNN) for performing the automatic document processing of music score images. This process is focused on layering the image into its constituent parts (namely, background, staff lines, music notes, and text) by training a classifier with examples of these parts. A comprehensive experimentation in terms of the configuration of the networks was carried out, which illustrates interesting results as regards to both the efficiency and effectiveness of these models. In addition, a cross-manuscript adaptation experiment was presented in which the networks are evaluated on a different manuscript from the one they were trained. The results suggest that the CNN is capable of adapting its knowledge, and so starting from a pre-trained CNN reduces (or eliminates) the need for new labeled data.},
	number = {5},
	journal = {Applied Sciences},
	author = {Calvo-Zaragoza, Jorge and Castellanos, Francisco J. and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018},
	keywords = {music score images, optical music recognition, medieval manuscripts, convolutional neural networks, music document processing},
	file = {Calvo-Zaragoza et al_2018_Deep Neural Networks for Document Processing of Music Score Images.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2018_Deep Neural Networks for Document Processing of Music Score Images.pdf:application/pdf},
}

@inproceedings{calvo-zaragoza_camera-primus_2018,
	address = {Paris, France},
	title = {Camera-{PrIMuS}: {Neural} {End}-to-{End} {Optical} {Music} {Recognition} on {Realistic} {Monophonic} {Scores}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/33_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Calvo-Zaragoza, Jorge and Rizo, David},
	year = {2018},
	pages = {248--255},
}

@inproceedings{calvo-zaragoza_why_2018,
	address = {Paris, France},
	title = {Why {WoRMS}?},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {7--8},
}

@inproceedings{calvo-zaragoza_discussion_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Discussion {Group} {Summary}: {Optical} {Music} {Recognition}},
	isbn = {978-3-030-02283-9},
	doi = {10.1007/978-3-030-02284-6_12},
	abstract = {This document summarizes the discussion of the interest group on Optical Music Recognition (OMR) that took place in the 12th IAPR International Workshop on Graphics Recognition, and presents the main conclusions drawn during the session: OMR should revisit how it describes itself, and the OMR community should intensify its collaboration both internally and with other stakeholders.},
	booktitle = {Graphics {Recognition}, {Current} {Trends} and {Evolutions}},
	publisher = {Springer International Publishing},
	author = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	editor = {Fornés, Alicia and Bart, Lamiroy},
	year = {2018},
	pages = {152--157},
}

@article{calvo-zaragoza_handwritten_2019,
	title = {Handwritten {Music} {Recognition} for {Mensural} notation with convolutional recurrent neural networks},
	volume = {128},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865519302338},
	doi = {https://doi.org/10.1016/j.patrec.2019.08.021},
	abstract = {Optical Music Recognition is the technology that allows computers to read music notation, which is also referred to as Handwritten Music Recognition when it is applied over handwritten notation. This technology aims at efficiently transcribing written music into a representation that can be further processed by a computer. This is of special interest to transcribe the large amount of music written in early notations, such as the Mensural notation, since they represent largely unexplored heritage for the musicological community. Traditional approaches to this problem are based on complex strategies with many explicit rules that only work for one particular type of manuscript. Machine learning approaches offer the promise of generalizable solutions, based on learning from just labelled examples. However, previous research has not achieved sufficiently acceptable results for handwritten Mensural notation. In this work we propose the use of deep neural networks, namely convolutional recurrent neural networks, which have proved effective in other similar domains such as handwritten text recognition. Our experimental results achieve, for the first time, recognition results that can be considered effective for transcribing handwritten Mensural notation, decreasing the symbol-level error rate of previous approaches from 25.7\% to 7.0\%.},
	journal = {Pattern Recognition Letters},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	year = {2019},
	keywords = {Convolutional recurrent neural networks, Handwritten Music Recognition, Mensural notation, Optical Music Recognition},
	pages = {115--121},
	file = {Calvo-Zaragoza et al_2019_Handwritten Music Recognition for Mensural notation with convolutional.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2019_Handwritten Music Recognition for Mensural notation with convolutional.pdf:application/pdf;Full Text:/home/ptorras/Zotero/storage/WKARVBZF/Calvo-Zaragoza et al. - 2019 - Handwritten Music Recognition for Mensural notatio.pdf:application/pdf},
}

@inproceedings{bosch_campos_sheet_2016,
	title = {Sheet {Music} {Statistical} {Layout} {Analysis}},
	doi = {10.1109/ICFHR.2016.0066},
	abstract = {In order to provide access to the contents of ancient music scores to researchers, the transcripts of both the lyrics and the musical notation is required. Before attempting any type of automatic or semi-automatic transcription of sheet music, an adequate layout analysis (LA) is needed. This LA must provide not only the locations of the different image regions, but also adequate region labels to distinguish between different region types such as staff, lyric, etc. To this end, we adapt a stochastic framework for LA based on Hidden Markov Models that we had previously introduced for detection and classification of text lines in typical handwritten text images. The proposed approach takes a scanned music score image as input and, after basic preprocessing, simultaneously performs region detection and region classification in an integrated way. To assess this statistical LA approach several experiments were carried out on a representative sample of a historical music archive, under different difficulty settings. The results show that our approach is able to tackle these structured documents providing good results not only for region detection but also for classification of the different regions.},
	booktitle = {15th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition}},
	author = {Bosch Campos, Vicente and Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal Ruiz, Enrique},
	year = {2016},
	note = {ISSN: 2167-6445},
	keywords = {Hidden Markov models, Training, Text recognition, Handwriting recognition, Feature extraction, Hidden Markov Models, Music, handwriting recognition, hidden Markov models, Document Layout Analysis, statistical analysis, hidden Markov model, music, handwritten character recognition, image classification, Layout, text detection, sheet music, handwritten text image, statistical LA, statistical layout analysis, text line classification, text line detection, text region detection},
	pages = {313--318},
}

@inproceedings{capela_integrated_2008,
	title = {Integrated recognition system for music scores},
	url = {http://hdl.handle.net/2027/spo.bbp2372.2008.114},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Capela, Artur and Cardoso, Jamie dos Santos and Rebelo, Ana and Guedes, Carlos},
	year = {2008},
	pages = {3--6},
}

@inproceedings{capela_staff_2008,
	title = {Staff {Line} {Detection} and {Removal} with {Stable} {Paths}},
	url = {http://www.inescporto.pt/~arebelo/publications/2008ACapelaSIGMAP.pdf},
	booktitle = {International {Conference} on {Signal} {Processing} and {Multimedia} {Applications}},
	author = {Capela, Artur and Rebelo, Ana and Cardoso, Jamie dos Santos and Guedes, Carlos},
	year = {2008},
}

@misc{capella-software_ag_capella_1996,
	title = {Capella {Scan}},
	url = {https://www.capella-software.com},
	author = {{capella-software AG}},
	year = {1996},
}

@inproceedings{cardoso_connected_2008,
	title = {A connected path approach for staff detection on a music score},
	doi = {10.1109/ICIP.2008.4711927},
	abstract = {The preservation of many music works produced in the past entails their digitalization and consequent accessibility in an easy-to-manage digital format. Carrying this task manually is very time consuming and error prone. While optical music recognition systems usually perform well on printed scores, the processing of handwritten musical scores by computers remain far from ideal. One of the fundamental stages to carry out this task is the staff line detection. In this paper a new method for the automatic detection of music staff lines based on a connected path approach is presented. Lines affected by curvature, discontinuities, and inclination are robustly detected. Experimental results show that the proposed technique consistently outperforms well-established algorithms.},
	booktitle = {15th {International} {Conference} on {Image} {Processing}},
	author = {Cardoso, Jamie dos Santos and Capela, Artur and Rebelo, Ana and Guedes, Carlos},
	year = {2008},
	note = {ISSN: 1522-4880},
	keywords = {Optical character recognition software, Handwriting recognition, Music, Detectors, optical music recognition, Multiple signal classification, Ordinary magnetoresistance, handwriting recognition, Biomedical optical imaging, Optical computing, optical character recognition, document image processing, music, Image analysis, image recognition, object detection, Computer errors, connected path approach, Cultural differences, curvature detection, handwritten musical score, image analysis, staff line detection},
	pages = {1005--1008},
}

@article{cardoso_staff_2009,
	title = {Staff {Detection} with {Stable} {Paths}},
	volume = {31},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2009.34},
	abstract = {The preservation of musical works produced in the past requires their digitalization and transformation into a machine-readable format. The processing of handwritten musical scores by computers remains far from ideal. One of the fundamental stages to carry out this task is the staff line detection. We investigate a general-purpose, knowledge-free method for the automatic detection of music staff lines based on a stable path approach. Lines affected by curvature, discontinuities, and inclination are robustly detected. Experimental results show that the proposed technique consistently outperforms well-established algorithms.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cardoso, Jamie dos Santos and Capela, Artur and Rebelo, Ana and Guedes, Carlos and Pinto da Costa, Joaquim},
	year = {2009},
	note = {Publisher: Institute of Electrical \& Electronics Engineers (IEEE)},
	keywords = {Writing, Optical character recognition software, Robustness, optical music recognition, Ordinary magnetoresistance, Character recognition, optical character recognition, Degradation, document image processing, music, Music information retrieval, Image analysis, handwritten character recognition, machine-readable format, image analysis, staff line detection, 0, automatic detection, Design methodology, digitalization, Document image processing, handwritten musical scores processing, music staff lines, stable path approach},
	pages = {1134--1139},
}

@article{carter_acquisition_1988,
	title = {The acquisition, representation and reconstruction of printed music by computer: {A} review},
	volume = {22},
	issn = {1572-8412},
	url = {https://doi.org/10.1007/BF00057651},
	doi = {10.1007/BF00057651},
	abstract = {Material published on the subject of Acquisition, Representation and Reconstruction of printed music by computer is reviewed.},
	number = {2},
	journal = {Computers and the Humanities},
	author = {Carter, Nicholas Paul and Bacon, Richard A. and Messenger, T.},
	year = {1988},
	pages = {117--136},
}

@incollection{carter_automatic_1992,
	address = {Berlin, Heidelberg},
	title = {Automatic {Recognition} of {Printed} {Music}},
	isbn = {978-3-642-77281-8},
	url = {https://doi.org/10.1007/978-3-642-77281-8_21},
	abstract = {There is a need for an automatic recognition system for printed music scores. The work presented here forms the basis of an omnifont, size-independent system with significant tolerance of noise and rotation of the original image. A structural decomposition technique is used based on an original transformation of the line adjacency graph. An example of output is given in the form of a data file and its score reconstruction.},
	booktitle = {Structured {Document} {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carter, Nicholas Paul and Bacon, Richard A.},
	year = {1992},
	doi = {10.1007/978-3-642-77281-8_21},
	pages = {456--465},
}

@incollection{carter_new_1992,
	title = {A {New} {Edition} of {Walton}'s {Façade} {Using} {Automatic} {Score} {Recognition}},
	abstract = {The availability of an automatic recognition system for printed music will facilitate applications such as musicological analysis, point-of-sale printing, creation of large format or braille scores and computer-based production of new editions. An example of the last of these possibilities is described here. A score-reading system is under development which makes use of a structural decomposition technique that is intended to be tolerant of significant variation in font, size, notation and noise in the source images. A description is given of the first "real-world" task to be undertaken using the system, i.e. the production of a new edition of Façade by William Walton. Sample output files and their corresponding reconstructions are given together with a discussion of the problems involved and the implications for future work.},
	booktitle = {Advances in {Structural} and {Syntactic} {Pattern} {Recognition}},
	publisher = {World Scientific},
	author = {Carter, Nicholas Paul},
	year = {1992},
	doi = {10.1142/9789812797919_0029},
	pages = {352--362},
}

@article{carter_segmentation_1992,
	title = {Segmentation and preliminary recognition of madrigals notated in white mensural notation},
	volume = {5},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/BF02627000},
	doi = {10.1007/BF02627000},
	abstract = {An automatic music score-reading system will facilitate applications including computer-based editing of new editions, production of databases for musicological research, and creation of braille or large-format scores for the blind or partially-sighted. The work described here deals specifically with initial processing of images containing early seventeenth century madrigals notated in white mensural notation. The problems of segmentation involved in isolating the musical symbols from the word-underlay and decorative graphics are compounded by the poor quality of the originals which present a significant challenge to any recognition system. The solution described takes advantage of structural decomposition techniques based on a novel transformation of the line adjacency graph which have been developed during work on a score-reading system for conventional music notation.},
	number = {3},
	journal = {Machine Vision and Applications},
	author = {Carter, Nicholas Paul},
	year = {1992},
	pages = {223--229},
}

@inproceedings{carter_conversion_1994,
	title = {Conversion of the {Haydn} symphonies into electronic form using automatic score recognition: a pilot study},
	url = {https://doi.org/10.1117/12.171115},
	doi = {10.1117/12.171115},
	abstract = {As part of the development of an automatic recognition system for printed music scores, a series of `real-world' tasks are being undertaken. The first of these involves the production of a new edition of an existing 104-page, engraved, chamber-music score for Oxford University Press. The next substantial project, which is described here, has begun with a pilot study with a view to conversion of the 104 Haydn symphonies from a printed edition into machine- readable form. The score recognition system is based on a structural decomposition approach which provides advantages in terms of speed and tolerance of significant variations in font, scale, rotation and noise. Inevitably, some editing of the output data files is required, partially due to the limited vocabulary of symbols supported by the system and their permitted superimpositions. However, the possibility of automatically processing the bulk of the contents of over 600 pages of orchestral score in less than a day of compute time makes the conversion task manageable. The influence that this undertaking is having on the future direction of system development also is discussed.},
	booktitle = {International {Symposium} on {Electronic} {Imaging}: {Science} and {Technology}},
	author = {Carter, Nicholas Paul},
	year = {1994},
	pages = {2181 -- 2181 -- 12},
}

@inproceedings{castellanos_document_2018,
	address = {Paris, France},
	title = {Document {Analysis} of {Music} {Score} {Images} with {Selectional} {Auto}-{Encoders}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/93_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Castellanos, Fancisco J. and Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018},
	pages = {256--263},
}

@article{castellanos_automatic_2020,
	title = {Automatic scale estimation for music score images},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417420304140},
	doi = {10.1016/j.eswa.2020.113590},
	abstract = {Optical Music Recognition (OMR) is the research field focused on the automatic reading of music from scanned images. Its main goal is to encode the content into a digital and structured format with the advantages that this entails. This discipline is traditionally aligned to a workflow whose first step is the document analysis. This step is responsible of recognizing and detecting different sources of information—e.g. music notes, staff lines and text—to extract them and then processing automatically the content in the following steps of the workflow. One of the most difficult challenges it faces is to provide a generic solution to analyze documents with diverse resolutions. The endless number of existing music sources does not meet a standard that normalizes the data collections, giving complete freedom for a wide variety of image sizes and scales, thereby making this operation unsustainable. In the literature, this question is commonly overlooked and a uniform scale is assumed. In this paper, a machine learning-based approach to estimate the scale of music documents with respect to a reference scale is presented. Our goal is to propose a robust and generalizable method to adapt the input image to the requirements of an OMR system. For this, two goal-directed case studies are included to evaluate the proposed approach over common task within the OMR workflow, comparing the behavior with other state-of-the-art methods. Results suggest that it is necessary to perform this additional step in the first stage of the workflow to correct the scale of the input images. In addition, it is empirically demonstrated that our specialized approach is more promising than image augmentation strategies for the multi-scale challenge.},
	journal = {Expert Systems with Applications},
	author = {Castellanos, Francisco J. and Gallego, Antonio-Javier and Calvo-Zaragoza, Jorge},
	year = {2020},
	keywords = {Optical Music Recognition, Music Score Images Analysis, Scale Correction, Scale Estimation},
	pages = {113590},
	file = {Castellanos et al_2020_Automatic scale estimation for music score images.pdf:/home/ptorras/zotpapers/Castellanos et al_2020_Automatic scale estimation for music score images.pdf:application/pdf},
}

@inproceedings{castellanos_unsupervised_2021,
	address = {Alicante, Spain},
	title = {Unsupervised {Neural} {Document} {Analysis} for {Music} {Score} {Images}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Castellanos, Francisco J. and Gallego, Antonio-Javier},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {50--54},
}

@inproceedings{castro_methods_2007,
	address = {Berlin, Heidelberg},
	title = {Methods for {Written} {Ancient} {Music} {Restoration}},
	isbn = {978-3-540-74260-9},
	doi = {10.1007/978-3-540-74260-9_106},
	abstract = {Degradation in old documents has been a matter of concern for a long time. With the easy access to information provided by technologies such as the Internet, new ways have arisen for consulting those documents without exposing them to yet more dangers of degradation. While restoration methods are present in the literature in relation to text documents and artworks, little attention has been given to the restoration of ancient music. This paper describes and compares different methods to restore images of ancient music documents degraded over time. Six different methods were tested, including global and adaptive thresholding, color clustering and edge detection. In this paper we conclude that those based on the Sauvola's thresholding algorithm are the better suited for our proposed goal of ancient music restoration.},
	booktitle = {Image {Analysis} and {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Castro, Pedro and Caldas Pinto, J. R.},
	editor = {Kamel, Mohamed and Campilho, Aurélio},
	year = {2007},
	pages = {1194--1205},
}

@inproceedings{castro_restoration_2007,
	address = {Berlin, Heidelberg},
	title = {Restoration of {Double}-{Sided} {Ancient} {Music} {Documents} with {Bleed}-{Through}},
	isbn = {978-3-540-76725-1},
	doi = {10.1007/978-3-540-76725-1_97},
	abstract = {Access to collections of cultural heritage is increasingly becoming a topic of interest for institutions like libraries. With the easy access to information provided by technologies such as the Internet, new ways exist for consulting ancient documents without exposing them to more dangers of degradation. One of those types of documents is written ancient music. These documents suffer from multiple kinds of degradation, where bleed-through outstands as the most damaging. This paper proposes a new method based on the Takagi Sugeno fuzzy classification algorithm to classify the pixels as bleed-through, after performing a general background restoration. This method is applied to a set of double-sided ancient music documents, and the obtained results compared with methods present in the literature.},
	booktitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Castro, Pedro and Almeida, R. J. and Caldas Pinto, J. R.},
	editor = {Rueda, Luis and Mery, Domingo and Kittler, Josef},
	year = {2007},
	pages = {940--949},
	file = {Castro et al_2007_Restoration of Double-Sided Ancient Music Documents with Bleed-Through.pdf:/home/ptorras/zotpapers/Castro et al_2007_Restoration of Double-Sided Ancient Music Documents with Bleed-Through.pdf:application/pdf},
}

@article{chanda_offline_2014,
	title = {Offline {Hand}-{Written} {Musical} {Symbol} {Recognition}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6981053},
	doi = {10.1109/ICFHR.2014.74},
	journal = {14th International Conference on Frontiers in Handwriting Recognition},
	author = {Chanda, Sukalpa and Das, Debleena and Pal, Umapada and Kimura, Fumitaka},
	year = {2014},
	note = {ISBN: 978-1-4799-4334-0
Publisher: Institute of Electrical \& Electronics Engineers (IEEE)},
	keywords = {character recognition, to classify, - musical score, mqdf, offline musical symbol recognition, svm},
	pages = {405--410},
}

@article{chen_optical_2013,
	title = {An {Optical} {Music} {Recognition} {System} for {Skew} or {Inverted} {Musical} {Scores}},
	volume = {27},
	doi = {10.1142/S0218001413530054},
	number = {07},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Chen, Yung-Sheng and Chen, Feng-Sheng and Teng, Chin-Hung},
	year = {2013},
}

@article{chen_optical_2014,
	title = {An optical music recognition system for traditional {Chinese} {Kunqu} {Opera} scores written in {Gong}-{Che} {Notation}},
	volume = {2014},
	issn = {1687-4722},
	doi = {10.1186/1687-4722-2014-7},
	abstract = {This paper presents an optical music recognition (OMR) system to process the handwritten musical scores of Kunqu Opera written in Gong-Che Notation (GCN). First, it introduces the background of Kunqu Opera and GCN. Kunqu Opera is one of the oldest forms of musical activity, spanning the sixteenth to eighteenth centuries, and GCN has been the most popular notation for recording musical works in China since the seventh century. Many Kunqu Operas that use GCN are available as original manuscripts or photocopies, and transforming these versions into a machine-readable format is a pressing need. The OMR system comprises six stages: image pre-processing, segmentation, feature extraction, symbol recognition, musical semantics, and musical instrument digital interface (MIDI) representation. This paper focuses on the symbol recognition stage and obtains the musical information with Bayesian, genetic algorithm, and K-nearest neighbor classifiers. The experimental results indicate that symbol recognition for Kunqu Opera's handwritten musical scores is effective. This work will help to preserve and popularize Chinese cultural heritage and to store Kunqu Opera scores in a machine-readable format, thereby ensuring the possibility of spreading and performing original Kunqu Opera musical scores.},
	number = {1},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Chen, Gen-Fang and Sheu, Jia-Shing},
	year = {2014},
	pages = {7},
	file = {Chen_Sheu_2014_An optical music recognition system for traditional Chinese Kunqu Opera scores.pdf:/home/ptorras/zotpapers/Chen_Sheu_2014_An optical music recognition system for traditional Chinese Kunqu Opera scores.pdf:application/pdf},
}

@inproceedings{chen_optical_2014-1,
	address = {Toronto, Canada},
	title = {Optical {Music} {Recognition} with {Human} {Labeled} {Constraints}},
	url = {http://www.doc.gold.ac.uk/~mas02mg/HCML2016/HCML2016_paper_3.pdf},
	booktitle = {{CHI}'14 {Workshop} on {Human}-{Centred} {Machine} {Learning}},
	author = {Chen, Liang and Jin, Rong and Raphael, Christopher},
	year = {2014},
	file = {Chen et al_2014_Optical Music Recognition with Human Labeled Constraints.pdf:/home/ptorras/zotpapers/Chen et al_2014_Optical Music Recognition with Human Labeled Constraints.pdf:application/pdf},
}

@inproceedings{chen_renotation_2015,
	address = {Cham},
	title = {Renotation from {Optical} {Music} {Recognition}},
	doi = {10.1007/978-3-319-20603-5_2},
	booktitle = {Mathematics and {Computation} in {Music}},
	publisher = {School of Informatics and Computing, Indiana University},
	author = {Chen, Liang and Jin, Rong and Raphael, Christopher},
	year = {2015},
	pages = {16--26},
}

@inproceedings{chen_ceres_2015,
	address = {Málaga, Spain},
	title = {Ceres: {An} {Interactive} {Optical} {Music} {Recognition} {System}},
	url = {http://ismir2015.uma.es/LBD/LBD10.pdf},
	booktitle = {Extended abstracts for the {Late}-{Breaking} {Demo} {Session} of the 16th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Chen, Liang and Raphael, Christopher},
	year = {2015},
	file = {Chen_Raphael_2015_Ceres.pdf:/home/ptorras/zotpapers/Chen_Raphael_2015_Ceres.pdf:application/pdf},
}

@inproceedings{chen_midi-assisted_2016,
	title = {{MIDI}-assisted egocentric optical music recognition},
	isbn = {978-1-5090-0641-0},
	doi = {10.1109/WACV.2016.7477714},
	abstract = {Egocentric vision has received increasing attention in recent years due to the vast development of wearable devices and their applications. Although there are numerous existing work on egocentric vision, none of them solve Optical Music Recognition (OMR) problem. In this paper, we propose a novel optical music recognition approach for egocentric device (e.g. Google Glass) with the assistance of MIDI data. We formulate the problem as a structured sequence alignment problem as opposed to the blind recognition in traditional OMR systems. We propose a linear-chain Conditional Random Field (CRF) to model the note event sequence, which translates the relative temporal relations contained by MIDI to spatial constraints over the egocentric observation. We performed evaluations to compare the proposed approach with several different baselines and proved that our approach achieved the highest recognition accuracy. We view our work as the first step towards egocentric optical music recognition, and believe it will bring insights for next-generation music pedagogy and music entertainment.},
	booktitle = {Winter {Conference} on {Applications} of {Computer} {Vision}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Chen, Liang and Duan, Kun},
	year = {2016},
}

@article{chen_human-directed_2016,
	title = {Human-{Directed} {Optical} {Music} {Recognition}},
	volume = {2016},
	doi = {10.2352/ISSN.2470-1173.2016.17.DRR-053},
	number = {17},
	journal = {Electronic Imaging},
	author = {Chen, Liang and Raphael, Christopher},
	year = {2016},
	note = {Publisher: Society for Imaging Science and Technology},
	pages = {1--9},
}

@inproceedings{chen_human-interactive_2016,
	title = {Human-{Interactive} {Optical} {Music} {Recognition}},
	isbn = {978-0-692-75506-8},
	url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/106_Paper.pdf},
	booktitle = {17th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Chen, Liang and Stolterman, Erik and Raphael, Christopher},
	editor = {Mandel, Michael I. and Devaney, Johanna and Turnbull, Douglas and Tzanetakis, George},
	year = {2016},
	pages = {647--653},
	file = {Chen et al_2016_Human-Interactive Optical Music Recognition.pdf:/home/ptorras/zotpapers/Chen et al_2016_Human-Interactive Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{chen_hybrid_2016,
	title = {A {Hybrid} {HMM}-{RNN} {Model} for {Optical} {Music} {Recognition}},
	url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/chen-hybrid.pdf},
	booktitle = {Extended abstracts for the {Late}-{Breaking} {Demo} {Session} of the 17th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Chen, Liang and Jin, Rong and Zhang, Simo and Lee, Stefan and Chen, Zhenhua and Crandall, David},
	year = {2016},
	file = {Chen et al_2016_A Hybrid HMM-RNN Model for Optical Music Recognition.pdf:/home/ptorras/zotpapers/Chen et al_2016_A Hybrid HMM-RNN Model for Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{chen_human-guided_2017,
	title = {Human-{Guided} {Recognition} of {Music} {Score} {Images}},
	doi = {10.1145/3144749.3144752},
	booktitle = {4th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM Press},
	author = {Chen, Liang and Jin, Rong and Raphael, Christopher},
	year = {2017},
}

@inproceedings{chen_renotation_2017,
	address = {Espoo, Finland},
	title = {Renotation of {Optical} {Music} {Recognition} {Data}},
	url = {http://smc2017.aalto.fi/media/materials/proceedings/SMC17_p287.pdf},
	booktitle = {14th {Sound} and {Music} {Computing} {Conference}},
	author = {Chen, Liang and Raphael, Christopher},
	year = {2017},
}

@inproceedings{chen_optical_2018,
	address = {Paris, France},
	title = {Optical {Music} {Recognition} and {Human}-in-the-loop {Computation}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Chen, Liang and Raphael, Christopher},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {11--12},
}

@inproceedings{chhabra_graphic_1998,
	address = {Berlin, Heidelberg},
	title = {Graphic symbol recognition: {An} overview},
	isbn = {978-3-540-69766-4},
	doi = {10.1007/3-540-64381-8_40},
	abstract = {Symbol recognition is one of the primary stages of any graphics recognition system. This paper reviews the current state of the art in graphic symbol recognition and raises some open issues that need further investigation. Work on symbol recognition tends to be highly application specific. Therefore, this review presents the symbol recognition methods in the context of specific applications.},
	booktitle = {Graphics {Recognition} {Algorithms} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chhabra, Atul K.},
	editor = {Tombre, Karl and Chhabra, Atul K.},
	year = {1998},
	pages = {68--79},
}

@inproceedings{choi_bootstrapping_2017,
	address = {Kyoto, Japan},
	title = {Bootstrapping {Samples} of {Accidentals} in {Dense} {Piano} {Scores} for {CNN}-{Based} {Detection}},
	isbn = {978-1-5386-3586-5},
	doi = {10.1109/ICDAR.2017.257},
	abstract = {State-of-the-art Optical Music Recognition system often fails to process dense and damaged music scores, where many symbols can present complex segmentation problems. We propose to resolve these segmentation problems by using a CNNbased detector trained with few manually annotated data. A data augmentation bootstrapping method is used to accurately train a deep learning model to do the localization and classification of an accidental symbol associated with a note head, or the note head if there is no accidental. Using 5-fold cross-validation, we obtain an average of 98.5\% localization with an IoU score over 0.5 and a classification accuracy of 99.2\%.},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Choi, Kwon-Young and Coüasnon, Bertrand and Ricquebourg, Yann and Zanibbi, Richard},
	year = {2017},
	note = {Backup Publisher: IAPR TC10 (Technical Committee on Graphics Recognition)
ISSN: 2379-2140},
	file = {Choi et al_2017_Bootstrapping Samples of Accidentals in Dense Piano Scores for CNN-Based.pdf:/home/ptorras/zotpapers/Choi et al_2017_Bootstrapping Samples of Accidentals in Dense Piano Scores for CNN-Based.pdf:application/pdf},
}

@inproceedings{choi_music_2018,
	address = {Paris, France},
	title = {Music {Symbol} {Detection} with {Faster} {R}-{CNN} {Using} {Synthetic} {Annotations}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Choi, Kwon-Young and Coüasnon, Bertrand and Ricquebourg, Yann and Zanibbi, Richard},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {9--10},
}

@inproceedings{choudhury_optical_2000,
	title = {Optical {Music} {Recognition} {System} within a {Large}-{Scale} {Digitization} {Project}},
	url = {http://jhir.library.jhu.edu/handle/1774.2/32794},
	booktitle = {1st {International} {Symposium} on {Music} {Information} {Retrieval}},
	author = {Choudhury, G. Sayeed and Droetboom, M. and DiLauro, Tim and Fujinaga, Ichiro and Harrington, Brian},
	year = {2000},
	file = {Choudhury et al_2000_Optical Music Recognition System within a Large-Scale Digitization Project.pdf:/home/ptorras/zotpapers/Choudhury et al_2000_Optical Music Recognition System within a Large-Scale Digitization Project.pdf:application/pdf},
}

@article{choudhury_digital_2000,
	title = {Digital workflow management: {The} {Lester} {S}. {Levy} digitized collection of sheet music},
	volume = {5},
	doi = {10.5210/fm.v5i6.756},
	abstract = {The paper describes the development of a set of workflow management tools (WMS) that will reduce the manual input necessary to manage the workflow of large-scale digitization projects. The WMS will also support the path from physical object and/or digitized material into a digital library repository by providing effective tools for perusing multimedia elements. The Lester S. Levy Collection of Sheet Music Project at the Milton S. Eisenhower Library at The Johns Hopkins University provides an ideal testbed for the development and evaluation of the WMS. Building upon previous effort to digitize the entire collection of over 29000 pieces of sheet music, optical music recognition (OMR) software will create sound files and full-text lyrics. The combination of image, text and sound files provide a comprehensive multimedia environment. The functionality of the collection will be enhanced by the incorporation of metadata, the implementation of a disk based search engine for lyrics, and the development of toolkits for searching sound files (0 Refs.) music; search engines; workflow management software},
	number = {6},
	journal = {First Monday},
	author = {Choudhury, G. Sayeed and Requardt, Cynthia and Fujinaga, Ichiro and DiLauro, Tim and Brown, Elisabeth W. and Warner, James W. and Harrington, Brian},
	year = {2000},
	keywords = {optical music recognition, sheet music, C6130M (Multimedia), C6160M, C7250N (Front end systems for online searching), C7820 (Humanities computing), digital library repository, digital workflow management, disk based search engine (Office automation), full-text lyrics, large-scale digitization projects, Lester S Levy digitized collection, metadata, multimedia elements, multimedia environment, OMR software, sound files, WMS, workflow management tools},
}

@article{choudhury_strike_2001,
	title = {Strike {Up} the {Score}: {Deriving} searchable and playable digital formats from sheet music},
	volume = {7},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/february01/choudhury/02choudhury.html},
	doi = {10.1045/february2001-choudhury},
	number = {2},
	journal = {D-Lib Magazine},
	author = {Choudhury, G. Sayeed and DiLauro, Tim and Droettboom, Michael and Fujinaga, Ichiro and MacMillan, Karl},
	year = {2001},
}

@inproceedings{church_improving_2014,
	title = {Improving {Rhythmic} {Transcriptions} via {Probability} {Models} {Applied} {Post}-{OMR}},
	url = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T116_357_Paper.pdf},
	booktitle = {15th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Church, Maura and Cuthbert, Michael Scott},
	editor = {Wang, Hsin-Min and Yang, Yi-Hsuan and Lee, Jin Ha},
	year = {2014},
	pages = {643--648},
}

@article{clarke_using_1988,
	title = {Using a micro to automate data acquisition in music publishing},
	volume = {24},
	issn = {0165-6074},
	url = {http://www.sciencedirect.com/science/article/pii/0165607488901093},
	doi = {10.1016/0165-6074(88)90109-3},
	abstract = {With the number of computer applications involving music information growing, and the transition from traditional music printing methods to computer typesetting that is being faced by music publishers, there is an increasing need for an efficient and accurate method of getting musical information into computers. This paper describes some of the technical problems encountered in developing a system, based upon the IBM PC and a low-cost scanning device, to automatically recognise the printed music notation on a sheet of music that is fed through the scanner.},
	number = {1},
	journal = {Microprocessing and Microprogramming},
	author = {Clarke, Alastair T. and Brown, B. Malcom and Thorne, M. P.},
	year = {1988},
	pages = {549--553},
}

@article{clarke_coping_1989,
	title = {Coping with some really rotten problems in automatic music recognition},
	volume = {27},
	issn = {0165-6074},
	url = {http://www.sciencedirect.com/science/article/pii/0165607489901087},
	doi = {10.1016/0165-6074(89)90108-7},
	abstract = {This paper describes some of the problems encountered, and some of the techniques that have been used and implemented, during the development of an Optical Character Recognition system for printed music. It focuses on the recognition of chords and clusters, subdivision into single “lines” of music, and translation into musical code. Whereas other, mainframe based, music recognition systems have rarely attacked these problems, our methods have given some considerable success with an IBM PC.},
	number = {1},
	journal = {Microprocessing and Microprogramming},
	author = {Clarke, Alastair T. and Brown, B. Malcom and Thorne, M. P.},
	year = {1989},
	pages = {547--550},
}

@inproceedings{clarke_recognizing_1993,
	title = {Recognizing musical text},
	doi = {10.1117/12.150288},
	abstract = {This paper reports on some recent developments in a software product that recognizes printed music notation. There are a number of computer systems available which assist in the task of printing music; however the full potential of these systems cannot be realized until the musical text has been entered into the computer. It is this problem that we address in this paper. The software we describe, which uses computationally inexpensive methods, is designed to analyze a music score, previously read by a flat bed scanner, and to extract the musical information that it contains. The paper discusses the methods used to recognize the musical text: these involve sampling the image at strategic points and using this information to estimate the musical symbol. It then discusses some hard problems that have been encountered during the course of the research; for example the recognition of chords and note clusters. It also reports on the progress that has been made in solving these problems and concludes with a discussion of work that needs to be undertaken over the next five years in order to transform this research prototype into a commercial product.},
	booktitle = {Machine {Vision} {Applications}, {Architectures}, and {Systems} {Integration}},
	author = {Clarke, Alastair T. and Brown, B. Malcom and Thorne, M. P.},
	year = {1993},
}

@inproceedings{clausen_unified_2002,
	title = {A unified approach to content-based and fault tolerant music identification},
	doi = {10.1109/WDM.2002.1176194},
	abstract = {In this paper we propose a unified approach to content-based search in different kinds of music data. Our approach is based on a general algorithmic framework for searching patterns of complex objects in large databases. In particular we describe how this approach may be used to allow for polyphonic search in polyphonic scores as well as for the identification of PCM audio material. We give an overview on the various aspects of our technology including fault tolerant search methods. Several areas of application are suggested. We give an overview on several prototypic systems we developed for those applications including the notify! and the audentify! systems.},
	booktitle = {2nd {International} {Conference} on {Web} {Delivering} of {Music}},
	author = {Clausen, Michael and Kurth, Frank},
	year = {2002},
	keywords = {Computer science, Databases, music information retrieval, Multiple signal classification, Prototypes, Content based retrieval, content-based retrieval, music, Music information retrieval, polyphonic search, computational complexity, information retrieval, algorithmic complexities, content based retrieval, Fault diagnosis, Fault tolerance, fault tolerant search, large databases, music identification, PCM audio material, Phase change materials, score-based music, Search methods},
	pages = {56--65},
}

@article{clausen_unified_2004,
	title = {A unified approach to content-based and fault-tolerant music recognition},
	volume = {6},
	issn = {1520-9210},
	doi = {10.1109/TMM.2004.834859},
	abstract = {In this paper, we propose a unified approach to fast index-based music recognition. As an important area within the field of music information retrieval (MIR), the goal of music recognition is, given a database of musical pieces and a query document, to locate all occurrences of that document within the database, up to certain possible errors. In particular, the identification of the query with regard to the database becomes possible. The approach presented in this paper is based on a general algorithmic framework for searching complex patterns of objects in large databases. We describe how this approach may be applied to two important music recognition tasks: The polyphonic (musical score-based) search in polyphonic score data and the identification of pulse-code modulation audio material from a given acoustic waveform. We give an overview on the various aspects of our technology including fault-tolerant search methods. Several areas of application are suggested. We describe several prototypic systems we have developed for those applications including the notify! and the audentify! systems for score- and waveform-based music recognition, respectively.},
	number = {5},
	journal = {IEEE Transactions on Multimedia},
	author = {Clausen, Michael and Kurth, Frank},
	year = {2004},
	keywords = {Databases, music information retrieval, Multiple signal classification, pattern recognition, audio signal processing, Content based retrieval, content-based retrieval, music, Music information retrieval, Fault tolerance, Search methods, Acoustic materials, Acoustic pulses, acoustic waveform, Acoustic waves, audio databases, audio identification, audio material, database indexing, fault tolerant computing, fault-tolerant search method, index-based music recognition, pattern searching, polyphonic search method, pulse code modulation, Pulse modulation, pulse-code modulation, query formulation, transposition-invariant search},
	pages = {717--731},
}

@inproceedings{colesnicov_digitization_2019,
	title = {On {Digitization} of {Documents} with {Script} {Presentable} {Content}},
	url = {https://ibn.idsi.md/sites/default/files/imag_file/321-324_7.pdf},
	abstract = {The paper is dedicated to details of the digitization of printed documents that include formalized script presentable content, in connection with the revitalization of the cultural heritage. We discuss the process and the necessary software by an example of music, as the recognition of scores is a solved task.},
	booktitle = {Proceedings of the {Fifth} {Conference} of {Mathematical} {Society} of {Moldova}},
	author = {Colesnicov, Alexandru and Cojocaru, Svetlana and Luca, Mihaela and Malahov, Ludmila},
	year = {2019},
	file = {Colesnicov et al_2019_On Digitization of Documents with Script Presentable Content.pdf:/home/ptorras/zotpapers/Colesnicov et al_2019_On Digitization of Documents with Script Presentable Content.pdf:application/pdf},
}

@inproceedings{couasnon_using_1994,
	address = {Kaiserslautern, Germany},
	title = {Using {Grammars} to {Segment} and {Recognize} {Music} {Scores}},
	url = {ftp://ftp.idsa.prd.fr/local/IMADOC/couasnon/Articles/das94.ps},
	booktitle = {International {Association} for {Pattern} {Recognition} {Workshop} on {Document} {Analysis} {Systems}},
	author = {Coüasnon, Bertrand and Camillerapp, Jean},
	year = {1994},
	pages = {15--27},
}

@inproceedings{couasnon_using_1995,
	title = {Using {Logic} {Programming} {Languages} {For} {Optical} {Music} {Recognition}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.8439},
	abstract = {Optical Music Recognition is a particular form of document analysis in which there is much knowledge about document structure. Indeed there exists an important set of rules for musical notation, but current systems do not fully use them. We propose a new solution using a grammar to guide the segmentation of the graphical ob jects and their recognition. The grammar is essentially a description of the relations (relative position and size, adjacency, etc) between the graphical ob jects. Inspired by Denite Clause Grammar techniques, the grammar can be directly implemented in Prolog, a higher-order dialect of Prolog. Moreover, the translation from the grammar into Prolog code can be done automatically. Our approach is justied by the rst encouraging results obtained with a prototype for music score recognition.},
	booktitle = {3rd {International} {Conference} on the {Practical} {Application} of {Prolog}},
	author = {Coüasnon, Bertrand and Brisset, Pascal and Stéphan, Igor},
	year = {1995},
	keywords = {Optical Music Recognition, DCG, Document analysis, Grammar Trans},
}

@inproceedings{couasnon_way_1995,
	title = {A {Way} to {Separate} {Knowledge} {From} {Program} in {Structured} {Document} {Analysis}: {Application} to {Optical} {Music} {Recognition}},
	doi = {10.1109/ICDAR.1995.602099},
	abstract = {Optical Music Recognition is a form of document analysis for which a priori knowledge is particularly important. Musical notation is governed by a substantial set of rules, but current systems fail to use them adequately. In complex scores, existing systems cannot overcome the well-known segmentation problems of document analysis, due mainly to the high density of music information. This paper proposes a new method of recognition which uses a grammar in order to formalize the syntactic rules and represent the context. However, where objects touch, there is a discrepancy between the way the existing knowledge (grammar) will describe an object and the way it is recognized, since touching objects have to be segmented first. Following a description of the grammar, this paper shall go on to propose the use of an operator to modify the way the grammar parses the image so that the system can deal with certain touching objects (e.g. where an accidental touches a notehead).},
	booktitle = {3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Coüasnon, Bertrand and Camillerapp, Jean},
	year = {1995},
	keywords = {Image segmentation, Text analysis, document analysis, Image recognition, optical music recognition, Multiple signal classification, Labeling, document image processing, music, Image analysis, image segmentation, character recognition, musical notation, a priori knowledge, grammar, Information analysis, Joining processes, Particle beam optics, segmentation problems, syntactic rules, touching objects, Ultraviolet sources},
	pages = {1092--1097},
}

@inproceedings{couasnon_using_1995-1,
	title = {Using a grammar for a reliable full score recognition system},
	url = {https://pdfs.semanticscholar.org/3b97/949f436f929ed11ee76358e07fa1a61d2e01.pdf},
	abstract = {Optical Music Recognition needs to be reliable to avoid users to detect and correct errors by controlling all the recognized score. Reliability can be reach by improving the recognition quality (on segmentation problems) and by making the system able to detect itself its recognition errors. This is possible only by using as much as possible the musical knowledge. Therefore, we propose a grammar to formalize the musical knowledge on full cores with polyphonic staves. We then show how this grammar can help detection of most of errors on note duration. The presented system is in an implementation phase but is already able to deal with full scores and to point on errors.},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Coüasnon, Bertrand and Rétif, Bernard},
	year = {1995},
	pages = {187--194},
}

@inproceedings{couasnon_dmos_2001,
	title = {{DMOS}: a generic document recognition method, application to an automatic generator of musical scores, mathematical formulae and table structures recognition systems},
	doi = {10.1109/ICDAR.2001.953786},
	abstract = {Genericity in structured document recognition is a difficult challenge. We therefore propose a new generic document recognition method, called DMOS (Description and MOdification of Segmentation), that is made up of a new grammatical formalism, called EPF (Enhanced Position Formalism) and an associated parser which is able to introduce context in segmentation. We implement this method to obtain a generator of document recognition systems. This generator can automatically produce new recognition systems. It is only necessary to describe the document with an EPF grammar, which is then simply compiled. In this way, we have developed various recognition systems: one on musical scores, one on mathematical formulae and one on recursive table structures. We have also defined a specific application to damaged military forms of the 19th Century. We have been able to test the generated system on 5,000 of these military forms. This has permitted us to validate the DMOS method on a real-world application},
	booktitle = {6th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Coüasnon, Bertrand},
	year = {2001},
	keywords = {System testing, document image processing, history, music, image recognition, image segmentation, data structures, automatic musical score generator, compilation, damaged military forms, DMOS, Enhanced Position Formalism, EPF grammatical formalism, generic document recognition method, grammars, mathematical formulae, mathematics computing, military computing, parser, program compilers, recursive table structure recognition, segmentation context},
	pages = {215--220},
}

@article{craig-mcfeely_digital_2008,
	title = {Digital {Image} {Archive} of {Medieval} {Music}: {The} evolution of a digital resource},
	volume = {3},
	doi = {http://doi.org/10.16995/dm.16},
	journal = {Digital Medievalist},
	author = {Craig-McFeely, Julia},
	year = {2008},
}

@inproceedings{crawford_searching_2018,
	address = {Paris, France},
	title = {Searching {Page}-{Images} of {Early} {Music} {Scanned} with {OMR}: {A} {Scalable} {Solution} {Using} {Minimal} {Absent} {Words}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/210_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Crawford, Tim and Badkobeh, Golnaz and Lewis, David},
	year = {2018},
	pages = {233--239},
}

@inproceedings{dalitz_using_2005,
	address = {London, UK},
	title = {Using the {Gamera} framework for building a lute tablature recognition system},
	url = {http://ismir2005.ismir.net/proceedings/2012.pdf},
	booktitle = {6th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Dalitz, Christoph and Karsten, Thomas},
	year = {2005},
	pages = {478--481},
	file = {Dalitz_Karsten_2005_Using the Gamera framework for building a lute tablature recognition system.pdf:/home/ptorras/zotpapers/Dalitz_Karsten_2005_Using the Gamera framework for building a lute tablature recognition system.pdf:application/pdf},
}

@article{dalitz_comparative_2008,
	title = {A {Comparative} {Study} of {Staff} {Removal} {Algorithms}},
	volume = {30},
	issn = {0162-8828},
	doi = {10.1109/tpami.2007.70749},
	abstract = {This paper presents a quantitative comparison of different algorithms for the removal of stafflines from music images. It contains a survey of previously proposed algorithms and suggests a new skeletonization-based approach. We define three different error metrics, compare the algorithms with respect to these metrics, and measure their robustness with respect to certain image defects. Our test images are computer-generated scores on which we apply various image deformations typically found in real-world data. In addition to modern western music notation, our test set also includes historic music notation such as mensural notation and lute tablature. Our general approach and evaluation methodology is not specific to staff removal but applicable to other segmentation problems as well.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dalitz, Christoph and Droettboom, Michael and Pranzas, Bastian and Fujinaga, Ichiro},
	year = {2008},
	note = {Publisher: Institute of Electrical \& Electronics Engineers (IEEE)},
	keywords = {Music, Performance evaluation, Documentation, Segmentation, image recognition, image segmentation, Algorithms, Artificial Intelligence, Automated, Computer-Assisted, Image Enhancement, Image Interpretation, Information Storage and Retrieval, Pattern Recognition, Pixel classification, mensural notation, Automatic Data Processing, computer-generated scores, error metrics, historic music notation, image defects, image deformations, lute tablature, modern western music notation, Music (Optical Recognition), music images, real-world data, Reproducibility of Results, Sensitivity and Specificity, skeletonization-based approach, staff removal algorithms, staffline removal, Subtraction Technique},
	pages = {753--766},
}

@article{dalitz_optical_2008,
	title = {Optical recognition of psaltic {Byzantine} chant notation},
	volume = {11},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-008-0074-4},
	doi = {10.1007/s10032-008-0074-4},
	abstract = {This paper describes a document recognition system for the modern neume based notation of Byzantine music. We propose algorithms for page segmentation, lyrics removal, syntactical symbol grouping and the determination of characteristic page dimensions. All algorithms are experimentally evaluated on a variety of printed books for which we also give an optimal feature set for a nearest neighbour classifier. The system is based on the Gamera framework for document image analysis. Given that we cover all aspects of the recognition process, the paper can also serve as an illustration how a recognition system for a non standard document type can be designed from scratch.},
	number = {3},
	journal = {International Journal of Document Analysis and Recognition},
	author = {Dalitz, Christoph and Michalakis, Georgios K. and Pranzas, Christine},
	year = {2008},
	pages = {143--158},
}

@inproceedings{dalitz_german_2009,
	title = {German {Lute} {Tablature} {Recognition}},
	doi = {10.1109/ICDAR.2009.52},
	abstract = {This paper describes a document recognition system for 16th century German staffless lute tablature notation. We present methods for page layout analysis, symbol recognition and symbol layout analysis and report error rates for these methods on a variety of historic prints. Page layout analysis is based on horizontal separator lines, which may interfere with other symbols. The proposed algorithm for their detection and removal is also applicable to other single staff line detection problems (like percussion notation), for which common staff line removal algorithms fail.},
	booktitle = {10th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Dalitz, Christoph and Pranzas, Christine},
	year = {2009},
	note = {ISSN: 1520-5363},
	keywords = {Text analysis, Music, Natural languages, Rhythm, Image recognition, Error analysis, optical music recognition, document image processing, Instruments, Image analysis, Libraries, document recognition system, error rates, error statistics, German lute tablature recognition, German staffless lute tablature notation, page layout analysis, Particle separators, percussion notation, staff line removal, staff line removal algorithms, symbol layout analysis, symbol recognition},
	pages = {371--375},
}

@inproceedings{damm_multimodal_2008,
	address = {Chania, Greece},
	title = {Multimodal {Presentation} and {Browsing} of {Music}},
	isbn = {978-1-60558-198-9},
	url = {http://doi.acm.org/10.1145/1452392.1452436},
	doi = {10.1145/1452392.1452436},
	booktitle = {10th {International} {Conference} on {Multimodal} {Interfaces}},
	publisher = {ACM},
	author = {Damm, David and Fremerey, Christian and Kurth, Frank and Müller, Meinard and Clausen, Michael},
	year = {2008},
	keywords = {music information retrieval, music alignment, music browsing, music navigation, music synchronization},
	pages = {205--208},
}

@techreport{dan_automatic_1996,
	title = {Automatic {Optical} {Music} recognition},
	url = {https://www.cs.waikato.ac.nz/~davidb/omr/ftp/lee_report.ps.gz},
	abstract = {In this pro ject, the topic of automatic optical music recognition was studied. It is the conversion of an optically sampled image of a musical score into a representation that can be conveniently stored in computer storage and retrieved for various purpose. It is analogous to optical character recognition. Optical character recognition recognizes text characters in the input images and output the text in a machine-readable format. Similarly, an optical music recognition system recognizes the symbols on a musical score and output the results in a binary format. Subsequent processing on this output can provide a wide variety of applications, such as reprinting and archiving.},
	institution = {The University of Waikato, New Zealand},
	author = {Dan, Lee Sau},
	year = {1996},
}

@mastersthesis{desaedeleer_reading_2006,
	title = {Reading {Sheet} {Music}},
	url = {https://sourceforge.net/projects/openomr/},
	abstract = {Optical Music Recognition is the process of recognising a printed music score and converting it to a format that is understood by computers. This process involves detecting all musical elements present in the music score in such a way that the score can be represented digitally. For example, the score could be recognised and played back through the computer speakers. Much research has been carried out in this area and several approaches to performing OMR have been suggested. A more recent approach involves segmenting the image using a neural network to recognise the segmented symbols from which the score can be reconstructed. This project will survey the different techniques that have been used to perform OMR on printed music scores and an application by the name of OpenOMR will be developed. One of the aims is to create an open source project in which developers in the open source community will be able to contribute their ideas in order to enhance this application and progress the research in the OMR field.},
	school = {University of London},
	author = {Desaedeleer, Arnaud F.},
	year = {2006},
}

@phdthesis{diener_modeling_1990,
	address = {Palo Alto, CA},
	type = {{PhD} {Thesis}},
	title = {Modeling music notation: {A} three-dimensional approach},
	url = {ftp://ftp-ccrma.stanford.edu/pub/Publications/Theses/GRDThesis.ps.Z},
	school = {Stanford University},
	author = {Diener, Glendon Ross},
	year = {1990},
}

@inproceedings{diet_probado_2007,
	address = {Vienna, Austria},
	title = {The {Probado} {Music} {Repository} at the {Bavarian} {State} {Library}},
	url = {http://ismir2007.ismir.net/proceedings/ISMIR2007_p501_diet.pdf},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Diet, Jürgen and Kurth, Frank},
	year = {2007},
	pages = {501--504},
	file = {Diet_Kurth_2007_The Probado Music Repository at the Bavarian State Library.pdf:/home/ptorras/zotpapers/Diet_Kurth_2007_The Probado Music Repository at the Bavarian State Library.pdf:application/pdf},
}

@article{diet_optical_2018,
	title = {Optical {Music} {Recognition} in der {Bayerischen} {Staatsbibliothek}},
	doi = {10.18452/18953},
	language = {German},
	journal = {BIBLIOTHEK – Forschung und Praxis},
	author = {Diet, Jürgen},
	year = {2018},
	file = {Diet_2018_Optical Music Recognition in der Bayerischen Staatsbibliothek.pdf:/home/ptorras/zotpapers/Diet_2018_Optical Music Recognition in der Bayerischen Staatsbibliothek.pdf:application/pdf},
}

@inproceedings{diet_innovative_2018,
	address = {Paris, France},
	title = {Innovative {MIR} {Applications} at the {Bayerische} {Staatsbibliothek}},
	url = {https://dlfm.web.ox.ac.uk/sites/default/files/dlfm/documents/media/diet-innovative-mir-bsb.pdf},
	abstract = {This short position paper gives an insight into the digitization of music prints in the Bayerische Staatsbibliothek and describes two music information retrieval applications in the Bayerische Staatsbibliothek. One of them is a melody search application based on OMR data that has been generated with 40.000 pages of digitized music prints containing all compositions of L. van Beethoven, G. F. Händel, F. Liszt, and F. Schubert. The other one is the incipit search in the International Inventory of Musical Sources (Répertoire International des Sources Musicales, RISM).},
	booktitle = {5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	author = {Diet, Jürgen},
	year = {2018},
	keywords = {Optical Music Recognition, Digial Library, Digitalization},
	file = {Diet_2018_Innovative MIR Applications at the Bayerische Staatsbibliothek.pdf:/home/ptorras/zotpapers/Diet_2018_Innovative MIR Applications at the Bayerische Staatsbibliothek.pdf:application/pdf},
}

@article{ding_optical_2014,
	title = {Optical music recognition of the singer using formant frequency estimation of vocal fold vibration and lip motion with interpolated {GMM} classifiers},
	volume = {16},
	issn = {1392-8716},
	url = {https://www.jvejournals.com/article/14921},
	abstract = {The main work of this paper is to identify the musical genres of the singer by performing the optical detection of lip motion. Recently, optical music recognition has attracted much attention. Optical music recognition in this study is a type of automatic techniques in information engineering, which can be used to determine the musical style of the singer. This paper proposes a method for optical music recognition where acoustic formant analysis of both vocal fold vibration and lip motion are employed with interpolated Gaussian mixture model (GMM) estimation to perform musical genre classification of the singer. The developed approach for such classification application is called GMM-Formant. Since humming and voiced speech sounds cause periodic vibrations of the vocal folds and then the corresponding motion of the lip, the proposed GMM-Formant firstly operates to acquire the required formant information. Formant information is important acoustic feature data for recognition classification. The proposed GMM-Formant method then uses linear interpolation for combining GMM likelihood estimates and formant evaluation results appropriately. GMM-Formant will effectively adjust the estimated formant feature evaluation outcomes by referring to certain degree of the likelihood score derived from GMM calculations. The superiority and effectiveness of presented GMM-Formant are demonstrated by a series of experiments on musical genre classification of the singer.},
	number = {5},
	journal = {Journal of Vibroengineering},
	author = {Ding, Ing-Jr and Yen, Chih-Ta and Chang, Che-Wei and Lin, He-Zhong},
	year = {2014},
	note = {Publisher: Vibromechanika},
	keywords = {Optical music recognition, Speech, Classification (of information), Formant frequency estimation, Frequency estimation, Gaussian distribution, Gaussian Mixture Model, Information engineerings, Linear Interpolation, Lip motions, Musical genre classification, Vibration analysis, Vocal fold vibration},
	pages = {2572--2581},
	file = {Ding et al_2014_Optical music recognition of the singer using formant frequency estimation of.pdf:/home/ptorras/zotpapers/Ding et al_2014_Optical music recognition of the singer using formant frequency estimation of.pdf:application/pdf},
}

@article{dinh_fast_2016,
	title = {Fast lyric area extraction from images of printed {Korean} music scores},
	volume = {E99D},
	issn = {0916-8532},
	doi = {10.1587/transinf.2015EDP7296},
	abstract = {In recent years, optical music recognition (OMR) has been extensively developed, particularly for use with mobile devices that require fast processing to recognize and play live the notes in images captured from sheet music. However, most techniques that have been developed thus far have focused on playing back instrumental music and have ignored the importance of lyric extraction, which is time consuming and affects the accuracy of the OMR tools. The text of the lyrics adds complexity to the page layout, particularly when lyrics touch or overlap musical symbols, in which case it is very difficult to separate them from each other. In addition, the distortion that appears in captured musical images makes the lyric lines curved or skewed, making the lyric extraction problem more complicated. This paper proposes a new approach in which lyrics are detected and extracted quickly and effectively. First, in order to resolve the distortion problem, the image is undistorted by a method using information of stave lines and bar lines. Then, through the use of a frequency count method and heuristic rules based on projection, the lyric areas are extracted, the cases where symbols touch the lyrics are resolved, and most of the information from the musical notation is kept even when the lyrics and music notes are overlapping. Our algorithm demonstrated a short processing time and remarkable accuracy on two test datasets of images of printed Korean musical scores: The first set included three hundred scanned musical images; the second set had two hundred musical images that were captured by a digital camera. © 2016 The Institute of Electronics, Information and Communication Engineers.},
	number = {6},
	journal = {IEICE Transactions on Information and Systems},
	author = {Dinh, Cong Minh and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung},
	year = {2016},
	note = {Publisher: Maruzen Co., Ltd.},
	keywords = {Optical music recognition, Image processing, Extraction, Fast Processing, Frequency counts, Heuristic methods, Heuristic rules, Lyric area, Musical notation, Musical symbols, Short processing time},
	pages = {1576--1584},
	file = {Dinh et al_2016_Fast lyric area extraction from images of printed Korean music scores.pdf:/home/ptorras/zotpapers/Dinh et al_2016_Fast lyric area extraction from images of printed Korean music scores.pdf:application/pdf},
}

@article{dorfer_towards_2016,
	title = {Towards {End}-to-{End} {Audio}-{Sheet}-{Music} {Retrieval}},
	volume = {abs/1612.05070},
	url = {http://arxiv.org/abs/1612.05070},
	journal = {Computing Research Repository},
	author = {Dorfer, Matthias and Arzt, Andreas and Widmer, Gerhard},
	year = {2016},
	file = {Dorfer et al_2016_Towards End-to-End Audio-Sheet-Music Retrieval.pdf:/home/ptorras/zotpapers/Dorfer et al_2016_Towards End-to-End Audio-Sheet-Music Retrieval.pdf:application/pdf},
}

@inproceedings{dorfer_towards_2016-1,
	title = {Towards {Score} {Following} {In} {Sheet} {Music} {Images}},
	isbn = {978-0-692-75506-8},
	url = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/027_Paper.pdf},
	booktitle = {17th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Dorfer, Matthias and Arzt, Andreas and Widmer, Gerhard},
	editor = {Mandel, Michael I. and Devaney, Johanna and Turnbull, Douglas and Tzanetakis, George},
	year = {2016},
	pages = {789--795},
	file = {Dorfer et al_2016_Towards Score Following In Sheet Music Images.pdf:/home/ptorras/zotpapers/Dorfer et al_2016_Towards Score Following In Sheet Music Images.pdf:application/pdf},
}

@article{dorfer_learning_2018,
	title = {Learning {Audio}–{Sheet} {Music} {Correspondences} for {Cross}-{Modal} {Retrieval} and {Piece} {Identification}},
	volume = {1},
	doi = {10.5334/tismir.12},
	number = {1},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Dorfer, Matthias and Hajič jr., Jan and Arzt, Andreas and Frostel, Harald and Widmer, Gerhard},
	year = {2018},
	pages = {22--33},
	file = {Dorfer et al_2018_Learning Audio–Sheet Music Correspondences for Cross-Modal Retrieval and Piece.pdf:/home/ptorras/zotpapers/Dorfer et al_2018_Learning Audio–Sheet Music Correspondences for Cross-Modal Retrieval and Piece.pdf:application/pdf},
}

@inproceedings{dorfer_learning_2018-1,
	address = {Paris, France},
	title = {Learning {To} {Listen}, {Read} {And} {Follow}: {Score} {Following} {As} {A} {Reinforcement} {Learning} {Game}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/45_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Dorfer, Matthias and Henkel, Florian and Widmer, Gerhard},
	year = {2018},
	pages = {784--791},
}

@article{dovey_overview_2004,
	title = {Overview of the {OMRAS} {Project}: {Online} {Music} {Retrieval} and {Searching}},
	volume = {55},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20063},
	doi = {10.1002/asi.20063},
	abstract = {Until recently, most research on music information retrieval concentrated on monophonic music. Online Music Retrieval and Searching (OMRAS) is a three-year project funded under the auspices of the JISC (Joint Information Systems Committee)/NSF (National Science Foundation) International Digital Library Initiative which began in 1999 and whose remit was to investigate the issues surrounding polyphonic music information retrieval. Here we outline the work OMRAS has achieved in pattern matching, document retrieval, and audio transcription, as well as some prototype work in how to implement these techniques into library systems.},
	number = {12},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Dovey, Matthew J.},
	year = {2004},
	pages = {1100--1107},
}

@techreport{droettboom_interpreting_2001,
	title = {Interpreting the semantics of music notation using an extensible and object-oriented system},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.7545&rep=rep1&type=pdf},
	institution = {John Hopkins University},
	author = {Droettboom, Michael and Fujinaga, Ichiro},
	year = {2001},
	file = {Droettboom_Fujinaga_2001_Interpreting the semantics of music notation using an extensible and.pdf:/home/ptorras/zotpapers/Droettboom_Fujinaga_2001_Interpreting the semantics of music notation using an extensible and.pdf:application/pdf},
}

@inproceedings{droettboom_optical_2002,
	address = {Berlin, Heidelberg},
	title = {Optical {Music} {Interpretation}},
	isbn = {978-3-540-70659-5},
	doi = {10.1007/3-540-70659-3_39},
	abstract = {A system to convert digitized sheet music into a symbolic music representation is presented. A pragmatic approach is used that conceptualizes this primarily two-dimensional structural recognition problem as a one-dimensional one. The transparency of the implementation owes a great deal to its implementation in a dynamic, object-oriented language. This system is a part of a locally developed end-to-end solution for the conversion of digitized sheet music into symbolic form.},
	booktitle = {Structural, {Syntactic}, and {Statistical} {Pattern} {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Droettboom, Michael and Fujinaga, Ichiro and MacMillan, Karl},
	editor = {Caelli, Terry and Amin, Adnan and Duin, Robert P. W. and de Ridder, Dick and Kamel, Mohamed},
	year = {2002},
	pages = {378--387},
	file = {Droettboom et al_2002_Optical Music Interpretation.pdf:/home/ptorras/zotpapers/Droettboom et al_2002_Optical Music Interpretation.pdf:application/pdf},
}

@inproceedings{droettboom_using_2002,
	address = {London, UK},
	title = {Using the {Gamera} framework for the recognition of cultural heritage materials},
	url = {http://droettboom.com/papers/p74-droettboom.pdf},
	booktitle = {Joint {Conference} on {Digital} {Libraries}},
	author = {Droettboom, Michael and Fujinaga, Ichiro and MacMillan, Karl and Chouhury, G. Sayeed and DiLauro, Tim and Patton, Mark and Anderson, Teal},
	year = {2002},
	pages = {12--17},
	file = {Droettboom et al_2002_Using the Gamera framework for the recognition of cultural heritage materials.pdf:/home/ptorras/zotpapers/Droettboom et al_2002_Using the Gamera framework for the recognition of cultural heritage materials.pdf:application/pdf},
}

@inproceedings{droettboom_symbol-level_2004,
	title = {Symbol-level groundtruthing environment for {OMR}},
	url = {http://ismir2004.ismir.net/proceedings/p090-page-497-paper117.pdf},
	booktitle = {5th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Droettboom, Michael and Fujinaga, Ichiro},
	year = {2004},
	pages = {497--500},
}

@inproceedings{dutta_efficient_2010,
	title = {An {Efficient} {Staff} {Removal} {Approach} from {Printed} {Musical} {Documents}},
	doi = {10.1109/ICPR.2010.484},
	abstract = {Staff removal is an important preprocessing step of the Optical Music Recognition (OMR). The process aims to remove the stafflines from a musical document and retain only the musical symbols, later these symbols are used effectively to identify the music information. This paper proposes a simple but robust method to remove stafflines from printed musical scores. In the proposed methodology we have considered a staffline segment as a horizontal linkage of vertical black runs with uniform height. We have used the neighbouring properties of a staffline segment to validate it as a true segment. We have considered the dataset along with the deformations described in for evaluation purpose. From experimentation we have got encouraging results.},
	booktitle = {20th {International} {Conference} on {Pattern} {Recognition}},
	author = {Dutta, Anjan and Pal, Umapada and Fornés, Alicia and Llados, Josep},
	year = {2010},
	note = {ISSN: 1051-4651},
	keywords = {Image segmentation, Error analysis, optical music recognition, Pattern recognition, OMR, optical character recognition, document image processing, music, Computer vision, musical symbols, Couplings, efficient staff removal approach, Emulation, music information, musical scores, Pixel, printed musical documents, printed musical scores, staff removal, staffline, staffline height, staffline segment, staffline segments, staffspace height},
	pages = {1965--1968},
}

@inproceedings{egozy_computer-assisted_2022,
	address = {Online},
	title = {Computer-{Assisted} {Measure} {Detection} in a {Music} {Score}-{Following} {Application}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Egozy, Eran and Clester, Ian},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {33--36},
	file = {Egozy_Clester_2022_Computer-Assisted Measure Detection in a Music Score-Following Application.pdf:/home/ptorras/zotpapers/Egozy_Clester_2022_Computer-Assisted Measure Detection in a Music Score-Following Application.pdf:application/pdf},
}

@inproceedings{eipert_editor_2019,
	address = {Delft, The Netherlands},
	title = {Editor {Support} for {Digital} {Editions} of {Medieval} {Monophonic} {Music}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Eipert, Tim and Herrman, Felix and Wick, Christoph and Puppe, Frank and Haug, Andreas},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {4--7},
}

@inproceedings{elezi_deepscores_2018,
	address = {Paris, France},
	title = {{DeepScores} and {Deep} {Watershed} {Detection}: current state and open issues},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Elezi, Ismail and Tuggener, Lukas and Pelillo, Marcello and Stadelmann, Thilo},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {13--14},
}

@mastersthesis{elezi_exploiting_2020,
	title = {Exploiting {Contextual} {Information} with {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/pdf/2006.11706.pdf},
	school = {Ca' Foscari, University of Venice},
	author = {Elezi, Ismail},
	year = {2020},
	file = {Elezi_2020_Exploiting Contextual Information with Deep Neural Networks.pdf:/home/ptorras/zotpapers/Elezi_2020_Exploiting Contextual Information with Deep Neural Networks.pdf:application/pdf},
}

@mastersthesis{essmayr_optische-musik-erkennung_1994,
	address = {Austria},
	title = {Optische-{Musik}-{Erkennung} ({OME}), {Erkennung} von {Notenschrift}},
	url = {https://www.cs.waikato.ac.nz/~davidb/omr/ftp/94-we.ps.gz},
	language = {German},
	school = {Johannes Kepler University Linz},
	author = {Essmayr, Wolfgang},
	year = {1994},
}

@incollection{fahmy_graph_1993,
	title = {Graph {Grammar} {Processing} of {Uncertain} {Data}},
	abstract = {Abstract Graph grammars may be used to extract the information content from diagrams where there is uncertainty about symbol identity. The input to the graph grammar is derived from the output of a symbol recognizer. We propose a way in which uncertainty can be represented by a graph and a method which extracts the information content of the diagram. We consider the application of graph grammars to the recognition of diagrams such as music scores.},
	booktitle = {Advances in {Structural} and {Syntactic} {Pattern} {Recognition}},
	publisher = {World Scientific},
	author = {Fahmy, Hoda M. and Blostein, Dorothea},
	year = {1993},
	doi = {10.1142/9789812797919\_0031},
	pages = {373--382},
}

@article{fahmy_graph_1993-1,
	title = {A graph grammar programming style for recognition of music notation},
	volume = {6},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/BF01211933},
	doi = {10.1007/BF01211933},
	abstract = {Graph grammars are a promising tool for solving picture processing problems. However, the application of graph grammars to diagram recognition has been limited to rather simple analysis of local symbol configurations. This paper introduces the Build-Weed-Incorporate programming style for graph grammars and shows its application in determining the meaning of complex diagrams, where the interaction among physically distant symbols is semantically important. Diagram recognition can be divided into two stages: symbol recognition and high-level recognition. Symbol recognition has been studied extensively in the literature. In this work we assume the existence of a symbol recognizer and use a graph grammar to assemble the diagram's information content from the symbols and their spatial relationships. The Build-Weed-Incorporate approach is demonstrated by a detailed discussion of a graph grammar for high-level recognition of music notation.},
	number = {2},
	journal = {Machine Vision and Applications},
	author = {Fahmy, Hoda M. and Blostein, Dorothea},
	year = {1993},
	pages = {83--99},
}

@inproceedings{fahmy_graph-rewriting_1994,
	title = {Graph-rewriting approach to discrete relaxation: application to music recognition},
	doi = {10.1117/12.171116},
	abstract = {In image analysis, low-level recognition of the primitives plays a very important role. Once the primitives of the image are recognized, depending on the application, many types of analyses can take place. It is likely that associated with each object or primitive is a set of possible interpretations, herein referred to as the label set. The low-level recognizer may associate a probability with each label in the label set. We can use the constraints of the application domain to reduce the ambiguity in the object's identity. This process is variously termed constraint satisfaction, labeling, or relaxation. In this paper, we focus on the discrete form of relaxation. Our contribution lies in the development of a graph-rewriting approach which does not assume the degree of localness is high. We apply our approach to the recognition of music notation, where non-local interactions between primitives must be used in order to reduce ambiguity in the identity of the primitives. We use graph-rewriting rules to express not only binary constraints, but also higher-order notational constraints.},
	booktitle = {International {Symposium} on {Electronic} {Imaging}: {Science} and {Technology}},
	author = {Fahmy, Hoda M. and Blostein, Dorothea},
	year = {1994},
	pages = {2181 -- 2181 -- 12},
}

@article{fahmy_graph-rewriting_1998,
	title = {A graph-rewriting paradigm for discrete relaxation: {Application} to sheet-music recognition},
	volume = {12},
	doi = {10.1142/S0218001498000439},
	abstract = {In image analysis, recognition of the primitives plays an important role. Subsequent analysis is used to interpret the arrangement of primitives. This subsequent analysis must make allowance for errors or ambiguities in the recognition of primitives. In this paper, we assume that the primitive recognizer produces a set of possible interpretations for each primitive. To reduce this primitive-recognition ambiguity, we use contextual information in the image, and apply constraints from the image domain. This process is variously termed constraint satisfaction, labeling or discrete relaxation. Existing methods for discrete relaxation are limited in that they assume a priori knowledge of the neighborhood model: before relaxation begins, the system is told (or can determine) which sets of primitives are related by constraints. These methods do not apply to image domains in which complex analysis is necessary to determine which primitives are related by constraints. For example, in music notation, we must recognize which notes belong to one measure, before it is possible to apply the constraint that the number of beats in the measure should match the time signature. Such constraints can be handled by our graph-rewriting paradigm for discrete relaxation: here neighborhood-model construction is interleaved with constraint-application. In applying this approach to the recognition of simple music notation, we use approximately 180 graph-rewriting rules to express notational constraints and semantic-interpretation rules far music notation. The graph rewriting rules express both binary and higher-order notational constraints. As image-interpretation proceeds, increasingly abstract levels of interpretation are assigned to (groups of) primitives. This allows application of higher-level constraints, which can be formulated only after partial interpretation of the image.},
	number = {6},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Fahmy, Hoda M. and Blostein, Dorothea},
	year = {1998},
	keywords = {graphics recognition, constraints, diagram understanding, discrete relaxation, document-image analysis, graph rewriting, interaction model, label set, languages, neighborhood model, optical music recognition Grammars, systems},
	pages = {763--799},
}

@article{fang_visual_2015,
	title = {Visual music score detection with unsupervised feature learning method based on {K}-means},
	volume = {6},
	issn = {1868-8071},
	doi = {10.1007/s13042-014-0260-2},
	abstract = {Automatic music score detection plays important role in the optical music recognition (OMR). In a visual image, the characteristic of the music scores is frequently degraded by illumination, distortion and other background elements. In this paper, to reduce the influences to OMR caused by those degradations especially the interference of Chinese character, an unsupervised feature learning detection method is proposed for improving the correctness of music score detection. Firstly, a detection framework was constructed. Then sub-image block features were extracted by simple unsupervised feature learning (UFL) method based on K-means and classified by SVM. Finally, music score detection processing was completed by connecting component searching algorithm based on the sub-image block label. Taking Chinese text as the main interferences, the detection rate was compared between UFL method and texture feature method based on 2D Gabor filter in the same framework. The experiment results show that unsupervised feature learning method gets less error detection rate than Gabor texture feature method with limited training set. © 2014, Springer-Verlag Berlin Heidelberg.},
	number = {2},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Fang, Yang and Gui-fa, Teng},
	year = {2015},
	note = {Publisher: Springer Verlag},
	keywords = {Optical music recognition, Feature extraction, Image processing, Music scores, Learning systems, Detection framework, Gabor, Gabor filters, Gabor texture features, Interference suppression, Searching algorithms, Textures, Unsupervised feature learning, Visual image},
	pages = {277--287},
}

@inproceedings{ferrand_scheduling_1998,
	address = {Berlin, Heidelberg},
	title = {Scheduling to {Reduce} {Uncertainty} in {Syntactical} {Music} {Structures}},
	isbn = {978-3-540-49523-9},
	doi = {10.1007/10692710_26},
	abstract = {In this paper, we focus on the syntactical aspects of music representation. We look at a music score as a structured layout of events with intrinsic temporal significance and we show that important basic relations between these events can be inferred from the topology of symbol objects in a music score. Within this framework, we propose a scheduling algorithm to find consistent assignments of events to voices, in the presence of uncertain information. Based on some experimental results, we show how we may use this approach to improve the accuracy of an Optical Music Recognition system.},
	booktitle = {Advances in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ferrand, Miguel and Cardoso, Amílcar},
	editor = {de Oliveira, Flávio Moreira},
	year = {1998},
	pages = {249--258},
}

@inproceedings{ferrand_hypothetical_1999,
	title = {Hypothetical reasoning: {An} application to {Optical} {Music} {Recognition}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.7672&rep=rep1&type=pdf},
	booktitle = {Appia-{Gulp}-{Prode}'99 joint conference on declarative programming},
	author = {Ferrand, Miguel and Leite, João Alexandre and Cardoso, Amilcar},
	year = {1999},
	pages = {367--381},
	file = {Ferrand et al_1999_Hypothetical reasoning.pdf:/home/ptorras/zotpapers/Ferrand et al_1999_Hypothetical reasoning.pdf:application/pdf},
}

@inproceedings{ferrand_improving_1999,
	address = {Berlin, Heidelberg},
	title = {Improving {Optical} {Music} {Recognition} by {Means} of {Abductive} {Constraint} {Logic} {Programming}},
	isbn = {978-3-540-48159-1},
	doi = {10.1007/3-540-48159-1_24},
	abstract = {In this paper we propose a hybrid system that bridges the gap between traditional image processing methods, used for low-level object recognition, and abductive constraint logic programming used for high-level musical interpretation. Optical Music Recognition (OMR) is the automatic recognition of a scanned page of printed music. All such systems are evaluated by their rate of successful recognition; therefore a reliable OMR program should be able to detect and eventually correct its own recognition errors. Since we are interested in dealing with polyphonic music, some additional complexity is introduced as several concurrent voices and simultaneous musical events may occur. In RIEM, the OMR system we are developing, when events are inaccurately recognized they will generate inconsistencies in the process of voice separation. Furthermore if some events are missing a consistent voice separation may not even be possible.},
	booktitle = {Progress in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ferrand, Miguel and Leite, João Alexandre and Cardoso, Amilcar},
	editor = {Barahona, Pedro and Alferes, José J.},
	year = {1999},
	pages = {342--356},
}

@mastersthesis{fornes_analysis_2005,
	title = {Analysis of {Old} {Handwritten} {Musical} {Scores}},
	url = {http://www.cvc.uab.es/~afornes/publi/AFornes_Master.pdf},
	school = {Universitat Autònoma de Barcelona},
	author = {Fornés, Alicia},
	year = {2005},
}

@inproceedings{fornes_primitive_2006,
	address = {Berlin, Heidelberg},
	title = {Primitive {Segmentation} in {Old} {Handwritten} {Music} {Scores}},
	isbn = {978-3-540-34712-5},
	doi = {10.1007/11767978_25},
	abstract = {Optical Music Recognition consists in the identification of music information from images of scores. In this paper, we propose a method for the early stages of the recognition: segmentation of staff lines and graphical primitives in handwritten scores. After introducing our work with modern musical scores (where projections and Hough Transform are effectively used), an approach to deal with ancient handwritten scores is exposed. The recognition of such these old scores is more difficult due to paper degradation and the lack of a standard in musical notation. Our method has been tested with several scores of 19th century with high performance rates.},
	booktitle = {Graphics {Recognition}. {Ten} {Years} {Review} and {Future} {Perspectives}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fornés, Alicia and Lladós, Josep and Sánchez, Gemma},
	editor = {Liu, Wenyin and Lladós, Josep},
	year = {2006},
	keywords = {Grammar Rule, High Performance Rate, Hough Transform, Musical Symbol, Zernike Moment},
	pages = {279--290},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/J4UKQBE4/Fornés et al. - 2006 - Primitive Segmentation in Old Handwritten Music Sc.pdf:application/pdf},
}

@inproceedings{fornes_writer_2008,
	address = {Nara, Japan},
	title = {Writer {Identification} in {Old} {Handwritten} {Music} {Scores}},
	doi = {10.1109/DAS.2008.29},
	abstract = {The aim of writer identification is determining the writer of a piece of handwriting from a set of writers. In this paper we present a system for writer identification in old handwritten music scores. Even though an important amount of compositions contains handwritten text in the music scores, the aim of our work is to use only music notation to determine the author. The steps of the system proposed are the following. First of all, the music sheet is preprocessed and normalized for obtaining a single binarized music line, without the staff lines. Afterwards, 100 features are extracted for every music line, which are subsequently used in a k-NN classifier that compares every feature vector with prototypes stored in a database. By applying feature selection and extraction methods on the original feature set, the performance is increased. The proposed method has been tested on a database of old music scores from the 17th to 19th centuries, achieving a recognition rate of about 95\%.},
	booktitle = {8th {International} {Workshop} on {Document} {Analysis} {Systems}},
	author = {Fornés, Alicia and Lladós, Josep and Sánchez, Gemma and Bunke, Horst},
	year = {2008},
	keywords = {Testing, Computer science, Handwriting recognition, Mathematics, Text analysis, Feature extraction, document analysis, Image recognition, handwriting recognition, Prototypes, feature extraction, music, music notation, Computer vision, document handling, binarized music line, feature selection, Handwritten recognition, handwritten text, k-NN classifier, music sheet preprocessing, Old documents, old handwritten music scores, pattern classification, Spatial databases, writer identification, Writer Identification},
	pages = {347--353},
	file = {Fornes et al_2008_Writer Identification in Old Handwritten Music Scores.pdf:/home/ptorras/zotpapers/Fornes et al_2008_Writer Identification in Old Handwritten Music Scores.pdf:application/pdf},
}

@article{fornes_use_2009,
	title = {On the {Use} of {Textural} {Features} for {Writer} {Identification} in {Old} {Handwritten} {Music} {Scores}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5277541},
	doi = {10.1109/ICDAR.2009.100},
	journal = {10th International Conference on Document Analysis and Recognition},
	author = {Fornés, Alicia and Lladós, Josep and Sánchez, Gemma and Bunke, Horst},
	year = {2009},
	note = {ISBN: 978-1-4244-4500-4},
	keywords = {writer features},
	pages = {996--1000},
	file = {Fornes et al_2009_On the Use of Textural Features for Writer Identification in Old Handwritten.pdf:/home/ptorras/zotpapers/Fornes et al_2009_On the Use of Textural Features for Writer Identification in Old Handwritten.pdf:application/pdf},
}

@phdthesis{fornes_writer_2009,
	type = {{PhD} {Thesis}},
	title = {Writer {Identification} by a {Combination} of {Graphical} {Features} in the {Framework} of {Old} {Handwritten} {Music} {Scores}},
	url = {http://www.cvc.uab.es/~afornes/publi/PhDAliciaFornes.pdf},
	school = {Universitat Autònoma de Barcelona},
	author = {Fornés, Alicia},
	year = {2009},
}

@inproceedings{fornes_icdar_2011,
	title = {The {ICDAR} 2011 {Music} {Scores} {Competition}: {Staff} {Removal} and {Writer} {Identification}},
	doi = {10.1109/ICDAR.2011.300},
	abstract = {In the last years, there has been a growing interest in the analysis of handwritten music scores. In this sense, our goal has been to foster the interest in the analysis of handwritten music scores by the proposal of two different competitions: Staff removal and Writer Identification. Both competitions have been tested on the CVC-MUSCIMA database: a ground-truth of handwritten music score images. This paper describes the competition details, including the dataset and ground-truth, the evaluation metrics, and a short description of the participants, their methods, and the obtained results.},
	booktitle = {International {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Fornés, Alicia and Dutta, Anjan and Gordo, Albert and Llados, Josep},
	year = {2011},
	note = {ISSN: 2379-2140},
	keywords = {Handwriting recognition, Databases, Music, Error analysis, Measurement, Support vector machines, music, handwritten character recognition, Educational institutions, music scores, staff removal, writer identification, competition, CVC-MUSCIMA database, evaluation metrics, handwritten music score images, ICDAR 2011 music scores competition},
	pages = {1511--1515},
}

@inproceedings{fornes_2012_2013,
	address = {Berlin, Heidelberg},
	title = {The 2012 {Music} {Scores} {Competitions}: {Staff} {Removal} and {Writer} {Identification}},
	isbn = {978-3-642-36824-0},
	doi = {10.1007/978-3-642-36824-0_17},
	abstract = {Since there has been a growing interest in the analysis of handwritten music scores, we have tried to foster this interest by proposing in ICDAR and GREC two different competitions: Staff removal and Writer identification. Both competitions have been tested on the CVC-MUSCIMA database of handwritten music score images. In the corresponding ICDAR publication, we have described the ground-truth, the evaluation metrics, the participants' methods and results. As a result of the discussions with attendees in ICDAR and GREC concerning our music competition, we decided to propose a new experiment for an extended competition. Thus, this paper is focused on this extended competition, describing the new set of images and analyzing the new results.},
	booktitle = {Graphics {Recognition}. {New} {Trends} and {Challenges}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fornés, Alicia and Dutta, Anjan and Gordo, Albert and Lladós, Josep},
	editor = {Kwon, Young-Bin and Ogier, Jean-Marc},
	year = {2013},
	pages = {173--186},
}

@inproceedings{fornes_icdargrec_2014,
	address = {Berlin, Heidelberg},
	title = {The {ICDAR}/{GREC} 2013 {Music} {Scores} {Competition}: {Staff} {Removal}},
	isbn = {978-3-662-44854-0},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-44854-0_16},
	abstract = {The first competition on music scores that was organized at ICDAR and GREC in 2011 awoke the interest of researchers, who participated in both staff removal and writer identification tasks. In this second edition, we focus on the staff removal task and simulate a real case scenario concerning old and degraded music scores. For this purpose, we have generated a new set of semi-synthetic images using two degradation models that we previously introduced: local noise and 3D distortions. In this extended paper we provide an extended description of the dataset, degradation models, evaluation metrics, the participant's methods and the obtained results that could not be presented at ICDAR and GREC proceedings due to page limitations.},
	booktitle = {Graphics {Recognition}. {Current} {Trends} and {Challenges}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fornés, Alicia and Kieu, Van Cuong and Visani, Muriel and Journet, Nicholas and Dutta, Anjan},
	editor = {Lamiroy, Bart and Ogier, Jean-Marc},
	year = {2014},
	pages = {207--220},
}

@book{fornes_graphics_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Graphics {Recognition}, {Current} {Trends} and {Evolutions}},
	volume = {11009},
	isbn = {978-3-030-02283-9},
	publisher = {Springer International Publishing},
	editor = {Fornés, Alicia and Bart, Lamiroy},
	year = {2018},
	doi = {10.1007/978-3-030-02284-6},
	file = {Fornes_Bart_2018_Graphics Recognition, Current Trends and Evolutions.pdf:/home/ptorras/zotpapers/Fornes_Bart_2018_Graphics Recognition, Current Trends and Evolutions.pdf:application/pdf},
}

@inproceedings{fotinea_optical_2000,
	address = {Paris, France},
	title = {An {Optical} {Notation} {Recognition} {System} for {Printed} {Music} {Based} on {Template} {Matching} and {High} {Level} {Reasoning}},
	url = {http://dl.acm.org/citation.cfm?id=2856151.2856159},
	booktitle = {{RIAO} '00 {Content}-{Based} {Multimedia} {Information} {Access}},
	publisher = {Le centre de hautes etudes internationales d'informatique documentaire},
	author = {Fotinea, Stavroula-Evita and Giakoupis, George and Livens, Aggelos and Bakamidis, Stylianos and Carayannis, George},
	year = {2000},
	pages = {1006--1014},
}

@inproceedings{fremerey_automatic_2008,
	title = {Automatic {Mapping} of {Scanned} {Sheet} {Music} to {Audio} {Recordings}},
	isbn = {978-0-615-24849-3},
	url = {http://ismir2008.ismir.net/papers/ISMIR2008_116.pdf},
	abstract = {Significant digitization efforts have resulted in large multimodal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are first transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven’s piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material.},
	booktitle = {9th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Fremerey, Christian and Müller, Meinard and Kurth, Frank and Clausen, Michael},
	year = {2008},
	pages = {413--418},
	file = {Fremerey et al_2008_Automatic Mapping of Scanned Sheet Music to Audio Recordings.pdf:/home/ptorras/zotpapers/Fremerey et al_2008_Automatic Mapping of Scanned Sheet Music to Audio Recordings.pdf:application/pdf},
}

@inproceedings{fremerey_handling_2009,
	title = {Handling {Scanned} {Sheet} {Music} and {Audio} {Recordings} in {Digital} {Music} {Libraries}},
	url = {https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/03-publications/2009_FremereyDaMuKuCl_ScanAudio_DAGA.pdf},
	booktitle = {International {Conference} on {Acoustics} {NAG}/{DAGA}},
	author = {Fremerey, Christian and Damm, David and Kurth, Frank and Clausen, Michael},
	year = {2009},
	pages = {1--2},
	file = {Fremerey et al_2009_Handling Scanned Sheet Music and Audio Recordings in Digital Music Libraries.pdf:/home/ptorras/zotpapers/Fremerey et al_2009_Handling Scanned Sheet Music and Audio Recordings in Digital Music Libraries.pdf:application/pdf},
}

@inproceedings{fuente_multimodal_2021,
	address = {Alicante, Spain},
	title = {Multimodal {Audio} and {Image} {Music} {Transcription}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Fuente, Carlos de la and Valero-Mas, Jose J. and Castellanos, Francisco J. and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {18--22},
}

@mastersthesis{fujinaga_optical_1988,
	title = {Optical {Music} {Recognition} using {Projections}},
	url = {https://www.researchgate.net/profile/Ichiro_Fujinaga/publication/38435306_Optical_music_recognition_using_projections/links/546ca7980cf24b753c628c6e.pdf},
	abstract = {This research examines the feasibility of implementing an optical music score recognition system on a microcomputer. Projection technique is the principal mcthod employed in the recognition process, assisted by some of the structural roles governing musical notation. Musical examples, excerpted mostly from solo repertoire for monophonic instruments and representing various publishers, are used as samples to develop a computer program that recognizes a set of musical symbols. A final test of the system is undertaken, involving additional samples of monophohnic music which were not used in the development stage. With these samples, an average recognition rate of 70\% is attained without any operator intervention. On an IMB-AT-compatible microcomputer, the total processing time including the scanning operation is about two minutes per page.},
	school = {McGill University},
	author = {Fujinaga, Ichiro},
	year = {1988},
	file = {Fujinaga_1988_Optical Music Recognition using Projections.pdf:/home/ptorras/zotpapers/Fujinaga_1988_Optical Music Recognition using Projections.pdf:application/pdf},
}

@inproceedings{fujinaga_optical_1993,
	title = {Optical music recognition system which learns},
	doi = {10.1117/12.139262},
	abstract = {This paper describes an optical music recognition system composed of a database and three interdependent processes: a recognizer, an editor, and a learner. Given a scanned image of a musical score, the recognizer locates, separates, and classifies symbols into musically meaningful categories. This classification is based on the k-nearest neighbor method using a subset of the database that contains features of symbols classified in previous recognition sessions. Output of the recognizer is corrected by a musically trained human operator using a music notation editor. The editor provides both visual and high-quality audio feedback of the output. Editorial corrections made by the operator are passed to the learner which then adds the newly acquired data to the database. The learner's main task, however, involves selecting a subset of the database and reweighing the importance of the features to improve accuracy and speed for subsequent sessions. Good preliminary results have been obtained with everything from professionally engraved scores to hand-written manuscripts.},
	booktitle = {Enabling {Technologies} for {High}-{Bandwidth} {Applications}},
	author = {Fujinaga, Ichiro},
	year = {1993},
}

@inproceedings{fujinaga_exemplar-based_1996,
	address = {Hong Kong},
	title = {Exemplar-based learning in adaptive optical music recognition system},
	isbn = {962-85092-1-7},
	url = {http://hdl.handle.net/2027/spo.bbp2372.1996.015},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Fujinaga, Ichiro},
	year = {1996},
	pages = {55--56},
}

@phdthesis{fujinaga_adaptive_1996,
	type = {{PhD} {Thesis}},
	title = {Adaptive optical music recognition},
	url = {http://www.music.mcgill.ca/~ich/research/diss/FujinagaDiss.pdf},
	school = {McGill University},
	author = {Fujinaga, Ichiro},
	year = {1996},
	keywords = {Musical notation -- Data processing., Optical character recognition devices.},
	file = {Fujinaga - Adaptive optical music recognition.pdf:/home/ptorras/Zotero/storage/FHKDBHSG/Fujinaga - Adaptive optical music recognition.pdf:application/pdf;Fujinaga_1996_Adaptive optical music recognition.pdf:/home/ptorras/zotpapers/Fujinaga_1996_Adaptive optical music recognition.pdf:application/pdf;Thesis | Adaptive optical music recognition | ID\: 7w62f989f | eScholarship@McGill:/home/ptorras/Zotero/storage/UUCPINH4/7w62f989f.html:text/html},
}

@inproceedings{fujinaga_implementation_1998,
	address = {Seoul, South Korea},
	title = {Implementation of exemplar-based learning model for music cognition},
	url = {https://pdfs.semanticscholar.org/8d27/309a9070c5737a1eb5fa7ef5dfc6c9484b89.pdf},
	booktitle = {International {Conference} on {Music} {Perception} and {Cognition}},
	author = {Fujinaga, Ichiro and Moore, Stephan and Sullivan, David S.},
	year = {1998},
	pages = {171--179},
	file = {Fujinaga et al_1998_Implementation of exemplar-based learning model for music cognition.pdf:/home/ptorras/zotpapers/Fujinaga et al_1998_Implementation of exemplar-based learning model for music cognition.pdf:application/pdf},
}

@misc{fujinaga_optical_2000,
	title = {Optical {Music} {Recognition} {Bibliography}},
	url = {http://www.music.mcgill.ca/~ich/research/omr/omrbib.html},
	author = {Fujinaga, Ichiro},
	year = {2000},
	keywords = {Optical Music Recognition, OMR, Bibliography, References},
}

@incollection{fujinaga_staff_2004,
	title = {Staff detection and removal},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IGI Global},
	author = {Fujinaga, Ichiro},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch001},
	pages = {1--39},
}

@inproceedings{fujinaga_introduction_2014,
	title = {Introduction to {SIMSSA} ({Single} {Interface} for {Music} {Score} {Searching} and {Analysis})},
	doi = {10.1145/2660168.2660184},
	booktitle = {1st {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Fujinaga, Ichiro and Hankinson, Andrew and Cumming, Julie E.},
	year = {2014},
	pages = {1--3},
}

@article{fujinaga_simssa_2014,
	title = {{SIMSSA}: {Single} {Interface} for {Music} {Score} {Searching} and {Analysis}},
	volume = {6},
	url = {http://data.jssa.info/paper/2014v06n03/7.Fujinaga.pdf},
	number = {3},
	journal = {Journal of the Japanese Society for Sonic Arts},
	author = {Fujinaga, Ichiro and Hankinson, Andrew},
	year = {2014},
	pages = {25--30},
	file = {Fujinaga_Hankinson_2014_SIMSSA.pdf:/home/ptorras/zotpapers/Fujinaga_Hankinson_2014_SIMSSA.pdf:application/pdf},
}

@incollection{fujinaga_automatic_2018,
	address = {Berlin, Heidelberg},
	title = {Automatic {Score} {Extraction} with {Optical} {Music} {Recognition} ({OMR})},
	isbn = {978-3-662-55004-5},
	abstract = {Optical music recognition (OMR optical music recognition (OMR) ) describes the process of automatically transcribing music notation from a digital image. Although similar to optical character recognition (OCR optical character recognition (OCR) ), the process and procedures of OMR diverge due to the fundamental differences between text and music notation, such as the two-dimensional nature of the notation system and the overlay of music symbols on top of staff lines. The OMR process can be described as a sequence of steps, with techniques adapted from disciplines including image processing, machine learning, grammars, and notation encoding. The sequence and specific techniques used can differ depending on the condition of the image, the type of notation, and the desired output.},
	booktitle = {Springer {Handbook} of {Systematic} {Musicology}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fujinaga, Ichiro and Hankinson, Andrew and Pugin, Laurent},
	year = {2018},
	doi = {10.1007/978-3-662-55004-5_16},
	pages = {299--311},
	file = {Fujinaga et al_2018_Automatic Score Extraction with Optical Music Recognition (OMR).pdf:/home/ptorras/zotpapers/Fujinaga et al_2018_Automatic Score Extraction with Optical Music Recognition (OMR).pdf:application/pdf},
}

@techreport{galea_review_2014,
	title = {A review on printed music recognition system developed in institute of computer science iaşi},
	url = {http://www12.tuiasi.ro/users/103/Buletin_2014_1_49-66_4_Galea__AC%201_2014.pdf},
	number = {Lxiv},
	institution = {Universitatea Tehnicǎ Gheorghe Asachi din Iaşi},
	author = {Gâlea, Dan and Rotaru, Florin and Bejinariu, Silviu-Ioan and Bulea, Mihai and Murgu, Dan and Pescaru, Simona and Apopei, Vasile and Murgu, Mihaela and Rusu, Irina},
	year = {2014},
	keywords = {classification, to classify, 2010 mathematics subject classification, 68t10, 68u10, musical information reconstruction, musical symbols recognition, staff lines detection, symbol},
}

@article{gallego_staff-line_2017,
	title = {Staff-line removal with selectional auto-encoders},
	volume = {89},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417304712},
	doi = {10.1016/j.eswa.2017.07.002},
	abstract = {Abstract Staff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images.},
	journal = {Expert Systems with Applications},
	author = {Gallego, Antonio-Javier and Calvo-Zaragoza, Jorge},
	year = {2017},
	keywords = {Staff-line removal},
	pages = {138--148},
	file = {Gallego_Calvo-Zaragoza_2017_Staff-line removal with selectional auto-encoders.pdf:/home/ptorras/zotpapers/Gallego_Calvo-Zaragoza_2017_Staff-line removal with selectional auto-encoders.pdf:application/pdf},
}

@inproceedings{gan_musica_2005,
	address = {Denver, USA},
	title = {Música {Colonial}: 18th {Century} {Music} {Score} {Meets} 21st {Century} {Digitalization} {Technology}},
	isbn = {1-58113-876-8},
	doi = {10.1145/1065385.1065482},
	booktitle = {5th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {ACM},
	author = {Gan, Ting},
	year = {2005},
	keywords = {music score transcription, music sheet digitization},
	pages = {379--379},
}

@inproceedings{garrido-munoz_end--end_2022,
	address = {Online},
	title = {End-to-{End} {Graph} {Prediction} for {Optical} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Garrido-Munoz, Carlos and Ríos-Vila, Antonio and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {25--28},
	file = {Garrido-Munoz et al_2022_End-to-End Graph Prediction for Optical Music Recognition.pdf:/home/ptorras/zotpapers/Garrido-Munoz et al_2022_End-to-End Graph Prediction for Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{genfang_pick-up_2009,
	title = {Pick-up the {Musical} {Information} from {Digital} {Musical} {Score} {Based} on {Mathematical} {Morphology} and {Music} {Notation}},
	doi = {10.1109/ETCS.2009.261},
	abstract = {The basic rule of musical notation for image processing is analyzed, in this paper. Using the structuring elements of musical notation and the basic algorithms of mathematical morphology, a new recognizing for the musical information of digital musical score is presented, and then the musical information is transformed to MIDI file for the communication and restoration of musical score. The results of experiment show that the statistic average value of recognition rate for musical information from digital musical score is 94.4\%, and can be satisfied the practical applied demand, and it is a new way for applications of digital library, musical education, musical theory analysis and so on.},
	booktitle = {1st {International} {Workshop} on {Education} {Technology} and {Computer} {Science}},
	author = {Genfang, Chen and Wenjun, Zhang and Qiuqiu, Wang},
	year = {2009},
	keywords = {Software libraries, Music, Image recognition, Image processing, music, Image analysis, image recognition, music notation, Morphology, Music notation, Information analysis, Computer science education, digital library, digital musical score, image processing, Image restoration, mathematical morphology, Mathematical morphology, MIDI, MIDI file, musical education, musical information, Musical score, musical theory analysis, TV},
	pages = {1141--1144},
}

@article{george_online_2003,
	title = {Online {Pen}-{Based} {Recognition} of {Music} {Notation} with {Artificial} {Neural} {Networks}},
	volume = {27},
	doi = {10.1162/014892603322022673},
	number = {2},
	journal = {Computer Music Journal},
	author = {George, Susan E.},
	year = {2003},
	pages = {70--79},
}

@book{george_visual_2004,
	title = {Visual {Perception} of {Music} {Notation} {On}-{Line} and {Off}-{Line} {Recognition}},
	isbn = {1-931777-94-2},
	url = {https://books.google.at/books?isbn=1591402980},
	publisher = {IRM Press},
	author = {George, Susan E.},
	year = {2004},
}

@incollection{george_evaluation_2004,
	address = {Hershey, PA},
	title = {Evaluation in the {Visual} {Perception} of {Music} {Notation}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IRM Press},
	author = {George, Susan E.},
	editor = {George, S.},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch010},
	pages = {304--349},
}

@incollection{george_lyric_2004,
	address = {Hershey, PA},
	title = {Lyric {Recognition} and {Christian} {Music}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IRM Press},
	author = {George, Susan E.},
	editor = {George, S.},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch007},
	pages = {198--226},
}

@incollection{george_wavelets_2004,
	address = {Hershey, PA},
	title = {Wavelets for {Dealing} with {Super}-{Imposed} {Objects} in {Recognition} of {Music} {Notation}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IRM Press},
	author = {George, Susan E.},
	editor = {George, S.},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch003},
	pages = {78--107},
}

@incollection{george_pen-based_2004,
	address = {Hershey, PA},
	title = {Pen-{Based} {Input} for {On}-{Line} {Handwritten} {Music} {Notation}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IRM Press},
	author = {George, Susan E.},
	editor = {George, S.},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch005},
	pages = {128--160},
}

@inproceedings{geraud_morphological_2014,
	title = {A morphological method for music score staff removal},
	isbn = {978-1-4799-5751-4},
	doi = {10.1109/ICIP.2014.7025526},
	abstract = {Removing the staff in music score images is a key to improve the recognition of music symbols and, with ancient and degraded handwritten music scores, it is not a straightforward task. In this paper we present the method that has won in 2013 the staff removal competition, organized at the International Conference on Document Analysis and Recognition (ICDAR). The main characteristics of this method is that it essentially relies on mathematical morphology filtering. So it is simple, fast, and its full source code is provided to favor reproducible research. © 2014 IEEE.},
	booktitle = {International {Conference} on {Image} {Processing}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Géraud, Thierry},
	year = {2014},
	keywords = {Image processing, Character recognition, Document image analysis, Music scores, Document analysis, Mathematical morphology, Filtration, Information retrieval systems, Mathematical morphology filtering, Reproducible research, Source codes},
	pages = {2599--2603},
}

@article{gezerlis_optical_2002,
	title = {Optical character recognition of the {Orthodox} {Hellenic} {Byzantine} {Music} notation},
	volume = {35},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S003132030100098X},
	doi = {https://doi.org/10.1016/S0031-3203(01)00098-X},
	abstract = {In this paper we present for the first time, the development of a new system for the off-line optical recognition of the characters used in the orthodox Hellenic Byzantine Music notation, that has been established since 1814. We describe the structure of the new system and propose algorithms for the recognition of the 71 distinct character classes, based on Wavelets, 4-projections and other structural and statistical features. Using a nearest neighbor classifier, combined with a post classification schema and a tree-structured classification philosophy, an accuracy of 99.4\% was achieved, in a database of about 18,000 Byzantine character patterns that have been developed for the needs of the system.},
	number = {4},
	journal = {Pattern Recognition},
	author = {Gezerlis, Velissarios G. and Theodoridis, Sergios},
	year = {2002},
	keywords = {Neural networks, Optical music recognition, Byzantine Music, Byzantine Music database, Byzantine Music notation, Contour processing, Nearest neighbor classifier, Off-line character recognition, Projections, Wavelets},
	pages = {895--914},
}

@inproceedings{gocke_building_2003,
	title = {Building a system for writer identification on handwritten music scores},
	isbn = {0 88986 363 6},
	url = {http://users.cecs.anu.edu.au/~roland/Publications/Goecke_SPPRA2003.pdf},
	abstract = {A significant example of the integration of musicology and computer science. The problem of writer identification process by historical musicologists is identified and possible solutions by computer technology are assessed. The system outline is unique and seems convincing including the interesting ideas such as the feature trees and consistency check. However, it lacks any concrete methods to implement the proposed system and any evaluation.},
	booktitle = {{IASTED} {International} {Conference} on {Signal} {Processing}, {Pattern} {Recognition}, and {Applications}},
	publisher = {Acta Press},
	author = {Göcke, Roland},
	year = {2003},
	keywords = {music scores, writer features, image processing, handwriting identifica-},
	pages = {250--255},
	file = {Gocke_2003_Building a system for writer identification on handwritten music scores.pdf:/home/ptorras/zotpapers/Gocke_2003_Building a system for writer identification on handwritten music scores.pdf:application/pdf},
}

@article{gomez_optical_2017,
	title = {Optical {Music} {Recognition}: {Staffline} {Detection} and {Removal}},
	journal = {International Journal of Application or Innovation in Engineering \& Management},
	author = {Gomez, Ashley Antony and Sujatha, C. N.},
	year = {2017},
}

@article{gordo_writer_2013,
	title = {Writer identification in handwritten musical scores with bags of notes},
	volume = {46},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320312004475},
	doi = {https://doi.org/10.1016/j.patcog.2012.10.013},
	abstract = {Writer Identification is an important task for the automatic processing of documents. However, the identification of the writer in graphical documents is still challenging. In this work, we adapt the Bag of Visual Words framework to the task of writer identification in handwritten musical scores. A vanilla implementation of this method already performs comparably to the state-of-the-art. Furthermore, we analyze the effect of two improvements of the representation: a Bhattacharyya embedding, which improves the results at virtually no extra cost, and a Fisher Vector representation that very significantly improves the results at the cost of a more complex and costly representation. Experimental evaluation shows results more than 20 points above the state-of-the-art in a new, challenging dataset.},
	number = {5},
	journal = {Pattern Recognition},
	author = {Gordo, Albert and Fornés, Alicia and Valveny, Ernest},
	year = {2013},
	keywords = {Writer identification, Bag of notes, Handwritten musical scores},
	pages = {1337--1345},
}

@inproceedings{gotham_scores_2018,
	address = {Paris, France},
	title = {Scores of {Scores}: {An} {Openscore} {Project} to {Encode} and {Share} {Sheet} {Music}},
	isbn = {978-1-4503-6522-2},
	url = {http://doi.acm.org/10.1145/3273024.3273026},
	doi = {10.1145/3273024.3273026},
	booktitle = {5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Gotham, Mark and Jonas, Peter and Bower, Bruno and Bosworth, William and Rootham, Daniel and VanHandel, Leigh},
	year = {2018},
	keywords = {music information retrieval, musical scores, corpus study, crowdsourcing, digital music library, song},
	pages = {87--95},
}

@inproceedings{goularas_optical_2019,
	title = {Optical {Music} {Recognition} of the {Hamparsum} {Notation}},
	doi = {10.1109/IPTA.2019.8936130},
	abstract = {This paper presents a method for the recognition of music notes from the Hamparsum music notation system. This notation was widely used during the last two centuries of the Ottoman Empire and it is still in use today. The Hamparsum notation presents significant differences compared to the European music notation, in terms of symbols and structure. Moreover, the notes can consist of more than one individual symbols. The proposed recognition method comprises several steps and algorithms, including a feature extraction based on Gabor Filters, recognition of symbols using a Support Vector Machine classifier, a method for assigning recognized symbols to a candidate Hamparsum note and a final recognition system based on template matching. This work will help to popularize this unique cultural heritage by providing Hamparsum scores in a machine-readable format.},
	booktitle = {2019 {Ninth} {International} {Conference} on {Image} {Processing} {Theory}, {Tools} and {Applications} ({IPTA})},
	author = {Goularas, Dionysis and Çınar, Kürşat},
	month = nov,
	year = {2019},
	note = {ISSN: 2154-5111},
	keywords = {Machine Learning, Optical Music Recognition, Image Processing, Feature Extraction},
	pages = {1--7},
}

@inproceedings{gover_notation-based_2019,
	address = {New York, NY, USA},
	series = {{DLfM} ’19},
	title = {A {Notation}-{Based} {Query} {Language} for {Searching} in {Symbolic} {Music}},
	isbn = {978-1-4503-7239-8},
	url = {https://doi.org/10.1145/3358664.3358667},
	doi = {10.1145/3358664.3358667},
	booktitle = {6th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Gover, Matan and Fujinaga, Ichiro},
	year = {2019},
	note = {event-place: The Hague, Netherlands},
	keywords = {computational musicology, Humdrum, Music Encoding Initiative, music searching, query language, regular expressions, symbolic music},
	pages = {79--83},
}

@mastersthesis{gozzi_omrjx_2010,
	title = {{OMRJX}: {A} framework for piano scores optical music recognition},
	url = {https://www.politesi.polimi.it/bitstream/10589/12761/3/2011_03_Gozzi.pdf},
	school = {Politecnico di Milano},
	author = {Gozzi, Gianmarco},
	year = {2010},
	file = {Gozzi_2010_OMRJX.pdf:/home/ptorras/zotpapers/Gozzi_2010_OMRJX.pdf:application/pdf},
}

@inproceedings{hajic_jr_further_2016,
	address = {New York, USA},
	title = {Further {Steps} towards a {Standard} {Testbed} for {Optical} {Music} {Recognition}},
	isbn = {978-0-692-75506-8},
	url = {https://wp.nyu.edu/ismir2016/event/proceedings/},
	booktitle = {17th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {New York University},
	author = {Hajič jr., Jan and Novotný, Jiří and Pecina, Pavel and Pokorný, Jaroslav},
	editor = {Mandel, Michael and Devaney, Johanna and Turnbull, Douglas and Tzanetakis, George},
	year = {2016},
	note = {Backup Publisher: New York University},
	pages = {157--163},
	file = {Pokorny - 2016 - 1 Charles University, Institute of Formal and Appl.pdf:/home/ptorras/Zotero/storage/TX7F9BZT/Pokorny - 2016 - 1 Charles University, Institute of Formal and Appl.pdf:application/pdf},
}

@inproceedings{hajic_jr_prototyping_2017,
	address = {Suzhou, China},
	title = {Prototyping {Full}-{Pipeline} {Optical} {Music} {Recognition} with {MUSCIMarker}},
	url = {https://ismir2017.smcnus.org/lbds/Hajic2017.pdf},
	booktitle = {Extended abstracts for the {Late}-{Breaking} {Demo} {Session} of the 18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Hajič jr., Jan and Dorfer, Matthias},
	year = {2017},
}

@inproceedings{hajic_jr_groundtruthing_2017,
	address = {Kyoto, Japan},
	title = {Groundtruthing ({Not} {Only}) {Music} {Notation} with {MUSICMarker}: {A} {Practical} {Overview}},
	doi = {10.1109/ICDAR.2017.271},
	abstract = {Dataset creation for graphics recognition, especially for hand-drawn inputs, is often an expensive and time-consuming undertaking. The MUSCIMarker tool used for creating the MUSCIMA++ dataset for Optical Music Recognition (OMR) led to efficient use of annotation resources, and it provides enough flexibility to be applicable to creating datasets for other graphics recognition tasks where the ground truth can be represented similarly. First, we describe the MUSCIMA++ ground truth to define the range of tasks for which using MUSCIMarker to annotate ground truth is applicable. We then describe the MUSCIMarker tool itself, discuss its strong and weak points, and share practical experience with the tool from creating the MUSCIMA++ dataset.},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Hajič jr., Jan and Pecina, Pavel},
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical imaging, Optical Music Recognition, Text analysis, Graphics, Tools, Grammar, Integrated optics, music, music notation, computer graphics, Annotation Tools, dataset creation, Dataset Creation, graphics recognition tasks, MUSCIMA++ dataset, MUSCIMarker tool, MUSICMarker, Optical feedback},
	pages = {47--48},
}

@inproceedings{hajic_jr_towards_2018,
	address = {Paris, France},
	title = {Towards {Full}-{Pipeline} {Handwritten} {OMR} with {Musical} {Symbol} {Detection} by {U}-{Nets}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/175_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Hajič jr., Jan and Dorfer, Matthias and Widmer, Gerhard and Pecina, Pavel},
	year = {2018},
	pages = {225--232},
	file = {Pecina - 2018 - 1 Institute of Formal and Applied Linguistics, Cha.pdf:/home/ptorras/Zotero/storage/H7EMGI82/Pecina - 2018 - 1 Institute of Formal and Applied Linguistics, Cha.pdf:application/pdf},
}

@inproceedings{hajic_jr_case_2018,
	address = {Paris, France},
	title = {A {Case} for {Intrinsic} {Evaluation} of {Optical} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Hajič jr., Jan},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {15--16},
}

@article{hakim_convolutional_2019,
	title = {Convolutional {Neural} {Network} untuk {Pengenalan} {Citra} {Notasi} {Musik}},
	volume = {18},
	issn = {2356-2579},
	url = {http://publikasi.dinus.ac.id/index.php/technoc/article/view/2387},
	doi = {10.33633/tc.v18i3.2387},
	language = {Indonesian},
	number = {3},
	journal = {Techno.COM},
	author = {Hakim, Dzikry Maulana and Rainarli, Ednawati},
	year = {2019},
	pages = {214--226},
	file = {Hakim_Rainarli_2019_Convolutional Neural Network untuk Pengenalan Citra Notasi Musik.pdf:/home/ptorras/zotpapers/Hakim_Rainarli_2019_Convolutional Neural Network untuk Pengenalan Citra Notasi Musik.pdf:application/pdf},
}

@article{han_optical_2014,
	title = {Optical {Music} {Score} {Recognition} {System} for {Smart} {Mobile} {Devices}},
	volume = {10},
	doi = {10.5392/IJoC.2014.10.4.063},
	number = {4},
	journal = {International Journal of Contents},
	author = {Han, Sejin and Lee, Gueesang},
	year = {2014},
	keywords = {music recognition, to classify, music ocr, optical music score recognition},
	pages = {63--68},
	file = {Han_Lee_2014_Optical Music Score Recognition System for Smart Mobile Devices.pdf:/home/ptorras/zotpapers/Han_Lee_2014_Optical Music Score Recognition System for Smart Mobile Devices.pdf:application/pdf},
}

@inproceedings{hankinson_interchange_2010,
	address = {Utrecht, The Netherlands},
	title = {An {Interchange} {Format} for {Optical} {Music} {Recognition} {Applications}},
	url = {http://ismir2010.ismir.net/proceedings/ismir2010-11.pdf},
	booktitle = {11th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Hankinson, Andrew and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2010},
	pages = {51--56},
	file = {Hankinson et al_2010_An Interchange Format for Optical Music Recognition Applications.pdf:/home/ptorras/zotpapers/Hankinson et al_2010_An Interchange Format for Optical Music Recognition Applications.pdf:application/pdf},
}

@inproceedings{hankinson_digital_2012,
	title = {Digital {Document} {Image} {Retrieval} {Using} {Optical} {Music} {Recognition}},
	url = {http://ismir2012.ismir.net/event/papers/577-ismir-2012.pdf},
	booktitle = {13th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Hankinson, Andrew and Burgoyne, John Ashley and Vigliensoni, Gabriel and Porter, Alastair and Thompson, Jessica and Liu, Wendy and Chiu, Remi and Fujinaga, Ichiro},
	editor = {Gouyon, Fabien and Herrera, Perfecto and Martins, Luis Gustavo and Müller, Meinard},
	year = {2012},
	pages = {577--582},
}

@inproceedings{hankinson_creating_2012,
	address = {Lyon, France},
	title = {Creating a {Large}-scale {Searchable} {Digital} {Collection} from {Printed} {Music} {Materials}},
	isbn = {978-1-4503-1230-1},
	doi = {10.1145/2187980.2188221},
	booktitle = {21st {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Hankinson, Andrew and Burgoyne, John Ashley and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2012},
	keywords = {optical music recognition, music notation, music score searching, web applications},
	pages = {903--908},
}

@inproceedings{hankinson_simssa_2012,
	address = {Montréal, QC},
	title = {{SIMSSA}: {Single} {Interface} for {Music} {Score} {Searching} and {Analysis}},
	url = {https://www.iaml.info/sites/default/files/pdf/20120711a_montreal_programme.pdf},
	booktitle = {Conference of the {International} {Association} of {Music} {Libraries}},
	author = {Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2012},
	file = {Hankinson_Fujinaga_2012_SIMSSA.pdf:/home/ptorras/zotpapers/Hankinson_Fujinaga_2012_SIMSSA.pdf:application/pdf},
}

@misc{hankinson_optical_2012,
	title = {Optical {Music} {Recognition} {Bibliography}},
	url = {http://ddmal.music.mcgill.ca/research/omr/omr_bibliography},
	author = {Hankinson, Andrew},
	year = {2012},
	keywords = {Optical Music Recognition, OMR, Bibliography, References},
}

@inproceedings{hankinson_using_2013,
	address = {Vienna, Austria},
	title = {Using optical music recognition to navigate and retrieve music documents},
	url = {https://www.iaml.info/sites/default/files/pdf/2013-07-25_iaml_vienna_conference_programme.pdf},
	booktitle = {Conference of the {International} {Association} of {Music} {Libraries}},
	author = {Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2013},
	file = {Hankinson_Fujinaga_2013_Using optical music recognition to navigate and retrieve music documents.pdf:/home/ptorras/zotpapers/Hankinson_Fujinaga_2013_Using optical music recognition to navigate and retrieve music documents.pdf:application/pdf},
}

@phdthesis{hankinson_optical_2014,
	type = {{PhD} {Thesis}},
	title = {Optical music recognition infrastructure for large-scale music document analysis},
	url = {http://digitool.library.mcgill.ca/webclient/DeliveryManager?pid=130291},
	school = {McGill University},
	author = {Hankinson, Andrew},
	year = {2014},
}

@article{helsen_optical_2014,
	title = {Optical music recognition and manuscript chant sources},
	volume = {42},
	doi = {10.1093/em/cau092},
	number = {4},
	journal = {Early Music},
	author = {Helsen, Kate and Bain, Jennifer and Fujinaga, Ichiro and Hankinson, Andrew and Lacoste, Debra},
	year = {2014},
	pages = {555--558},
}

@techreport{hemmatifar_deeppiano_2018,
	title = {{DeepPiano}: {A} {Deep} {Learning} {Approach} to {Translate} {Music} {Notation} to {English} {Alphabet}},
	url = {http://cs230.stanford.edu/files_winter_2018/projects/6940264.pdf},
	institution = {Stanford University},
	author = {Hemmatifar, Ali and Krishna, Ashish},
	year = {2018},
	note = {Backup Publisher: Stanford University},
	file = {Hemmatifar_Krishna_2018_DeepPiano.pdf:/home/ptorras/zotpapers/Hemmatifar_Krishna_2018_DeepPiano.pdf:application/pdf},
}

@inproceedings{henkel_audio-conditioned_2019,
	address = {Delft, The Netherlands},
	title = {Audio-{Conditioned} {U}-{Net} for {Position} {Estimation} in {Full} {Sheet} {Images}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Henkel, Florian and Kelz, Rainer and Widmer, Gerhard},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {8--11},
}

@inproceedings{henkel_learning_2020,
	title = {Learning to {Read} and {Follow} {Music} in {Complete} {Score} {Sheet} {Images}},
	url = {https://program.ismir2020.net/poster_6-02.html},
	booktitle = {Proceedings of the 21st {Int}. {Society} for {Music} {Information} {Retrieval} {Conf}.},
	author = {Henkel, Florian and Kelz, Rainer and Widmer, Gerhard},
	year = {2020},
}

@book{hewlett_computing_1990,
	title = {Computing in {Musicology}: {A} {Directory} of {Research}},
	volume = {6},
	url = {http://wiki.ccarh.org/images/f/f5/Computing_in_Musicology_06_00.pdf},
	publisher = {Center for Computer},
	editor = {Hewlett, Walter B. and Selfridge-Field, Eleanor},
	year = {1990},
	note = {Section: Optical recognition of musical data},
	file = {Hewlett_Selfridge-Field_1990_Computing in Musicology.pdf:/home/ptorras/zotpapers/Hewlett_Selfridge-Field_1990_Computing in Musicology.pdf:application/pdf},
}

@inproceedings{homenda_optical_1995,
	title = {Optical pattern recognition for printed music notation},
	url = {https://doi.org/10.1117/12.205779},
	doi = {10.1117/12.205779},
	abstract = {The paper presents problems related to automated recognition of printed music notation. Music notation recognition is a challenging problem in both fields: pattern recognition and knowledge representation. Music notation symbols, though well characterized by their features, are arranged in elaborated way in real music notation, which makes recognition task very difficult and still open for new ideas. On the other hand, the aim of the system, i.e. application of acquired printed music into further processing requires special representation of music data. Due to complexity of music nature and music notation, music representation is one of the key issue in music notation recognition and music processing. The problems of pattern recognition and knowledge representation in context or music processing are discussed in this paper. MIDISCAN, the computer system for music notation recognition and music processing, is presented.},
	booktitle = {Symposium on {OE}/{Aerospace} {Sensing} and {Dual} {Use} {Photonics}},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law},
	year = {1995},
}

@article{homenda_automatic_1996,
	title = {Automatic recognition of printed music and its conversion into playable music data},
	volume = {25},
	url = {http://control.ibspan.waw.pl:3000/contents/export?filename=1996-2-12_homenda.pdf},
	abstract = {The paper describes MIDISCAN-a recognition system for printed music notation. Music notation recognition is a challenging problem in both fields: pattern recognition and knowledge representation. Music notation symbols, though well characterized by their features, are arranged in an elaborate way in real music notation, which makes recognition task very difficult and still open for new ideas, as for example, fuzzy set application in skew correction and stave location. On the other hand, the aim of the system, i.e. conversion of acquired printed music into playable MIDI format requires special representation of music data. The problems of pattern recognition and knowledge representation in context of music processing are discussed in this paper (16 Refs.) music; optical character recognition},
	number = {2},
	journal = {Control and Cybernetics},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law},
	year = {1996},
	keywords = {C5260B (Computer vision and image processing techniques), knowledge representation, MIDI format engineering techniques), MIDISCAN, music notation recognition, music notation symbols, music processing, playable music data, printed music recognition, skew correction, stave location},
	pages = {353--367},
}

@incollection{homenda_optical_2001,
	address = {Heidelberg},
	title = {Optical {Music} {Recognition}: the {Case} of {Granular} {Computing}},
	isbn = {978-3-7908-1823-9},
	url = {https://doi.org/10.1007/978-3-7908-1823-9_15},
	abstract = {The paper deals with optical music recognition (OMR) as a process of structured data processing applied to music notation. Granularity of OMR in both its aspects: data representation and data processing is especially emphasised in the paper. OMR is a challenge in intelligent computing technologies, especially in such fields as pattern recognition and knowledge representation and processing. Music notation is a language allowing for communication in music, one of most sophisticated field of human activity, and has a high level of complexity itself. On the one hand, music notation symbols vary in size and have complex shapes; they often touch and overlap each other. This feature makes the recognition of music symbols a very difficult and complicated task. On the other hand, music notation is a two dimensional language in which importance of geometrical and logical relations between its symbols may be compared to the importance of the symbols alone. Due to complexity of music nature and music notation, music representation, necessary to store and reuse recognised information, is also the key issue in music notation recognition and music processing. Both: the data representation and the data processing used in OMR is highly structured, granular rather than numeric. OMR technology fits paradigm of granular computing},
	booktitle = {Granular {Computing}: {An} {Emerging} {Paradigm}},
	publisher = {Physica-Verlag HD},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law},
	year = {2001},
	doi = {10.1007/978-3-7908-1823-9_15},
	pages = {341--366},
}

@inproceedings{homenda_automatic_2004,
	address = {Divnormorkoye, Russia},
	title = {Automatic {Recognition} of {Music} {Notation} {Using} {Neural} {Networks}},
	url = {https://www.researchgate.net/publication/275207587_Automatic_recognition_of_music_notation_using_neural_networks},
	booktitle = {International {Conference} on {AI} and {Systems}},
	publisher = {Warsaw University of Technology},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law and Luckner, Marcin},
	year = {2004},
	file = {Homenda_Luckner_2004_Automatic Recognition of Music Notation Using Neural Networks.pdf:/home/ptorras/zotpapers/Homenda_Luckner_2004_Automatic Recognition of Music Notation Using Neural Networks.pdf:application/pdf},
}

@inproceedings{homenda_music_2004,
	address = {Warszawa, Poland},
	title = {Music {Symbol} {Recognition}: {Neural} {Networks} vs. {Statistical} {Methods}},
	url = {http://viking.ibspan.waw.pl/eurofuse2004/},
	booktitle = {{EUROFUSE} {Workshop} {On} {Data} {And} {Knowledge} {Engineering}},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law and Mossakowski, K.},
	editor = {De Baets, B. and De Caluwe, R. and De Tre, G. and Fodor, Janos and Kaprzyk, J. and Zadrozny, S.},
	year = {2004},
}

@inproceedings{homenda_automatic_2006,
	address = {Vancouver, Canada},
	title = {Automatic {Knowledge} {Acquisition}: {Recognizing} {Music} {Notation} with {Methods} of {Centroids} and {Classifications} {Trees}},
	doi = {10.1109/IJCNN.2006.247339},
	abstract = {This paper presents a pattern recognition study aimed al music symbols recognition. The study is focused on classification methods of music symbols based on decision trees and clustering method applied to classes of music symbols that face classification problems. Classification is made on the basis of extracted features. A comparison of selected classifiers was made on some classes of nutation symbols distorted by a variety of factors as image noise, printing defects, different fonts, skew and curvature of scanning, overlapped symbols.},
	booktitle = {International {Joint} {Conference} on {Neural} {Network}},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law and Luckner, Marcin},
	year = {2006},
	note = {ISSN: 2161-4393},
	keywords = {Neural networks, Optical character recognition software, Text recognition, Multiple signal classification, Ordinary magnetoresistance, pattern recognition, Printing, feature extraction, music, pattern classification, music notation recognition, centroids, Classification tree analysis, classifications trees, classifiers, Decision trees, knowledge acquisition, Knowledge acquisition, music symbols recognition, Tiles},
	pages = {3382--3388},
}

@inproceedings{homenda_automatic_2006-1,
	address = {Vancouver, Canada},
	title = {Automatic understanding of images: integrated syntactic and semantic analysis of music notation},
	doi = {10.1109/IJCNN.2006.247261},
	abstract = {The paper introduces an approach to image processing and recognition based on the perception of images as subjects being exchanged in the man-computer communication. The approach reveals the parallel syntactic and semantic attempts to automatic image understanding. Both attempts are reflected in the paradigms of information granulation and granular computing. The parallel syntactic and semantic processing of images allows for solving problems raised by difficulties and complexity of the detailed syntactic description of images as well as difficulties of detailed semantic analysis. The study presented in this paper is cast on the practical task of the music notation recognition.},
	booktitle = {International {Joint} {Conference} on {Neural} {Network}},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law},
	year = {2006},
	note = {ISSN: 2161-4393},
	keywords = {Artificial intelligence, Natural languages, Multiple signal classification, music, Image analysis, image recognition, music notation, Humans, Information science, Emulation, automatic image understanding, Civil engineering, granular computing, Hardware, image perception, information granulation, integrated syntactic, Machine intelligence, man-computer communication, parallel semantic image processing, parallel syntactic image processing, semantic analysis, semantic networks},
	pages = {3026--3033},
}

@article{homenda_decision_2014,
	title = {Decision trees and their families in imbalanced pattern recognition: {Recognition} with and without {Rejection}},
	volume = {8838},
	issn = {0302-9743},
	doi = {10.1007/978-3-662-45237-0_22},
	abstract = {Decision trees are considered to be among the best classifiers. In this work we use decision trees and its families to the problem of imbalanced data recognition. Considered are aspects of recognition without rejection and with rejection: it is assumed that all recognized elements belong to desired classes in the first case and that some of them are outside of such classes and are not known at classifiers training stage. The facets of imbalanced data and recognition with rejection affect different real world problems. In this paper we discuss results of experiment of imbalanced data recognition on the case study of music notation symbols. Decision trees and three methods of joining decision trees (simple voting, bagging and random forest) are studied. These methods are used for recognition without and with rejection. © IFIP International Federation for Information Processing 2014.},
	journal = {Lecture Notes in Computer Science},
	author = {Homenda, W{\textbackslash}ladys{\textbackslash}law and Lesinski, Wojciech},
	editor = {Saeed K., Snasel V., Saeed K.},
	year = {2014},
	note = {ISBN: 9783662452363
Publisher: Springer Verlag},
	keywords = {Optical music recognition, Pattern recognition, Music notation, Decision trees, Bagging, Imbalanced data, Industrial management, Information management, Information systems, Random forests, Real-world problem},
	pages = {219--230},
	file = {Homenda_Lesinski_2014_Decision trees and their families in imbalanced pattern recognition.pdf:/home/ptorras/zotpapers/Homenda_Lesinski_2014_Decision trees and their families in imbalanced pattern recognition.pdf:application/pdf},
}

@inproceedings{hori_automatic_1999,
	title = {Automatic music score recognition/play system based on decision based neural network},
	doi = {10.1109/MMSP.1999.793817},
	abstract = {This paper proposes an automatic music score recognition system based on a hierarchically structured decision based neural network (DBNN), which can classify patterns with nonlinear decision boundaries. Currently, this system yields around a 97\% recognition rate for printed music scores.},
	booktitle = {3rd {Workshop} on {Multimedia} {Signal} {Processing}},
	author = {Hori, Toyokazu and Wada, Shinichiro and Tai, Howzan and Kung, S. Y.},
	year = {1999},
	keywords = {Neural networks, Feature extraction, Multiple signal classification, Pattern recognition, document image processing, music, image recognition, Image edge detection, Multi-layer neural network, Principal component analysis, image classification, neural nets, pattern classification, automatic music score play system, automatic music score recognition system, Convergence, Density measurement, hierarchically structured decision based neural network, Length measurement, nonlinear decision boundaries, printed music scores},
	pages = {183--184},
}

@inproceedings{huang_automatic_2015,
	address = {Málaga, Spain},
	title = {Automatic {Handwritten} {Mensural} {Notation} {Interpreter}: {From} {Manuscript} to {MIDI} {Performance}},
	isbn = {978-84-606-8853-2},
	url = {http://ismir2015.uma.es/articles/191_Paper.pdf},
	booktitle = {16th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Huang, Yu-Hui and Chen, Xuanli and Beck, Serafina and Burn, David and Van Gool, Luc},
	editor = {Müller, Meinard and Wiering, Frans},
	year = {2015},
	pages = {79--85},
	file = {Huang et al_2015_Automatic Handwritten Mensural Notation Interpreter.pdf:/home/ptorras/zotpapers/Huang et al_2015_Automatic Handwritten Mensural Notation Interpreter.pdf:application/pdf},
}

@article{huang_state---art_2019,
	title = {State-of-the-{Art} {Model} for {Music} {Object} {Recognition} with {Deep} {Learning}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/13/2645},
	doi = {10.3390/app9132645},
	abstract = {Optical music recognition (OMR) is an area in music information retrieval. Music object detection is a key part of the OMR pipeline. Notes are used to record pitch and duration and have semantic information. Therefore, note recognition is the core and key aspect of music score recognition. This paper proposes an end-to-end detection model based on a deep convolutional neural network and feature fusion. This model is able to directly process the entire image and then output the symbol categories and the pitch and duration of notes. We show a state-of-the-art recognition model for general music symbols which can get 0.92 duration accurary and 0.96 pitch accuracy .},
	number = {13},
	journal = {Applied Sciences},
	author = {Huang, Zhiquing and Jia, Xiang and Guo, Yifan},
	year = {2019},
	keywords = {deep learning, optical music recognition, object detection, music scores},
	pages = {2645--2665},
	file = {Huang et al_2019_State-of-the-Art Model for Music Object Recognition with Deep Learning.pdf:/home/ptorras/zotpapers/Huang et al_2019_State-of-the-Art Model for Music Object Recognition with Deep Learning.pdf:application/pdf},
}

@inproceedings{inesta_hispamus_2018,
	address = {Paris, France},
	title = {{HISPAMUS}: {Handwritten} {Spanish} {Music} {Heritage} {Preservation} by {Automatic} {Transcription}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Iñesta, José Manuel and León, Pedro J. Ponce de and Rizo, David and Oncina, José and Micó, Luisa and Rico-Juan, Juan Ramón and Pérez-Sancho, Carlos and Pertusa, Antonio},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {17--18},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/X5JKY2HJ/Iñesta et al. - 2018 - HISPAMUS Handwritten Spanish Music Heritage Prese.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/327PCBSW/forum.html:text/html},
}

@inproceedings{inesta_muret_2019,
	address = {Delft, The Netherlands},
	title = {{MuRET} as a software for the transcription of historical archives},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Iñesta, José M. and Rizo, David and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {12--15},
}

@misc{gear_up_ab_iseenotes_2017,
	title = {{iSeeNotes}},
	url = {http://www.iseenotes.com},
	author = {{Gear Up AB}},
	year = {2017},
}

@incollection{itagaki_automatic_1992,
	address = {Berlin, Heidelberg},
	title = {Automatic {Recognition} of {Several} {Types} of {Musical} {Notation}},
	isbn = {978-3-642-77281-8},
	url = {https://doi.org/10.1007/978-3-642-77281-8_22},
	abstract = {This paper describes recent progress towards systems for automatic recognition of several different types of musical notation, including printed sheet music, Braille music, and dance notation.},
	booktitle = {Structured {Document} {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Itagaki, Takebumi and Isogai, Masayuki and Hashimoto, Shuji and Ohteru, Sadamu},
	year = {1992},
	doi = {10.1007/978-3-642-77281-8_22},
	pages = {466--476},
}

@inproceedings{jacquemard_automated_2022,
	address = {Online},
	title = {Automated {Transcription} of {Electronic} {Drumkits}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Jacquemard, Florent and Rodriguez-de la Nava, Lydia and Digard, Martin},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {37--41},
	file = {Jacquemard et al_2022_Automated Transcription of Electronic Drumkits.pdf:/home/ptorras/zotpapers/Jacquemard et al_2022_Automated Transcription of Electronic Drumkits.pdf:application/pdf},
}

@incollection{jastrzebska_optical_2014,
	address = {Cham},
	title = {Optical {Music} {Recognition} as the {Case} of {Imbalanced} {Pattern} {Recognition}: {A} {Study} of {Complex} {Classifiers}},
	isbn = {978-3-319-01857-7},
	abstract = {The article is focused on a particular aspect of classification, namely the imbalance of recognized classes. Imbalanced data adversely affects the recognition ability and requires proper classifier's construction. The aim of presented study is to explore the capabilities of classifier combining methods with such raised problem. In this paper authors discuss results of experiment of imbalanced data recognition on the case study of music notation symbols. Applied classification methods include: simple voting method, bagging and random forest.},
	booktitle = {International {Conference} on {Systems} {Science} 2013},
	publisher = {Springer International Publishing},
	author = {Jastrzebska, Agnieszka and Lesinski, Wojciech},
	year = {2014},
	doi = {10.1007/978-3-319-01857-7_31},
	pages = {325--335},
}

@inproceedings{jastrzebska_optical_2016,
	address = {Cham},
	title = {Optical {Music} {Recognition} as the {Case} of {Imbalanced} {Pattern} {Recognition}: {A} {Study} of {Single} {Classifiers}},
	isbn = {978-3-319-19090-7},
	doi = {https://link.springer.com/chapter/10.1007%2F978-3-319-19090-7_37},
	abstract = {The article is focused on a particular aspect of classification, namely the imbalance of recognized classes. The paper contains a comparative study of results of musical symbols classification using known algorithms: k-nearest neighbors, k-means, Mahalanobis minimal distance, and decision trees. Authors aim at addressing the problem of imbalanced pattern recognition. First, we theoretically analyze difficulties entailed in the classification of music notation symbols. Second, in the enclosed case study we investigate the fitness of named single classifiers on real data. Conducted experiments are based on own implementations of named algorithms with all necessary image processing tasks. Results are highly satisfying.},
	booktitle = {Knowledge, {Information} and {Creativity} {Support} {Systems}: {Recent} {Trends}, {Advances} and {Solutions}},
	publisher = {Springer International Publishing},
	author = {Jastrzebska, Agnieszka and Lesinski, Wojciech},
	editor = {Skulimowski, Andrzej M.J. and Kacprzyk, Janusz},
	year = {2016},
	pages = {493--505},
}

@mastersthesis{jastrzebski_omr_2014,
	title = {{OMR} for sheet music digitization},
	url = {http://www.zsi.pwr.wroc.pl/~kopel/mgr/2014.07%20mgr%20Jastrzebski.pdf},
	school = {Politechnika Wroc{\textbackslash}lawska},
	author = {Jastrzębski, Krzysztof},
	year = {2014},
}

@inproceedings{jin_interpreting_2012,
	address = {Porto, Portugal},
	title = {Interpreting {Rhythm} in {Optical} {Music} {Recognition}},
	url = {http://ismir2012.ismir.net/event/papers/151-ismir-2012.pdf},
	booktitle = {13th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Jin, Rong and Raphael, Christopher},
	editor = {Gouyon, Fabien and Herrera, Perfecto and Martins, Luis Gustavo and Müller, Meinard},
	year = {2012},
	pages = {151--156},
}

@phdthesis{jin_graph-based_2017,
	type = {{PhD} {Thesis}},
	title = {Graph-{Based} {Rhythm} {Interpretation} in {Optical} {Music} {Recognition}},
	url = {https://search.proquest.com/openview/891a2b54a68dba0e3a03698c18dfac06/},
	school = {Indiana University},
	author = {Jin, Rong},
	year = {2017},
}

@mastersthesis{johansen_optical_2009,
	title = {Optical {Music} {Recognition}},
	url = {https://www.duo.uio.no/handle/10852/10832},
	school = {University of Oslo},
	author = {Johansen, Linn Saxrud},
	year = {2009},
	note = {Issue: April},
	file = {Johansen_2009_Optical Music Recognition.pdf:/home/ptorras/zotpapers/Johansen_2009_Optical Music Recognition.pdf:application/pdf},
}

@incollection{jones_optical_2008,
	title = {Optical {Music} {Imaging}: {Music} {Document} {Digitisation}, {Recognition}, {Evaluation}, and {Restoration}},
	abstract = {This paper presents the applications and practices in the domain of music imaging for musical scores (music sheets and music manuscripts), which include music sheet digitisation, optical music recognition (OMR) and optical music restoration. With a general background of Optical Music Recognition (OMR), the paper discusses typical obstacles in this domain and reports currently available commercial OMR software. It reports hardware and software related to music imaging, discussed the SharpEye optical music recognition system and provides an evaluation of a number of OMR systems. Besides the main focus on the transformation from images of music scores to symbolic format, this paper also discusses optical music image restoration and the application of music imaging techniques for graphical preservation and potential applications for cross-media integration.},
	booktitle = {Interactive multimedia music technologies},
	publisher = {IGI Global},
	author = {Jones, Graham and Ong, Bee and Bruno, Ivan and Ng, Kia},
	year = {2008},
	doi = {10.4018/978-1-59904-150-6.ch004},
	pages = {50--79},
}

@article{ju_assisted_2019,
	title = {Assisted {Music} {Score} {Reading} {Using} {Fixed}-{Gaze} {Head} {Movement}: {Empirical} {Experiment} and {Design} {Implications}},
	volume = {3},
	issn = {2573-0142},
	url = {http://doi.acm.org/10.1145/3300962},
	doi = {10.1145/3300962},
	number = {EICS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Ju, Qinjie and Chalon, René and Derrode, Stéphane},
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: ACM},
	keywords = {music score, eye-tracking, gaze interaction, head movement},
	pages = {3:1--3:29},
}

@article{kassler_optical_1972,
	title = {Optical {Character}-{Recognition} of {Printed} {Music} : {A} {Review} of {Two} {Dissertations}. {Automatic} {Recognition} of {Sheet} {Music} by {Dennis} {Howard} {Pruslin} ; {Computer} {Pattern} {Recognition} of {Standard} {Engraved} {Music} {Notation} by {David} {Stewart} {Prerau}.},
	volume = {11},
	url = {http://www.jstor.org/stable/832471},
	abstract = {Stable URL: http://www.jstor.org/stable/832471},
	number = {1},
	journal = {Perspectives of New Music},
	author = {Kassler, Michael},
	year = {1972},
	pages = {250--254},
}

@inproceedings{katayose_expression_1990,
	title = {Expression extraction in virtuoso music performances},
	doi = {10.1109/ICPR.1990.118216},
	abstract = {An approach to music interpretation by computers is discussed. A rule-based music interpretation system is being developed that generates sophisticated performance from a printed music score. The authors describe the function of learning how to play music, which is the most important process in music interpretation. The target to be learned is expression rules and grouping strategy: expression rules are used to convert dynamic marks and motives into concrete performance data, and grouping strategy is used to extract motives from sequences of notes. They are learned from a given virtuoso performance. The delicate control of attack timing and of the duration and strength of the notes is extracted by the music transcription function. The performance rules are learned by investigating how the same or similar musical primitives are played in a performance. As for the grouping strategy, the system analyzes how the player grouped music and registers dominant note sequences to extract motives.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {10th {International} {Conference} on {Pattern} {Recognition}},
	author = {Katayose, H. and Fukuoka, T. and Takami, K. and Inokuchi, S.},
	year = {1990},
	keywords = {Music, Multiple signal classification, Concrete, Performance analysis, Data mining, Humans, Control engineering, dominant note sequences, electronic music, expression extraction, knowledge based systems, Manuals, motives, music transcription function, printed music score, Registers, rule-based music interpretation system, Synthesizers, virtuoso music performances},
	pages = {780--784 vol.1},
}

@article{kato_robot_1987,
	title = {The robot musician 'wabot-2' (waseda robot-2)},
	volume = {3},
	issn = {0167-8493},
	url = {http://www.sciencedirect.com/science/article/pii/0167849387900027},
	doi = {https://doi.org/10.1016/0167-8493(87)90002-7},
	abstract = {The wabot-2 is an anthropomorphic robot playing keyboard instruments, developed by the study group of Waseda University's Science and Engineering Department. The wabot-2 is equipped with hands tapping softly on keys, with legs handling bass keys and expression pedal, with eyes reading a score, and with a mouth and ears to converse with humans. Based on wabot-2, wasubot has been developed by Sumitomo Electric Industries Ltd., whose artistic skill has been demonstrated in performing music at the Japanese Government Pavillion in Expo'85. The present paper summarizes the wabot-2's motion, visual and vocal subsystems as well as its supervisory system and singing voice-tracking subsystem.},
	number = {2},
	journal = {Robotics},
	author = {Kato, Ichiro and Ohteru, Sadamu and Shirai, Katsuhiko and Matsushima, Toshiaki and Narita, Seinosuke and Sugano, Shigeki and Kobayashi, Tetsunori and Fujisawa, Eizo},
	year = {1987},
	keywords = {Speech recognition, Anthropomorphic robot, Autonomous robot, camera, Dexterity, High speed image processing, Multiple Degrees of Freedom, Speech synthesis},
	pages = {143--155},
}

@incollection{kato_recognition_1992,
	address = {Berlin, Heidelberg},
	title = {A {Recognition} {System} for {Printed} {Piano} {Music} {Using} {Musical} {Knowledge} and {Constraints}},
	isbn = {978-3-642-77281-8},
	url = {https://doi.org/10.1007/978-3-642-77281-8_20},
	abstract = {We describe a recognition system for printed piano music, which presents challenging problems in both image pattern matching and semantic analysis. In music notation, the shape of symbols is simple, but confusing connections and overlaps among symbols occur. In order to deal with these difficulties, proper knowledge is required, so our system adopts a top-down approach based on bar-unit recognition to use musical knowledge and constraints effectively. Recognition results, described with a symbolic playable data format, exceed 90\% correct on beginner's piano music.},
	booktitle = {Structured {Document} {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kato, Hirokazu and Inokuchi, Seiji},
	year = {1992},
	doi = {10.1007/978-3-642-77281-8_20},
	pages = {435--455},
}

@inproceedings{kim_recognition_1987,
	title = {Recognition system for a printed music score},
	url = {http://www.dbpia.co.kr/Journal/ArticleDetail/NODE00396371},
	booktitle = {{TENCON} 87- {Computers} and {Communications} {Technology} {Toward} 2000},
	author = {Kim, W. J. and Chung, M. J. and Bien, Z.},
	year = {1987},
	pages = {573--577},
}

@article{kiriella_music_2014,
	title = {Music {Training} {Interface} for {Visually} {Impaired} through a {Novel} {Approach} to {Optical} {Music} {Recognition}},
	volume = {3},
	issn = {2010-2283},
	doi = {10.7603/s40601-013-0045-6},
	abstract = {Some inherited barriers which limits the human abilities can be surprisingly win through technology. This research focuses on defining a more reliable and a controllable interface for visually impaired people to read and study eastern music notations which are widely available in printed format. One of another concept behind was that differently-abled people should be assisted in a way which they can proceed interested tasks in an independent way. The research provide means to continue on researching the validity of using a controllable auditory interface instead using Braille music scripts converted with the help of 3rd parties. The research further summarizes the requirements aroused by the relevant users, design considerations, evaluation results on user feedbacks of proposed interface.},
	number = {4},
	journal = {GSTF Journal on Computing},
	author = {Kiriella, Dawpadee B. and Kumari, Shyama C. and Ranasinghe, Kavindu C. and Jayaratne, Lakshman},
	year = {2014},
	pages = {45},
}

@inproceedings{kletz_detecting_2021,
	address = {Alicante, Spain},
	title = {Detecting {Staves} and {Measures} in {Music} {Scores} with {Deep} {Learning}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Kletz, Marc and Pacha, Alexander},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {8--12},
}

@inproceedings{knopke_towards_2007,
	address = {Vienna, Austria},
	title = {Towards {Musicdiff} : {A} {Foundation} for {Improved} {Optical} {Music} {Recognition} {Using} {Multiple} {Recognizers}},
	isbn = {978-3-85403-218},
	url = {http://homes.sice.indiana.edu/donbyrd/Papers/ismir_2007_omr.pdf},
	abstract = {This paper presents work towards a “musicdiff” program for comparing files representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and oth- ers strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basicmethodology requires several stages: documents must be scanned and submitted to severalOMR programs, programswhose strengths andweaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimen- tary error correction. We also describe a visualization tool for comparingmultiple versions on ameasure-by-measure basis.},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Knopke, Ian and Byrd, Donald},
	year = {2007},
	keywords = {evaluation},
	pages = {123--126},
	file = {Knopke_Byrd_2007_Towards Musicdiff.pdf:/home/ptorras/zotpapers/Knopke_Byrd_2007_Towards Musicdiff.pdf:application/pdf},
}

@inproceedings{kodirov_music_2014,
	address = {Siem Reap, Cambodia},
	title = {Music with {Harmony}: {Chord} {Separation} and {Recognition} in {Printed} {Music} {Score} {Images}},
	isbn = {978-1-4503-2644-5},
	doi = {10.1145/2557977.2558042},
	abstract = {Optical music recognition systems are in the general interest recently. These systems achieve accurate symbol recognition at some level. However, chords are not considered in these systems yet they play a role in music. Therefore, we aimed to develop an algorithm that can deal with separation and recognition of chords in music score images. Separation is necessary because the chords can be touched, overlapped or/and broken due to noise and other reasons. By considering these problems, we propose top-down based separation using domain information and characteristics of the chords. To handle recognition, we propose a modified zoning method with k-nearest neighbor classifier. Also, we analyzed several classifiers with different features to see which method is reliable for the chord recognition. Since this topic is not considered with special focus before, there is not a standard benchmark to evaluate performance of the algorithm. Thus, we introduce a new dataset, namely OMR-ChSR6306, which includes a wide range of chords such as single chords, touched chords, and overlapped chords. Experiments on the proposed dataset demonstrate that our algorithm can separate and recognize the chords, with 100\% separation and 98.98\% recognition accuracy respectively.},
	booktitle = {8th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication}},
	publisher = {ACM},
	author = {Kodirov, Elyor and Han, Sejin and Lee, Guee-Sang and Kim, YoungChul},
	year = {2014},
	keywords = {music score images, recognition, chord, k-nearest neighbor, separation, zoning},
	pages = {1--8},
}

@inproceedings{kolakowska_applying_2008,
	title = {Applying decision trees to the recognition of musical symbols},
	doi = {10.1109/INFTECH.2008.4621624},
	abstract = {The paper presents an experimental study on the recognition of printed musical scores. The first part of the study focuses on data preparation. Bitmaps containing musical symbols are converted to feature vectors using various methods. The vectors created in such a way are used to train classifiers which are the essential part of the study. Several decision tree classifiers are applied to this recognition task. These classifiers are created using different decision tree induction methods. The algorithms incorporate different criteria to select attributes in the nodes of the trees. Moreover, some of them apply stopping criteria, whereas the others perform tree pruning. The classification accuracy of the decision trees is estimated on data taken from musical scores. Eventually the usefulness of decision trees in the recognition of printed musical symbols is evaluated.},
	booktitle = {1st {International} {Conference} on {Information} {Technology}},
	author = {Ko{\textbackslash}lakowska, Agata},
	year = {2008},
	keywords = {Training, Accuracy, document image processing, music, image classification, Classification algorithms, Machine learning algorithms, printed musical scores, Classification tree analysis, Decision trees, data preparation, decision tree classifiers, decision tree induction methods, decision trees, feature vectors, musical symbol recognition, printed musical symbols, Software algorithms, task recognition, tree pruning},
	pages = {1--4},
}

@misc{ragan_kompapp_2017,
	title = {{KompApp}},
	url = {http://kompapp.com},
	author = {Ragan, Gene},
	year = {2017},
}

@inproceedings{konwer_staff_2018,
	title = {Staff line {Removal} using {Generative} {Adversarial} {Networks}},
	doi = {10.1109/ICPR.2018.8546105},
	abstract = {Staff line removal is a crucial pre-processing step in Optical Music Recognition. In this paper we propose a novel approach for staff line removal, based on Generative Adversarial Networks. We convert staff line images into patches and feed them into a U-Net, used as Generator. The Generator intends to produce staff-less images at the output. Then the Discriminator does binary classification and differentiates between the generated fake staff-less image and real ground truth staff less image. For training, we use a Loss function which is a weighted combination of L2 loss and Adversarial loss. L2 loss minimizes the difference between real and fake staff-less image. Adversarial loss helps to retrieve more high quality textures in generated images. Thus our architecture supports solutions which are closer to ground truth and it reflects in our results. For evaluation we consider the ICDAR/GREC 2013 staff removal database. Our method achieves superior performance in comparison to other conventional approaches on the same dataset.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Konwer, Aishik and Bhunia, Ayan Kumar and Bhowmick, Abir and Bhunia, Ankan Kumar and Banerjee, Prithaj and Roy, Partha Pratim and Pal, Umapada},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Task analysis, Convolutional neural networks, Training, document image processing, music, image recognition, staff line removal, Adversarial loss, Adversarial Loss, crucial pre-processing step, Gallium nitride, generated fake staff-less, Generative Adversarial Network, generative adversarial networks, Generative adversarial networks, Generators, ground truth staff, ICDAR-GREC 2013 staff removal database, Image generation, staff line images, staff-less images, Staff-line Removal, U-Net},
	pages = {1103--1108},
	file = {Konwer et al_2018_Staff line Removal using Generative Adversarial Networks.pdf:/home/ptorras/zotpapers/Konwer et al_2018_Staff line Removal using Generative Adversarial Networks.pdf:application/pdf},
}

@article{kopec_markov_1996,
	title = {Markov source model for printed music decoding},
	volume = {5},
	url = {https://www.researchgate.net/profile/Philip_Chou2/publication/220050304_Markov_source_model_for_printed_music_decoding/links/00b7d51aaadea3a4b6000000/Markov-source-model-for-printed-music-decoding.pdf},
	doi = {10.1117/12.227527},
	abstract = {A Markov source model is described for a simple subset of printed music notation that was developed as an extended example of the document image decoding (DID) approach to document image analysis. The model is based on the Adobe Sonata music symbol set and a finite-state language of textual music messages. The music message language is defined and several important aspects of message imaging are discussed. Aspects of music notation that appear problematic for a finite-state representation are identified. Finally, an example of music image decoding and resynthesis using the model is presented. Development of the model was greatly facilitated by the duality between image synthesis and image decoding that is fundamental to the DID paradigm.},
	journal = {Journal of Electronic Imaging},
	author = {Kopec, Gary E. and Chou, Philip A. and Maltz, David A.},
	year = {1996},
	file = {Kopec et al_1996_Markov source model for printed music decoding.pdf:/home/ptorras/zotpapers/Kopec et al_1996_Markov source model for printed music decoding.pdf:application/pdf},
}

@inproceedings{kurth_framework_2008,
	address = {Berlin, Heidelberg},
	title = {A {Framework} for {Managing} {Multimodal} {Digitized} {Music} {Collections}},
	isbn = {978-3-540-87599-4},
	doi = {10.1007/978-3-540-87599-4_35},
	abstract = {In this paper, we present a framework for managing heterogeneous, multimodal digitized music collections containing visual music representations (scanned sheet music) as well as acoustic music material (audio recordings). As a first contribution, we propose a preprocessing workflow comprising feature extraction, audio indexing, and music synchronization (linking the visual with the acoustic data). Then, as a second contribution, we introduce novel user interfaces for multimodal music presentation, navigation, and content-based retrieval. In particular, our system offers high quality audio playback with time-synchronous display of the digitized sheet music. Furthermore, our system allows a user to select regions within the scanned pages of a musical score in order to search for musically similar sections within the audio documents. Our novel user interfaces and search functionalities will be integrated into the library service system of the Bavarian State Library as part of the Probado project.},
	booktitle = {Research and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kurth, Frank and Damm, David and Fremerey, Christian and Müller, Meinard and Clausen, Michael},
	editor = {Christensen-Dalsgaard, Birte and Castelli, Donatella and Ammitzbøll Jurik, Bolette and Lippincott, Joan},
	year = {2008},
	pages = {334--345},
	file = {Kurth et al_2008_A Framework for Managing Multimodal Digitized Music Collections.pdf:/home/ptorras/zotpapers/Kurth et al_2008_A Framework for Managing Multimodal Digitized Music Collections.pdf:application/pdf},
}

@inproceedings{kusakunniran_optical_2014,
	title = {Optical music recognition for traditional {Thai} sheet music},
	doi = {10.1109/ICSEC.2014.6978187},
	booktitle = {International {Computer} {Science} and {Engineering} {Conference}},
	publisher = {IEEE},
	author = {Kusakunniran, Worapan and Prempanichnukul, Attapol and Maneesutham, Arthid and Chocksawud, Kullachut and Tongsamui, Suparus and Thongkanchorn, Kittikhun},
	year = {2014},
	pages = {157--162},
}

@inproceedings{lallican_off-line_2000,
	address = {Amsterdam},
	title = {From {Off}-{Line} to {On}-{Line} {Handwriting} {Recognition}},
	isbn = {90-76942-01-3},
	url = {http://www.rug.nl/research/portal/files/2981118/paper-050-lallican.pdf},
	booktitle = {7th {International} {Workshop} on {Frontiers} in {Handwriting} {Recognition}},
	publisher = {International Unipen Foundation},
	author = {Lallican, P. M. and Viard-Gaudin, C. and Knerr, S.},
	editor = {Schomaker, L. R. B. and Vuurpijl, L. G.},
	year = {2000},
	pages = {303--312},
	file = {Lallican et al_2000_From Off-Line to On-Line Handwriting Recognition.pdf:/home/ptorras/zotpapers/Lallican et al_2000_From Off-Line to On-Line Handwriting Recognition.pdf:application/pdf},
}

@inproceedings{laplante_digitizing_2016,
	title = {Digitizing musical scores: {Challenges} and opportunities for libraries},
	doi = {10.1145/2970044.2970055},
	booktitle = {3rd {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Laplante, Audrey and Fujinaga, Ichiro},
	year = {2016},
	pages = {45--48},
	file = {Laplante_Fujinaga_2016_Digitizing musical scores.pdf:/home/ptorras/zotpapers/Laplante_Fujinaga_2016_Digitizing musical scores.pdf:application/pdf},
}

@inproceedings{lee_handwritten_2016,
	title = {Handwritten {Music} {Symbol} {Classification} {Using} {Deep} {Convolutional} {Neural} {Networks}},
	doi = {10.1109/ICISSEC.2016.7885856},
	abstract = {In this paper, we utilize deep Convolutional Neural Networks (CNNs) to classify handwritten music symbols in HOMUS data set. HOMUS data set is made up of various types of strokes which contain time information and it is expected that online techniques are more appropriate for classification. However, experimental results show that CNN which does not use time information achieved classification accuracy around 94.6\% which is way higher than 82\% of dynamic time warping (DTW), the prior state-of-the-art online technique. Finally, we achieved the best accuracy around 95.6\% with the ensemble of CNNs.},
	booktitle = {International {Conference} on {Information} {Science} and {Security}},
	author = {Lee, Sangkuk and Son, Sung Joon and Oh, Jiyong and Kwak, Nojun},
	year = {2016},
	keywords = {Neural networks, Handwriting recognition, Music, CNN, music, handwritten character recognition, image classification, neural nets, Electronic mail, Kernel, deep convolutional neural networks, DTW, dynamic time warping, Fats, handwritten music symbol classification, HOMUS data set, Smart phones, time information},
	pages = {1--5},
}

@techreport{lehman-borer_optical_2016,
	title = {Optical {Music} {Recognition}},
	url = {https://scholarship.tricolib.brynmawr.edu/handle/10066/18782},
	institution = {Swarthmore College},
	author = {Lehman-Borer, Ryerson},
	year = {2016},
	file = {Lehman-Borer_2016_Optical Music Recognition.pdf:/home/ptorras/zotpapers/Lehman-Borer_2016_Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{leplumey_robust_1993,
	title = {A robust detector for music staves},
	doi = {10.1109/ICDAR.1993.395591},
	abstract = {A method for the automatic recognition of music staves based on a prediction-and-check technique is presented in order to extract staves. It can detect lines with some curvature, discontinuities, and inclination. Lines are asserted to be a part of a staff if they can be grouped by five, thus completing the staff. This last phase also identifies additional staff lines.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {2nd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Leplumey, Ivan and Camillerapp, Jean and Lorette, G.},
	year = {1993},
	keywords = {Image coding, Image segmentation, Image recognition, Detectors, Multiple signal classification, Labeling, music, image recognition, additional staff lines, automatic recognition, curvature, discontinuities, Histograms, inclination, music staves, Noise robustness, prediction-and-check technique, robust detector, Samarium, Sampling methods},
	pages = {902--905},
}

@inproceedings{lesinski_optical_2015,
	title = {Optical {Music} {Recognition}: {Standard} and {Cost}-{Sensitive} {Learning} with {Imbalanced} {Data}},
	doi = {10.1007/978-3-319-24369-6_51},
	booktitle = {{IFIP} {International} {Conference} on {Computer} {Information} {Systems} and {Industrial} {Management}},
	publisher = {Springer},
	author = {Lesinski, Wojciech and Jastrzebska, Agnieszka},
	year = {2015},
	pages = {601--612},
	file = {Lesinski_Jastrzebska_2015_Optical Music Recognition.pdf:/home/ptorras/zotpapers/Lesinski_Jastrzebska_2015_Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{li_optical_2018,
	title = {Optical {Music} {Notes} {Recognition} for {Printed} {Music} {Score}},
	volume = {01},
	doi = {10.1109/ISCID.2018.00071},
	abstract = {To convert printed music score into a machine-readable format, a system that can automatically decode the symbolic image and play the music is proposed. The system takes a music score image as input, segments music symbols after preprocessing the image, then recognizes their pitch and duration. Finally, MIDI files are generated. The experiments on Rebelo Database shows that the proposed method obtains superior recognition accuracy against other methods.},
	booktitle = {11th {International} {Symposium} on {Computational} {Intelligence} and {Design} ({ISCID})},
	author = {Li, Chuanzhen and Zhao, Jiaqi and Cai, Juanjuan and Wang, Hui and Du, Huaichang},
	month = dec,
	year = {2018},
	note = {ISSN: 2473-3547},
	keywords = {Neural networks, Image segmentation, Databases, Music, Image recognition, Head, decoding, music, image recognition, image segmentation, segmentation, recognition, music score, machine-readable format, MIDI, printed music score, Magnetic heads, MIDI files, music score image, music symbols, music symbols segmentation, optical music notes recognition, symbolic image decoding},
	pages = {285--288},
}

@inproceedings{lin_integrating_2000,
	title = {Integrating {Paper} and {Digital} {Music} {Information} {Systems}},
	url = {http://ismir2000.ismir.net/posters/linbell_fullpaper.pdf},
	abstract = {Active musicians generally rely on extensive personal paper-based music information retrieval systems containing scores, parts, compositions, and arrangements of published and hand-written music. Many have a bias against using computers to store, edit and retrieve music, and prefer to work in the paper domain rather than using digital documents, despite the flexibility and powerful retrieval opportunities available. In this paper we propose a model of operation that blurs the boundaries between the paper and digital domains, offering musicians the best of both worlds. A survey of musicians identifies the problems and potential of working with digital tools, and we propose a system using colour printing and scanning technology that simplifies the process of moving music documents between the two domains},
	booktitle = {International {Society} for {Music} {Information} {Retrieval}},
	author = {Lin, Karen and Bell, Tim},
	year = {2000},
	keywords = {optical music recognition, to classify, user interfaces, user needs},
	pages = {23--25},
	file = {Lin_Bell_2000_Integrating Paper and Digital Music Information Systems.pdf:/home/ptorras/zotpapers/Lin_Bell_2000_Integrating Paper and Digital Music Information Systems.pdf:application/pdf},
}

@inproceedings{liu_note_2012,
	address = {Berlin, Heidelberg},
	title = {Note {Symbol} {Recognition} for {Music} {Scores}},
	isbn = {978-3-642-28490-8},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-642-28490-8_28},
	abstract = {Note symbol recognition plays a fundamental role in the process of an OMR system. In this paper, we propose new approaches for recognizing notes by extracting primitives and assembling them into constructed symbols. Firstly, we propose robust algorithms for extracting primitives (stems, noteheads and beams) based on Run-Length Encoding. Secondly, introduce the concept of interaction field to describe the relationship between primitives, and define six hierarchical categories for the structure of notes. Thirdly, propose an effective sequence to assemble the primitives into notes, guided by the mechanism of giving priority to the key structures. To evaluate the performance of those approaches,wepresent experimental results on real-life scores and comparisons with commercial systems. The results show our approaches can recognize notes with high-accuracy and powerful adaptability, especially for the complicated scores with high density of symbols.},
	booktitle = {Intelligent {Information} and {Database} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Liu, Xiaoxiang},
	editor = {Pan, Jeng-Shyang and Chen, Shyi-Ming and Nguyen, Ngoc Thanh},
	year = {2012},
	pages = {263--273},
}

@inproceedings{liu_robust_2015,
	title = {A {Robust} {Method} for {Musical} {Note} {Recognition}},
	isbn = {978-1-4673-8020-1},
	doi = {10.1109/CADGRAPHICS.2015.34},
	abstract = {Musical note recognition plays a fundamental role in the process of the optical music recognition system. In this paper, we propose a robust method for recognizing notes. The method includes three parts: (1) the description of relationships between primitives by introducing the concept of interaction field, (2) the definition of six hierarchical structure features for analyzing notes structures, (3) the workflow of primitive assembly under the guidance of giving priority to key structure features. To evaluate the performance of our method, we present experimental results on real-life scores and comparisons with two commercial products. Experiment show that our method lead to quite good results, especially for complicated scores.},
	booktitle = {14th {International} {Conference} on {Computer}-{Aided} {Design} and {Computer} {Graphics}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Liu, Xiaoxiang and Zhou, Mi and Xu, Peng},
	year = {2015},
	keywords = {Optical music recognition, Commercial products, Computer aided design, Computer graphics, Hierarchical structures, Interaction fields, Key structures, Musical notes, Primitive relationships, Robust methods},
	pages = {212--213},
}

@incollection{lopresti_issues_2002,
	address = {Ontario, Canada},
	title = {Issues in {Ground}-{Truthing} {Graphic} {Documents}},
	isbn = {978-3-540-45868-5},
	booktitle = {Graphics {Recognition} {Algorithms} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lopresti, Daniel and Nagy, George},
	year = {2002},
	doi = {10.1007/3-540-45868-9_5},
	pages = {46--67},
}

@misc{low_optical_2012,
	title = {Optical {Music} {Recognition} {Application}},
	url = {http://www.winlab.rutgers.edu/~crose/capstone12/entries/OpticalMusicRecognition.pdf},
	author = {Low, Grady and Chang, Yung-Ho},
	year = {2012},
	note = {Issue: April
Pages: 1–52},
	file = {Low_Chang_2012_Optical Music Recognition Application.pdf:/home/ptorras/zotpapers/Low_Chang_2012_Optical Music Recognition Application.pdf:application/pdf},
}

@inproceedings{luangnapa_optical_2012,
	title = {Optical {Music} {Recognition} on {Android} {Platform}},
	doi = {10.1007/978-3-642-35076-4},
	booktitle = {International {Conference} on {Advances} in {Information} {Technology}},
	publisher = {Springer},
	author = {Luangnapa, Nawapon and Silpavarangkura, Thongchai and Nukoolkit, Chakarida and Mongkolnam, Pornchai},
	year = {2012},
	pages = {106--115},
	file = {Luangnapa et al_2012_Optical Music Recognition on Android Platform.pdf:/home/ptorras/zotpapers/Luangnapa et al_2012_Optical Music Recognition on Android Platform.pdf:application/pdf},
}

@inproceedings{luckner_recognition_2006,
	title = {Recognition of {Noised} {Patterns} {Using} {Non}-{Disruption} {Learning} {Set}},
	doi = {10.1109/ISDA.2006.223},
	abstract = {In this paper the recognition of strongly noised symbols on the basis of non-disruption patterns is discussed taking music symbols as an example. Although Optical Music Recognition technology is not developed as successfully as OCR technology, several systems do recognize typical musical symbols to quite a good level. However, the recognition of non-typical fonts is still an unsolved issue. In this paper a model of a recognition system for unusual scores is presented. In the model described non-disruption symbols are used to generate a learning set that makes possible improved recognition as is presented on a real example of rests and accidentals recognition. Some techniques are presented with various recognition rates and computing times including supervised and unsupervised ones},
	booktitle = {6th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Luckner, Marcin},
	year = {2006},
	note = {ISSN: 2164-7143},
	keywords = {Testing, Optical character recognition software, optical music recognition, Ordinary magnetoresistance, Delay, Pattern recognition, optical character recognition, music symbols, Computer networks, Geodesy, Noise generators, noised pattern recognition, nondisruption learning set, nondisruption patterns, Optical noise, Probes, recognition system, strongly noised symbol recognition, supervised recognition, unsupervised recognition},
	pages = {557--562},
}

@inproceedings{luth_automatic_2002,
	title = {Automatic {Identification} of {Music} {Notations}},
	isbn = {0-7695-1862-1},
	doi = {10.1109/WDM.2002.1176212},
	booktitle = {2nd {International} {Conference} on {WEB} {Delivering} of {Music}},
	author = {Luth, Nailja},
	year = {2002},
}

@inproceedings{macmillan_gamera_2001,
	address = {Bloomington, IN},
	title = {Gamera: {A} structured document recognition application development environment},
	url = {https://jscholarship.library.jhu.edu/handle/1774.2/44376},
	booktitle = {2nd {International} {Symposium} on {Music} {Information} {Retrieval}},
	author = {MacMillan, Karl and Droettboom, Michael and Fujinaga, Ichiro},
	year = {2001},
	pages = {15--16},
	file = {MacMillan et al_2001_Gamera.pdf:/home/ptorras/zotpapers/MacMillan et al_2001_Gamera.pdf:application/pdf},
}

@inproceedings{macmillan_gamera_2002,
	title = {Gamera: {Optical} music recognition in a new shell},
	url = {http://www.music.mcgill.ca/~ich/research/icmc02/icmc2002.gamera.pdf},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {MacMillan, Karl and Droettboom, Michael and Fujinaga, Ichiro},
	year = {2002},
	keywords = {optical music recognition, to classify},
	pages = {482--485},
	file = {MacMillan et al_2002_Gamera.pdf:/home/ptorras/zotpapers/MacMillan et al_2002_Gamera.pdf:application/pdf},
}

@inproceedings{malik_handwritten_2013,
	title = {Handwritten {Musical} {Document} {Retrieval} {Using} {Music}-{Score} {Spotting}},
	doi = {10.1109/ICDAR.2013.170},
	abstract = {In this paper, we present a novel approach for retrieval of handwritten musical documents using a query sequence/word of musical scores. In our algorithm, the musical score-words are described as sequences of symbols generated from a universal codebook vocabulary of musical scores. Staff lines are removed first from musical documents using structural analysis of staff lines and symbol codebook vocabulary is created in offline. Next, using this symbol codebook the music symbol information in each document image is encoded. Given a query sequence of musical symbols in a musical score-line, the symbols in the query are searched in each of these encoded documents. Finally, a sub-string matching algorithm is applied to find query words. For codebook, two different feature extraction methods namely: Zernike Moments and 400 dimensional gradient features are tested and two unsupervised classifiers using SOM and K-Mean are evaluated. The results are compared with a baseline approach of DTW. The performance is measured on a collection of handwritten musical documents and results are promising.},
	booktitle = {12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Malik, Rakesh and Roy, Partha Pratim and Pal, Umapada and Kimura, Fumitaka},
	year = {2013},
	note = {ISSN: 1520-5363},
	keywords = {Vocabulary, Feature extraction, Heuristic algorithms, Indexing, document image processing, music, Vectors, Classification algorithms, Algorithm design and analysis, Approximate String Matching, document image, feature extraction methods, handwritten musical document retrieval, image retrieval, music-score spotting, Musical Document Retrieval, musical score-line, musical scores query sequence-word, Staff Removal, string matching, substring matching algorithm, Symbol Classification, universal codebook vocabulary, unsupervised classifiers, Zernike moments},
	pages = {832--836},
}

@inproceedings{marinai_projection_1999,
	title = {Projection {Based} {Segmentation} of {Musical} {Sheets}},
	isbn = {0-7695-0318-7},
	doi = {10.1109/ICDAR.1999.791838},
	abstract = {The automatic recognition of music scores is a key process for the electronic treatment of music information. In this paper we present the segmentation module of an OMR system. The proposed approach is based on the use of projection profiles for the location of elementary symbols that constitute the music notation. An extensive experimentation was made which the help of a tool developed to this purpose. Reported results shown a high efficiency in the correct location of elementary symbols},
	booktitle = {5th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Marinai, Simone and Nesi, Paolo},
	year = {1999},
	pages = {3--6},
	file = {Marinai_Nesi_1999_Projection Based Segmentation of Musical Sheets.pdf:/home/ptorras/zotpapers/Marinai_Nesi_1999_Projection Based Segmentation of Musical Sheets.pdf:application/pdf},
}

@article{martin_neural_1992,
	title = {Neural {Networks} for the {Recognition} of {Engraved} {Musical} {Scores}},
	volume = {06},
	doi = {10.1142/S0218001492000114},
	abstract = {The image analysis levels of a recognition system for engraved musical scores are described. Recognizing musical score images requires an accurate segmentation stage to isolate symbols from staff lines. This symbols/staves segregation is achieved by the use of inscribed line (chord) information. This information, processed by a multilayer perceptron, allows an efficient segmentation in terms of the remaining connected components. Some of these components are then classified, using another network, according to a coding of their skeleton graph. Special attention is paid to the design of the networks: the architectures are adapted to the specificities of each task. Multilayer perceptrons are employed here together with other more classical image analysis techniques which are also presented.},
	number = {01},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Martin, Philippe and Bellisant, Camille},
	year = {1992},
	pages = {193--208},
}

@phdthesis{martin_artificial_1992,
	type = {Theses},
	title = {Artificial neural networks : application to optical musical score recognition},
	url = {https://tel.archives-ouvertes.fr/tel-00340938},
	school = {Université Joseph-Fourier - Grenoble I},
	author = {Martin, Philippe},
	year = {1992},
	keywords = {analyse d'images, arbres de classification, automates à seuil, partitions musicales, reconnaissance de formes, réseaux de neurones artificiels, réseaux multi-couches},
	file = {Martin_1992_Artificial neural networks.pdf:/home/ptorras/zotpapers/Martin_1992_Artificial neural networks.pdf:application/pdf},
}

@inproceedings{mas-candela_sequential_2021,
	address = {Alicante, Spain},
	title = {Sequential {Next}-{Symbol} {Prediction} for {Optical} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Mas-Candela, Enrique and Alfaro-Contreras, María},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {13--17},
}

@mastersthesis{mateiu_unsupervised_2019,
	title = {Unsupervised {Learning} for {Domain} {Adaptation} in automatic classification tasks through {Neural} {Networks}},
	url = {http://hdl.handle.net/10045/96448},
	abstract = {Machine Learning systems have improved dramatically in recent years for automatic recognition and artificial intelligence tasks. In general, these systems are based on the use of a large amount of labeled data - also called training sets - in order to learn a model that fits the problem in question. The training set consists of examples of possible inputs to the system and the output expected from them. Achieving this training set is the main limitation to use Machine Learning systems, since it requires human effort to find and map possible inputs with their corresponding outputs. The situation is often frustrating since systems learn to solve the task for a specific domain - that is, a type of input with relatively homogeneous conditions – and they are not able to generalize to correctly solve the same task in other domains. This project considers the use of Domain Adaptation algorithms, which are capable of learning to adapt a Machine Learning model to work in an unknown domain based on only unlabeled data (unsupervised learning). This facilitates the transfer of systems to new domains because obtaining unlabeled data is relatively cheap, since the cost is to label them. To date, Domain Adaptation algorithms have been used in very restricted contexts, so this project aims to make an empirical evaluation of these algorithms in a greater number of cases, as well as propose possible improvements.},
	school = {Universidad de Alicante},
	author = {Mateiu, Tudor Nicolae},
	year = {2019},
	file = {Mateiu_2019_Unsupervised Learning for Domain Adaptation in automatic classification tasks.pdf:/home/ptorras/zotpapers/Mateiu_2019_Unsupervised Learning for Domain Adaptation in automatic classification tasks.pdf:application/pdf},
}

@inproceedings{matsushima_automated_1985,
	title = {Automated {High} {Speed} {Recognition} of {Printed} {Music} ({WABOT}-2 {Vision} {System})},
	url = {https://ci.nii.ac.jp/naid/10006937757/en/},
	abstract = {Concerns the intelligent robot WABOT-2, which can play an electronic piano, using ten fingers and feet, while reading printed music. It can hold a conversation with a man using an artificial voice. The paper reports on its vision system, which can recognize not only a printed score but also fine hand-written score or instant lettering score. The resulting musical robot vision performance is sufficient to permit the reading of one sheet of commercially available printed music for an electric piano with three parts. Pertinent data can be recognized in about 15 seconds, with 100\% accuracy (4 Refs.) electronic music; optical character recognition; robots},
	booktitle = {International {Conference} on {Advanced} {Robotics}},
	author = {Matsushima, T. and Sonomoto, I. and Harada, T. and Kanamori, K. and Ohteru, S.},
	year = {1985},
	keywords = {artificial voice, automated high speed recognition, C7410F (Communications computing), computer vision, electronic piano, fine hand-written score, instant lettering score computer vision equipment), intelligent robot, printed music, vision system, WABOT-2},
	pages = {477--482},
}

@inproceedings{mayer_obstacles_2022,
	address = {Online},
	title = {Obstacles with {Synthesizing} {Training} {Data} for {OMR}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Mayer, Jiří and Pecina, Pavel},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {15--19},
	file = {Mayer_Pecina_2022_Obstacles with Synthesizing Training Data for OMR.pdf:/home/ptorras/zotpapers/Mayer_Pecina_2022_Obstacles with Synthesizing Training Data for OMR.pdf:application/pdf},
}

@article{mcgee_optical_1991,
	title = {The {Optical} {Scanning} of {Medieval} {Music}},
	volume = {25},
	issn = {1572-8412},
	url = {https://doi.org/10.1007/BF00054288},
	doi = {10.1007/BF00054288},
	number = {1},
	journal = {Computers and the Humanities},
	author = {McGee, William and Merkley, Paul},
	year = {1991},
	pages = {47--53},
}

@article{mckay_style-independent_2007,
	title = {Style-independent computer-assisted exploratory analysis of large music collections},
	volume = {1},
	url = {https://www.researchgate.net/profile/Ichiro_Fujinaga/publication/237570792_Style-Independent_Computer-Assisted_Exploratory_Analysis_of_Large_Music_Collections_Buyuk_Muzik_Koleksiyonlarinin_Bicemden_Baimsiz_Bilgisayar_Destekli_Keif_Niteliinde_Cozumlenmesi/links/09e4150a2948c805f9000000.pdf},
	number = {1},
	journal = {Journal of Interdisciplinary Music Studies},
	author = {McKay, Cory and Fujinaga, Ichiro},
	year = {2007},
	pages = {63--85},
	file = {McKay_Fujinaga_2007_Style-independent computer-assisted exploratory analysis of large music.pdf:/home/ptorras/zotpapers/McKay_Fujinaga_2007_Style-independent computer-assisted exploratory analysis of large music.pdf:application/pdf},
}

@inproceedings{mcleod_evaluating_2018,
	address = {Paris, France},
	title = {Evaluating {Automatic} {Polyphonic} {Music} {Transcription}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/148_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {McLeod, Andrew and Steedman, Mark},
	year = {2018},
	pages = {42--49},
}

@techreport{mcpherson_page_1999,
	title = {Page {Turning} — {Score} {Automation} for {Musicians}},
	url = {http://hdl.handle.net/10092/13351},
	institution = {University of Canterbury, New Zealand},
	author = {McPherson, John R.},
	year = {1999},
	file = {McPherson_1999_Page Turning — Score Automation for Musicians.pdf:/home/ptorras/zotpapers/McPherson_1999_Page Turning — Score Automation for Musicians.pdf:application/pdf},
}

@misc{mcpherson_using_2001,
	title = {Using feedback to improve {Optical} {Music} {Recognition}},
	author = {McPherson, John R.},
	year = {2001},
	keywords = {feedback},
}

@inproceedings{mcpherson_introducing_2002,
	address = {Paris, France},
	title = {Introducing {Feedback} into an {Optical} {Music} {Recognition} {System}},
	url = {http://ismir2002.ircam.fr/proceedings/03-SP01-2.pdf},
	booktitle = {3rd {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {McPherson, John R.},
	year = {2002},
}

@techreport{mcpherson_coordinating_2002,
	title = {Coordinating {Knowledge} {Within} an {Optical} {Music} {Recognition} {System}},
	url = {https://www.researchgate.net/publication/2395549_Coordinating_Knowledge_Within_an_Optical_Music_Recognition_System},
	institution = {University of Waikato, Hamilton, New Zealand},
	author = {McPherson, John R. and Bainbridge, David},
	year = {2002},
	file = {McPherson_Bainbridge_2002_Coordinating Knowledge Within an Optical Music Recognition System.pdf:/home/ptorras/zotpapers/McPherson_Bainbridge_2002_Coordinating Knowledge Within an Optical Music Recognition System.pdf:application/pdf},
}

@phdthesis{mcpherson_coordinating_2006,
	type = {{PhD} {Thesis}},
	title = {Coordinating {Knowledge} {To} {Improve} {Optical} {Music} {Recognition}},
	url = {https://www.researchgate.net/profile/John_Mcpherson9/publication/242402211_COORDINATING_KNOWLEDGE_TO_IMPROVE_OPTICAL_MUSIC_RECOGNITION/links/55c0719908ae092e9666b75b.pdf},
	school = {The University of Waikato},
	author = {McPherson, John R.},
	year = {2006},
	file = {McPherson_2006_Coordinating Knowledge To Improve Optical Music Recognition.pdf:/home/ptorras/zotpapers/McPherson_2006_Coordinating Knowledge To Improve Optical Music Recognition.pdf:application/pdf},
}

@article{mehta_practical_2014,
	title = {Practical {Issues} in the {Field} of {Optical} {Music} {Recognition}},
	volume = {2},
	issn = {2321-7782},
	url = {http://www.ijarcsms.com/docs/paper/volume2/issue1/V2I1-0136.pdf},
	number = {1},
	journal = {International Journal of Advance Research in Computer Science and Management Studies},
	author = {Mehta, Apurva Ashokbhai and Bhatt, Malay S.},
	year = {2014},
	pages = {513--518},
}

@inproceedings{mehta_optical_2015,
	address = {Coimbatore, India},
	title = {Optical {Music} {Notes} {Recognition} for {Printed} {Piano} {Music} {Score} {Sheet}},
	isbn = {978-1-4799-6805-3},
	doi = {10.1109/ICCCI.2015.7218061},
	abstract = {Entertainment, Therapy and Education are the fields where music is always found in couple with homo-sapiens. Music is presented in various formats to us like aural, visual and one more - written form of music that is known very less to us. In a way music dominates our life. System discussed in this paper inputs music score written for piano music using modern staff notations as image. Segmentation is carried out using hierarchical decomposition using thresholding along with stave lines of score sheet. Segmented symbols are recognized through an established artificial neural network based on boosting approach. Recognized symbols are represented in an admissible way. System is capable enough of addressing very complex cases and validation is done over 53 songs available at various global music scores resources. Segmentation algorithms achieve accuracy of 99.12\% and segmented symbols are recognized with prompt accuracy of 92.38\% through the help of PCA and AdaBoost.},
	booktitle = {International {Conference} on {Computer} {Communication} and {Informatics}},
	author = {Mehta, Apurva A. and Bhatt, Malay S.},
	year = {2015},
	keywords = {to classify, - grand stave, adaboost, hierarchical decomposition, measures, modern staff notation, pca using, piano score, svd},
}

@article{mengarelli_omr_2019,
	title = {{OMR} metrics and evaluation: a systematic review},
	issn = {1573-7721},
	doi = {10.1007/s11042-019-08200-0},
	abstract = {Music is rhythm, timbre, tones, intensity and performance. Conventional Western Music Notation (CWMN) is used to generate Music Scores in order to register music on paper. Optical Music Recognition (OMR) studies techniques and algorithms for converting music scores into a readable format for computers. work presents a systematic literature review (SLR) searching for metrics and methods of evaluation and comparing for OMR systems and algorithms. The most commonly used metrics on OMR works are described. A research protocol is elaborated and executed. From 802 publications found, 94 are evaluated. All results are organized and classified focusing on metrics, stages, comparisons, OMR datasets and related works. Although there is still no standard methodology for evaluating OMR systems, a good number of datasets and metrics are already available and apply to all the stages of OMR. Some of the analyzed works can give good directions for future works.},
	journal = {Multimedia Tools and Applications},
	author = {Mengarelli, Luciano and Kostiuk, Bruno and Vitório, João G. and Tibola, Maicon A. and Wolff, William and Silla, Carlos N.},
	month = dec,
	year = {2019},
}

@techreport{metaj_mnr_2019,
	type = {resreport},
	title = {{MNR}: {MUSCIMA} {Notes} {Recognition}. {Using} {Faster} {R}-{CNN} on handwritten music dataset.},
	institution = {Politecnico di Milano},
	author = {Metaj, Stiven and Magnolfi, Federico},
	year = {2019},
	doi = {10.13140/RG.2.2.29120.48640},
	file = {Metaj_Magnolfi_2019_MNR.pdf:/home/ptorras/zotpapers/Metaj_Magnolfi_2019_MNR.pdf:application/pdf},
}

@inproceedings{mexin_tools_2017,
	address = {Espoo, Finland},
	title = {Tools for {Annotating} {Musical} {Measures} in {Digital} {Music} {Editions}},
	url = {http://smc2017.aalto.fi/media/materials/proceedings/SMC17_p279.pdf},
	booktitle = {14th {Sound} and {Music} {Computing} {Conference}},
	author = {Mexin, Yevgen and Hadjakos, Aristotelis and Berndt, Axel and Waloschek, Simon and Wawilow, Anastasia and Szwillus, Gerd},
	year = {2017},
	pages = {279--286},
}

@inproceedings{mico_incremental_2018,
	title = {Incremental {Learning} for {Recognition} of {Handwritten} {Mensural} {Notation}},
	url = {https://sites.google.com/site/faimmusic2018/program},
	booktitle = {11th {International} {Workshop} on {Machine} {Learning} and {Music}},
	author = {Micó, Luisa and Iñesta, José Manuel and Rizo, David},
	year = {2018},
}

@inproceedings{mico_adaptively_2020,
	address = {Cham},
	title = {Adaptively {Learning} to {Recognize} {Symbols} in {Handwritten} {Early} {Music}},
	isbn = {978-3-030-43887-6},
	doi = {10.1007/978-3-030-43887-6_40},
	abstract = {Human supervision is necessary for a correct edition and publication of handwritten early music collections. The output of an optical music recognition system for that kind of documents may contain a significant number of errors, making it tedious to correct for a human expert. An adequate strategy is needed to optimize the human feedback information during the correction stage to adapt the classifier to the specificities of each manuscript. In this paper, we compare the performance of a neural system, difficult and slow to be retrained, and a nearest neighbor strategy, based on the neural codes provided by a neural net, trained offline, used as a feature extractor.},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Micó, Luisa and Oncina, Jose and Iñesta, José M.},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	pages = {470--477},
}

@inproceedings{min_research_2011,
	title = {Research on numbered musical notation recognition and performance in a intelligent system},
	doi = {10.1109/ICBMEI.2011.5916943},
	abstract = {A intelligent system with numbered musical notation recognition and performance (NMRPIS) is presented which is based on notation recognition and can play digital music automatically. The system combines with OMR to analyze musical notation, interpret completely, form the output quickly and efficiently by the embedded program. The experimental result indicates this system has high classification rate and higher recognition performance.},
	booktitle = {International {Conference} on {Business} {Management} and {Electronic} {Information}},
	author = {Min, Du},
	year = {2011},
	keywords = {Optical imaging, Feature extraction, Heuristic algorithms, Music, Image recognition, optical music recognition, OMR, optical character recognition, music, pattern classification, electronic music, digital music, embedded program, embedded systems, intelligent system, Intelligent systems, notation recognition, numbered musical notation recognition, Support vector machine classification},
	pages = {340--343},
}

@mastersthesis{miro_recognition_2019,
	address = {Barcelona},
	title = {Recognition of musical symbols in scores using neural networks},
	url = {http://hdl.handle.net/2117/165583},
	abstract = {Object detection is present nowadays in many aspects of our life. From security to entertainment, its applications play a key role in computer vision and image processing worlds. This thesis addresses, through the usage of an object detector, the creation of an application that allows its user to play a music score. The main goal is to display a digital music score and be able to play it by touching on its notes. In order to achieve the proposed system, deep learning techniques based on neural networks are used to detect musical symbols from a digitized score and infer their position along the staff lines. Different models and approaches are considered to tackle the main objective.},
	school = {Universitat Politècnica de Catalunya},
	author = {Miró, Jordi Burgués},
	month = jun,
	year = {2019},
}

@inproceedings{mitobe_fast_2004,
	title = {A fast {HMM} algorithm based on stroke lengths for on-line recognition of handwritten music scores},
	doi = {10.1109/IWFHR.2004.2},
	abstract = {The hidden Markov model (HMM) has been successfully applied to various kinds of on-line recognition problems including, speech recognition, handwritten character recognition, etc. In this paper, we propose an on-line method to recognize handwritten music scores. To speed up the recognition process and improve usability of the system, the following methods are explained: (1) The target HMMs are restricted based on the length of a handwritten stroke, and (2) Probability calculations of HMMs are successively made as a stroke is being written. As a result, recognition rates of 85.78\% and average recognition times of 5.19 ms/stroke were obtained for 6,999 test strokes of handwritten music symbols, respectively. The proposed HMM recognition rate is 2.4\% higher than that achieved with the traditional method, and the processing time was 73\% of that required by the traditional method.},
	booktitle = {9th {International} {Workshop} on {Frontiers} in {Handwriting} {Recognition}},
	author = {Mitobe, Youichi and Miyao, Hidetoshi and Maruyama, Minoru},
	year = {2004},
	note = {ISSN: 1550-5235},
	keywords = {Hidden Markov models, Testing, Handwriting recognition, HMM, Shape, Multiple signal classification, hidden Markov models, Character recognition, Speech recognition, handwritten character recognition, fast HMM algorithm, Handwritten Music Score Recognition, handwritten music scores, handwritten music symbols, handwritten stroke, On-line Symbol Recognition, online recognition, Probability, Probability calculations, stroke lengths, Target recognition, Usability},
	pages = {521--526},
}

@inproceedings{miyao_head_1995,
	title = {Head and stem extraction from printed music scores using a neural network approach},
	isbn = {0-8186-7128-9},
	doi = {10.1109/ICDAR.1995.602095},
	abstract = {In an automatic music score recognition system, it is very important to extract heads and stems of notes, since these symbols are most ubiquitous in a score and musically important. The purpose of our system is to present an accurate and high-speed extraction of note heads (except the whole notes) and stems according to the following procedure. (1) We extract all regions which are considered as candidates of stems or heads. (2) To identify heads from the candidates, we use a three-layer neural network. (3) The weights for the network are learned by the back propagation method. In the learning, the network learns the spatial constraints between heads and surroundings rather than the shapes of heads. (4) After the learning process is completed we use this network to identify a number of test head candidates (5) The stem candidates touching the detected heads are extracted as true stems. As an experimental result, we obtained high recognition rates of 99.0\% and 99.2\% for stems and note heads, respectively. It took between 40 to 100 seconds to process a printed piano score on A4 sheet using a workstation. Therefore, our system can analyze it at least 10 times as fast as manual methods},
	booktitle = {3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Miyao, Hidetoshi and Nakano, Yasuaki},
	year = {1995},
	pages = {1074--1079},
}

@article{miyao_note_1996,
	title = {Note symbol extraction for printed piano scores using neural networks},
	volume = {E79-D},
	url = {https://search.ieice.org/bin/summary.php?id=e79-d_5_548},
	abstract = {In the traditional note symbol extraction processes, extracted candidates of note elements were identified using complex if-then rules based on the note formation rules and they needed subtle adjustment of parameters through many experiments. The purpose of our system is to avoid the tedious tasks and to present an accurate and high-speed extraction of note heads, stems and flags according to the following procedure. (1) We extract head and flag candidates based on the stem positions. (2) To identify heads and flags from the candidates, we use a couple of three-layer neural networks. To make the networks learn, we give the position informations and reliability factors of candidates to the input units. (3) With the weights learned by the net, the head and flag candidates are recognized. As an experimental result, we obtained a high extraction rate of more than 99\% for thirteen printed piano scores on A4 sheet which have various difficulties. Using a workstation (SPARC Station 10), it took about 90 seconds to do on the average. It means that our system can analyze piano scores 5 times or more as fast as the manual work. Therefore, our system can execute the task without the traditional tedious works, and can recognize them quickly and accurately (9 Refs.) recognition},
	number = {5},
	journal = {IEICE Transactions on Information and Systems},
	author = {Miyao, Hidetoshi and Nakano, Yasuaki},
	year = {1996},
	keywords = {character recognition, template matching, C1250B (Character recognition), C5290 (Neural computing techniques), flag candidates, head candidates, high-speed extraction, note symbol extraction, position informations, printed piano scores, reliability factors, score recognition techniques), SPARC Station 10, three-layer neural networks},
	pages = {548--554},
}

@inproceedings{miyao_format_2000,
	address = {Brasil},
	title = {Format of {Ground} {Truth} {Data} {Used} in the {Evaluation} of the {Results} of an {Optical} {Music} {Recognition} {System}},
	url = {https://www.researchgate.net/profile/Robert_Haralick/publication/242138660_Format_of_Ground_Truth_Data_Used_in_the_Evaluation_of_the_Results_of_an_Optical_Music_Recognition_System/links/0046353bac1589cc3f000000.pdf},
	booktitle = {4th {International} {Workshop} on {Document} {Analysis} {Systems}},
	author = {Miyao, Hidetoshi and Haralick, Robert Martin},
	year = {2000},
	keywords = {evaluation},
	pages = {497--506},
	file = {Miyao_Haralick_2000_Format of Ground Truth Data Used in the Evaluation of the Results of an Optical.pdf:/home/ptorras/zotpapers/Miyao_Haralick_2000_Format of Ground Truth Data Used in the Evaluation of the Results of an Optical.pdf:application/pdf},
}

@inproceedings{miyao_stave_2002,
	title = {Stave {Extraction} for {Printed} {Music} {Scores}},
	isbn = {978-3-540-45675-9},
	url = {https://link.springer.com/chapter/10.1007/3-540-45675-9_85},
	abstract = {In this paper, a satisfactory method is described for the extraction of staff lines in which there are some inclinations, discontinuities, and curvatures. The extraction calls for four processes: (1) Extraction of specific points on a stave on vertical scan lines, (2) Connection of the points using DP matching, (3) Composition of stave groups using labeling, and (4) Extraction and adjustment of the edges of lines. The experiment resulted in an extraction rate of 99.4\% for 71 printed music scores that included lines with some inclinations, discontinuities, and curvatures.},
	booktitle = {Intelligent {Data} {Engineering} and {Automated} {Learning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Miyao, Hidetoshi},
	editor = {Yin, Hujun and Allinson, Nigel and Freeman, Richard and Keane, John and Hubbard, Simon},
	year = {2002},
	pages = {562--568},
}

@inproceedings{miyao_online_2004,
	title = {An online handwritten music score recognition system},
	doi = {10.1109/icpr.2004.1334164},
	booktitle = {17th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {Institute of Electrical \& Electronics Engineers (IEEE)},
	author = {Miyao, Hidetoshi and Maruyama, Minoru},
	year = {2004},
}

@article{modayur_muser_1993,
	title = {{MUSER}: {A} prototype musical score recognition system using mathematical morphology},
	volume = {6},
	issn = {1432-1769},
	doi = {10.1007/BF01211937},
	abstract = {Music representation utilizes a fairly rich repertoire of symbols. These symbols appear on a score sheet with relatively little shape distortion, differing from the prototype symbol shapes mainly by a positional translation and scale change. The prototype system we describe in this article is aimed at recognizing printed music notation from digitized music score images. The recognition system is composed of two parts: a low-level vision module that uses morphological algorithms for symbol detection and a high-level module that utilizes prior knowledge of music notation to reason about spatial positions and spatial sequences of these symbols. The high-level module also employs verification procedures to check the veracity of the output of the morphological symbol recognizer. The system produces an ASCII representation of music scores that can be input to a music-editing system. Mathematical morphology provides us the theory and the tools to analyze shapes. This characteristic of mathematical morphology lends itself well to analyzing and subsequently recognizing music scores that are rich in well-defined musical symbols. Since morphological operations can be efficiently implemented in machine vision systems that have special hardware support, the recognition task can be performed in near real-time. The system achieves accuracy in excess of 95\% on the sample scores processed so far with a peak accuracy of 99.7\% for the quarter and eighth notes, demonstrating the efficacy of morphological techniques for shape extraction.},
	number = {2},
	journal = {Machine Vision and Applications},
	author = {Modayur, Bharath R. and Ramesh, Visvanathan and Haralick, Robert M. and Shapiro, Linda G.},
	year = {1993},
	pages = {140--150},
}

@techreport{modayur_music_1996,
	title = {Music {Score} {Recognition} - {A} {Selective} {Attention} {Approach} using {Mathematical} {Morphology}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.887&rep=rep1&type=pdf},
	institution = {Electrical Engineering Department, University of Washington, Seattle},
	author = {Modayur, Bharath R.},
	year = {1996},
	file = {Modayur_1996_Music Score Recognition - A Selective Attention Approach using Mathematical.pdf:/home/ptorras/zotpapers/Modayur_1996_Music Score Recognition - A Selective Attention Approach using Mathematical.pdf:application/pdf},
}

@inproceedings{montagner_learning_2014,
	title = {Learning to remove staff lines from music score images},
	doi = {10.1109/ICIP.2014.7025529},
	abstract = {The methods for removal of staff lines rely on characteristics specific to musical documents and they are usually not robust to some types of imperfections in the images. To overcome this limitation, we propose the use of binary morphological operator learning, a technique that estimates a local operator from a set of example images. Experimental results in both synthetic and real images show that our approach can adapt to different types of deformations and achieves similar or better performance than existing methods in most of the test scenarios.},
	booktitle = {International {Conference} on {Image} {Processing}},
	author = {Montagner, Igor dos Santos and Hirata, Roberto Jr. and Hirata, Nina S. T.},
	year = {2014},
	note = {ISSN: 1522-4880},
	keywords = {Machine Learning, Optical imaging, Optical Music Recognition, music score images, Training, Text analysis, Robustness, Accuracy, music, learning (artificial intelligence), document handling, Skeleton, Learning systems, Document analysis, Staff Removal, binary morphological operator learning, deformation, local operator, musical document characteristics, staff line removal method},
	pages = {2614--2618},
}

@inproceedings{montagner_machine_2014,
	title = {A {Machine} {Learning} based method for {Staff} {Removal}},
	isbn = {978-1-4799-5208-3},
	doi = {10.1109/ICPR.2014.545},
	abstract = {Staff line removal is an important pre-processing step to convert content of music score images to machine readable formats. Many heuristic algorithms have been proposed for staff removal and recently a competition was organized in the 2013 ICDAR/GREC conference. Music score images are often subject to different deformations and variations, and existing algorithms do not work well for all cases. We investigate the application of a machine learning based method for the staff removal problem. The method consists in learning multiple image operators from training input-output pairs of images and then combining the results of these operators. Each operator is based on local information provided by a neighborhood window, which is usually manually chosen based on the content of the images. We propose a feature selection based approach for automatically defining the windows and also for combining the operators. The performance of the proposed method is superior to several existing methods and is comparable to the best method in the competition. © 2014 IEEE.},
	booktitle = {22nd {International} {Conference} on {Pattern} {Recognition}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Montagner, Igor dos Santos and Hirata, Roberto Jr. and Hirata, Nina S. T.},
	year = {2014},
	note = {ISSN: 1051-4651},
	keywords = {Artificial intelligence, Heuristic algorithms, Pattern recognition, Music scores, Learning systems, Input-output, Line removal, Local information, Machine-readable format, Multiple image, Personnel training, Pre-processing step},
	pages = {3162--3167},
}

@article{montagner_staff_2017,
	title = {Staff removal using image operator learning},
	volume = {63},
	issn = {0031-3203},
	doi = {10.1016/j.patcog.2016.10.002},
	abstract = {Staff removal is an image processing task that aims to facilitate further analysis of music score images. Even when restricted to images in specific domains such as music score recognition, solving image processing problems usually requires the design of customized algorithms. To cope with image variabilities and the growing amount of data, machine learning based techniques emerge as a natural approach to be employed in image processing problems. In this sense, image operator learning methods are concerned with estimating, from sample pairs of input-output images of a transformation, a local function that characterizes the image transformation. These methods require the definition of some parameters, including the local information to be considered in the processing which is defined by a window. In this work we show how to apply the image operator learning technique to the staff line removal problem. We present an algorithm for window determination and show that it captures visual information relevant for staff removal. We also present a reference window set to be used in cases where the training set is not sufficiently large. Experimental results obtained with respect to synthetic and handwritten music scores under varying image conditions show that the learned image operators are comparable with especially designed state-of-the-art heuristic algorithms. © 2016 Elsevier Ltd},
	journal = {Pattern Recognition},
	author = {Montagner, Igor dos Santos and Hirata, Nina S.T. and Hirata, Roberto Jr.},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Optical music recognition, Artificial intelligence, Heuristic algorithms, Machine learning, Image processing, Staff removal, Document image analysis, Image operator, Image analysis, Learning systems, Optical data processing, Image operators, Image processing problems, Image transformations, Input-output image, Natural approaches, Problem solving, Visual information},
	pages = {310--320},
	file = {ScienceDirect Snapshot:/home/ptorras/Zotero/storage/BL9GT624/S0031320316303181.html:text/html},
}

@misc{ringwalt_moonlight_2018,
	title = {Moonlight},
	url = {https://github.com/ringw/moonlight},
	author = {Ringwalt, Dan},
	year = {2018},
}

@inproceedings{moss_challenging_2022,
	address = {Online},
	title = {Challenging sources: a new dataset for {OMR} of diverse 19th-century music theory examples},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Moss, Fabian C. and López, Néstor Nápoles and Köster, Maik and Rizo, David},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {4--8},
	file = {Moss et al_2022_Challenging sources.pdf:/home/ptorras/zotpapers/Moss et al_2022_Challenging sources.pdf:application/pdf},
}

@misc{pacha_music_2020,
	title = {The {Music} {Notation} {Graph} ({MuNG}) {Repository}},
	url = {https://github.com/OMR-Research/mung},
	author = {Pacha, Alexander and Hajič jr., Jan},
	year = {2020},
}

@misc{pacha_github_2017,
	title = {Github {Repository} of the {Music} {Score} {Classifier}},
	url = {https://github.com/apacha/MusicScoreClassifier},
	author = {Pacha, Alexander},
	year = {2017},
}

@inproceedings{napoles_encoding_2018,
	address = {Paris, France},
	title = {Encoding {Matters}},
	isbn = {978-1-4503-6522-2},
	url = {http://doi.acm.org/10.1145/3273024.3273027},
	doi = {10.1145/3273024.3273027},
	booktitle = {5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Nápoles, Néstor and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018},
	keywords = {music information retrieval, music21, music notation, music transcription, symbolic music, humdrum, humlib, mei, music encoding, musicxml, verovio, vis},
	pages = {69--73},
}

@techreport{nehab_staff_2003,
	title = {Staff {Line} {Detection} by {Skewed} {Projection}},
	url = {https://pdfs.semanticscholar.org/142c/dc7231a7a8093fd2da6f293a36862e592733.pdf},
	author = {Nehab, Diego},
	year = {2003},
}

@inproceedings{ng_segmentation_1992,
	address = {London},
	title = {Segmentation of {Music} {Primitives}},
	isbn = {978-1-4471-3201-1},
	doi = {10.1007/978-1-4471-3201-1_49},
	abstract = {In this paper, low-level knowledge directed pre-processing and segmentation of music scores are presented. We discuss some of the problems that have been overlooked by existing research but have proved to be major obstacles for robust optical music recognisers [1] to help entering music into a computer, including sub-segmentation of interconnected primitives and identification of nonstraight stave lines, and present solutions to these problems. We conclude that, with knowledge, a significant improvement in low-level segmentations can be achieved.},
	booktitle = {{BMVC92}},
	publisher = {Springer London},
	author = {Ng, Kia and Boyle, Roger},
	editor = {Hogg, David and Boyle, Roger},
	year = {1992},
	pages = {472--480},
}

@inproceedings{ng_low-_1995,
	title = {Low- and high-level approaches to optical music score recognition},
	doi = {10.1049/ic:19951184},
	abstract = {The computer has become an increasingly important device in music. It can not only generate sound but is also able to perform time consuming and repetitive tasks, such as transposition and part extraction, with speed and accuracy. However, a score must be represented in a machine readable format before any operation can be carried out. Current input methods, such as using an electronic keyboard, are time consuming and require human intervention. Optical music recognition (OMR) provides an interesting, efficient and automatic method to transform paper-based music scores into a machine representation. The authors outline the techniques for pre-processing and discuss the heuristic and musical rules employed to enhance recognition. A spin-off application that makes use of the intermediate results to enhance stave lines is also presented. The authors concentrate on the techniques used for time-signature detection, discuss the application of frequently-found rhythmical patterns to clarify the results of OMR, and propose possible enhancements using such knowledge. They believe that domain-knowledge enhancement is essential for complex document analysis and recognition. Other possible areas of development include melodic, harmonic and stylistic analysis to improve recognition results further.},
	booktitle = {{IEE} {Colloquium} on {Document} {Image} {Processing} and {Multimedia} {Environments}},
	author = {Ng, Kia and Boyle, Roger and Cooper, David},
	year = {1995},
	keywords = {Music, optical music recognition, pattern recognition, Pattern recognition, OMR, optical character recognition, music, optical music score recognition, automatic method, complex document analysis, domain-knowledge enhancement, frequently-found rhythmical pattern, heuristic rule, high-level approach, humanities, Humanities, low level approach, machine readable format, machine representation, music computing, musical score, Optical character recognition, paper-based music score, rhythm, stylistic analysis, technique, time-signature detection},
	pages = {31--36},
}

@article{ng_recognition_1996,
	title = {Recognition and reconstruction of primitives in music scores},
	volume = {14},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/0262885695010386},
	doi = {10.1016/0262-8856(95)01038-6},
	abstract = {Music recognition bears similarities and differences to OCR. In this paper we identify some of the problems peculiar to musical scores, and propose an approach which succeeds in a wide range of non-trivial cases. The composer customarily proceeds by writing notes, then stems, beams, ties and slurs — we have inverted this approach by segmenting and then subsegmenting scores to recapture the component parts of symbols. In this paper, we concentrate on the strategy of recognizing sub-segmented primitives, and the reassembly process which reconstructs low level graphical primitives back to musical symbols. The sub-segmentation process proves to be worthwhile, since many primitives complement each other and high level musical theory can be employed to enhance the recognition process.},
	number = {1},
	journal = {Image and Vision Computing},
	author = {Ng, Kia and Boyle, Roger},
	year = {1996},
	keywords = {OCR, to classify, Document analysis, Score recognition},
	pages = {39--46},
}

@inproceedings{ng_embracing_1999,
	title = {Embracing the {Composer} : {Optical} {Recognition} of {Handwrtten} {Manuscripts}},
	url = {https://ci.nii.ac.jp/naid/10011612045/en/},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Ng, Kia and Cooper, David and Stefani, Ewan and Boyle, Roger and Bailey, Nick},
	year = {1999},
	pages = {500--503},
}

@article{ng_music_2002,
	title = {Music manuscript tracing},
	volume = {2390},
	issn = {1611-3349},
	url = {http://www.springerlink.com/index/1JA4UUJULCH2XNTB.pdf},
	doi = {10.1007/3-540-45868-9_29},
	journal = {Lecture Notes in Computer Science},
	author = {Ng, Kia},
	year = {2002},
	note = {ISBN: 3 540 44066 6},
	pages = {322--334},
}

@incollection{ng_optical_2004,
	title = {Optical {Music} {Analysis} for {Printed} {Music} {Score} and {Handwritten} {Music} {Manuscript}},
	booktitle = {Visual {Perception} of {Music} {Notation}: {On}-{Line} and {Off} {Line} {Recognition}},
	publisher = {IGI Global},
	author = {Ng, Kia},
	year = {2004},
	doi = {10.4018/978-1-59140-298-5.ch004},
	pages = {108--127},
}

@inproceedings{ng_big_2014,
	title = {Big {Data} {Optical} {Music} {Recognition} with {Multi} {Images} and {Multi} {Recognisers}},
	url = {http://www.bcs.org/upload/pdf/ewic_ev14_s14paper4.pdf},
	doi = {10.14236/ewic/eva2014.26},
	booktitle = {{EVA} {London} 2014 on {Electronic} {Visualisation} and the {Arts}},
	publisher = {BCS},
	author = {Ng, Kia and McLean, Alex and Marsden, Alan},
	year = {2014},
	pages = {215--218},
	file = {Ng et al_2014_Big Data Optical Music Recognition with Multi Images and Multi Recognisers.pdf:/home/ptorras/zotpapers/Ng et al_2014_Big Data Optical Music Recognition with Multi Images and Multi Recognisers.pdf:application/pdf},
}

@inproceedings{nguyen_automatic_2014,
	address = {Siem Reap},
	title = {Automatic {Touching} {Detection} and {Recognition} of {Music} {Chord} {Using} {Auto}-encoding and {Softmax}},
	doi = {10.1145/2557977.2558055},
	abstract = {Humankind envisioned an age of automatic where many machines perform all cumbersome and tedious tasks and we just enjoy. Playing music is not a tedious work but a program that plays music from music sheet image automatically can increase productivity of musician or bring convenience to amateurs. Following its requirement, we studied a specific task in Optical Music Recognition problem that is touching chord. Specially, touching chord becomes a critical problem on mobile device captured image because of some objective conditions. In this paper we showed our proposed method which used Autoencoder and Softmax classifier. The experiment results showed that our method is very promising. We get 94.117\% accuracy in detect non-touching phase and 96.261\% in separate phase.},
	booktitle = {8th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Hong Quy and Yang, Hyung-Jeong and Kim, Soo-Hyung and Lee, Guee-Sang},
	year = {2014},
	keywords = {Optical music recognition, Learning systems, Information management, Auto encoders, Communication, Critical problems, Mobile devices, Specific tasks, Touching chord},
}

@article{nguyen_lightweight_2015,
	title = {A {Lightweight} and {Effective} {Music} {Score} {Recognition} on {Mobile} {Phones}},
	volume = {11},
	doi = {10.3745/JIPS.02.0023},
	number = {3},
	journal = {Journal of Information Processing Systems},
	author = {Nguyen, Tam and Lee, Gueesang},
	year = {2015},
	keywords = {to classify, music score, svm, symbol classification, mobile camera},
	pages = {438--449},
	file = {Nguyen_Lee_2015_A Lightweight and Effective Music Score Recognition on Mobile Phones.pdf:/home/ptorras/zotpapers/Nguyen_Lee_2015_A Lightweight and Effective Music Score Recognition on Mobile Phones.pdf:application/pdf},
}

@inproceedings{nhat_adaptive_2014,
	address = {Siem Reap, Cambodia},
	title = {Adaptive {Line} {Fitting} for {Staff} {Detection} in {Handwritten} {Music} {Score} {Images}},
	isbn = {978-1-4503-2644-5},
	doi = {10.1145/2557977.2558057},
	abstract = {The target of staff line detection is to extract staff lines accurately in order to remove them while preserves the shape of musical symbols. There are several researches in staff line detection and removal which provide good results with printed scores. However, in case of handwritten music scores, detecting staff lines still has problems due to the diversity of musical symbol shape, line curvature and disconnection. In this paper, we present a novel line fitting method for detecting the staff line in handwritten music score images. Our method first starts with the estimation of staff line height and staff space height. Then the staff segments are selected. Based on these staff candidates, we construct a line with the orientation of the staff segment and gradually fit it to the real lines. The staff line is then removed and the process is continuing until no line is detected. To show the effectiveness of our proposed method with different types of handwritten music score, images from the ICDAR/GREC 2013 dataset are tested. The experiment results show the advantages of our algorithm comparing with the previous approaches. Copyright 2014 ACM.},
	booktitle = {8th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication}},
	publisher = {ACM},
	author = {Nhat, Vo Quang and Lee, GueeSang},
	year = {2014},
	keywords = {stable path, staff line detection, staff line removal, line fitting},
	pages = {991--996},
}

@article{niitsuma_towards_2018,
	title = {Towards {Musicologist}-{Driven} {Mining} of {Handwritten} {Scores}},
	volume = {33},
	issn = {1541-1672},
	doi = {10.1109/MIS.2018.111144115},
	abstract = {Historical musicologists have been seeking for objective and powerful techniques to collect, analyse and verify their findings for many decades. The aim of this study is to propose a musicologist-driven mining method for extracting quantitative information from early music manuscripts. Our focus is on finding evidence for the chronological ordering of J.S. Bachs manuscripts. Bachs C-clefs were extracted from a wide range of manuscripts under the direction of domain experts, and with these the classification of C-clefs was conducted. The proposed methods were evaluated on a dataset containing over 1000 clefs extracted from J.S. Bachs manuscripts. The results show more than 70\% accuracy for dating J.S. Bachs manuscripts, providing a rough barometer to be combined with other evidence to evaluate musicologists hypotheses, and the practicability of this domain-driven approach is demonstrated.},
	number = {4},
	journal = {IEEE Intelligent Systems},
	author = {Niitsuma, Masahiro and Tomita, Yo and Yan, Wei Qi and Bell, David},
	year = {2018},
	keywords = {Feature extraction, pattern recognition, Support vector machines, music, Object recognition, Data mining, Intelligent systems, applications, arts and humanities, computer applications, computing methodologies, data mining, database applications, database management, handwriting analysis, information technology and systems, Knowledge discovery, Radio frequency},
	pages = {24--34},
	file = {Niitsuma et al_2018_Towards Musicologist-Driven Mining of Handwritten Scores.pdf:/home/ptorras/zotpapers/Niitsuma et al_2018_Towards Musicologist-Driven Mining of Handwritten Scores.pdf:application/pdf},
}

@article{noll_intelligentes_2019,
	title = {Intelligentes {Notenlesen}: {Programme} zum {Digitalisieren} gedruckter {Musiknoten}},
	volume = {18},
	url = {https://shop.heise.de/katalog/intelligentes-notenlesen},
	language = {German},
	journal = {c't},
	author = {Noll, Justus},
	year = {2019},
	pages = {122--126},
}

@misc{neuratron_notateme_2015,
	title = {{NotateMe}},
	url = {http://www.neuratron.com/notateme.html},
	author = {{Neuratron}},
	year = {2015},
}

@inproceedings{novotny_introduction_2015,
	title = {Introduction to {Optical} {Music} {Recognition}: {Overview} and {Practical} {Challenges}},
	url = {http://ceur-ws.org/Vol-1343/paper6.pdf},
	abstract = {Music has been always an integral part of human culture. In our computer age, it is not surprising that there is a growing interest to store music in a digitized form. Optical music recognition (OMR) refers to a discipline that investigates music score recognition systems. This is similar to well-known optical character recognition systems, except OMR systems try to automatically transform scanned sheet music into a computer-readable format. In such a digital format, semantic information is also stored (instrumentation, notes, pitches and duration, contextual information, etc.). This article introduces the OMR field and presents an overview of the relevant literature and basic techniques. Practical challenges and questions arising from the automatic recognition of music notation and its semantic interpretation are discussed as well as the most important open issues.},
	booktitle = {Annual {International} {Workshop} on {DAtabases}, {TExts}, {Specifications} and {Objects}},
	publisher = {CEUR-WS},
	author = {Novotnỳ, Jiri and Pokornỳ, Jaroslav},
	editor = {Necasky M., Moravec P., Pokorny J.},
	year = {2015},
	note = {ISSN: 1613-0073},
	keywords = {Artificial intelligence, Automatic recognition, Character recognition, Contextual information, Document image analysis, Learning systems, Optical character recognition, Optical character recognition system, Optical music recognition, Recognition systems, Semantic information, Semantic interpretation, Semantics, Specifications},
	pages = {65--76},
	file = {Novotny and Pokorny - Introduction to Optical Music Recognition - Overview and Practical Challenges.pdf:/home/ptorras/Zotero/storage/DVAWZ5LS/Novotny and Pokorny - IInnttrroodduuccttiioonn ttoo OOppttiiccaall M Muu.pdf:application/pdf;Novotny_Pokorny_2015_Introduction to Optical Music Recognition.pdf:/home/ptorras/zotpapers/Novotny_Pokorny_2015_Introduction to Optical Music Recognition.pdf:application/pdf},
}

@mastersthesis{nunez_alcover_glyph_2019,
	title = {Glyph and {Position} {Classification} of {Music} {Symbols} in {Early} {Manuscripts}},
	url = {http://hdl.handle.net/10045/96451},
	abstract = {In this research, we study how to classify of handwritten music symbols in early music manuscripts written in white Mensural notation, a common notation system used since the fourteenth century and until the Renaissance. The field of Optical Music Recognition researches how to automate the reading of musical scores to transcribe its content to a structured digital format such as MIDI. When dealing with music manuscripts, the traditional workflow establishes two separate stages of detection and classification of musical symbols. In the classification stage, most of the research focuses on detecting musical symbols, without taking into account that a musical note is defined in two components: glyph and its position with respect to the staff. Our purpose will consist of the design and implementation of architectures in the field of Deep Learning, using Convolutional Neural Networks (CNNs) as well as its evaluation and comparison to determine which model provides the best performance in terms of efficiency and precision for its implementation in an interactive scenario.},
	school = {Universidad de Alicante},
	author = {Núñez Alcover, Alicia},
	year = {2019},
	file = {Nunez Alcover_2019_Glyph and Position Classification of Music Symbols in Early Manuscripts.pdf:/home/ptorras/zotpapers/Nunez Alcover_2019_Glyph and Position Classification of Music Symbols in Early Manuscripts.pdf:application/pdf},
}

@inproceedings{nunez-alcover_glyph_2019,
	address = {Cham},
	title = {Glyph and {Position} {Classification} of {Music} {Symbols} in {Early} {Music} {Manuscripts}},
	isbn = {978-3-030-31321-0},
	doi = {10.1007/978-3-030-31321-0_14},
	abstract = {Optical Music Recognition is a field of research that automates the reading of musical scores so as to transcribe their content into a structured digital format. When dealing with music manuscripts, the traditional workflow establishes separate stages of detection and classification of musical symbols. In the latter, most of the research has focused on detecting musical glyphs, ignoring that the meaning of a musical symbol is defined by two components: its glyph and its position within the staff. In this paper we study how to perform both glyph and position classification of handwritten musical symbols in early music manuscripts written in white Mensural notation, a common notation system used for the most part of the XVI and XVII centuries. We make use of Convolutional Neural Networks as the classification method, and we tested several alternatives such as using independent models for each component, combining label spaces, or using both multi-input and multi-output models. Our results on early music manuscripts provide insights about the effectiveness and efficiency of each approach.},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Nuñez-Alcover, Alicia and de León, Pedro J. Ponce and Calvo-Zaragoza, Jorge},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	pages = {159--168},
}

@article{oh_online_2017,
	title = {Online recognition of handwritten music symbols},
	volume = {20},
	doi = {10.1007/s10032-017-0281-y},
	number = {2},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Oh, Jiyong and Son, Sung Joon and Lee, Sangkuk and Kwon, Ji-Won and Kwak, Nojun},
	year = {2017},
	note = {Publisher: Springer},
	pages = {79--89},
	file = {Oh et al_2017_Online recognition of handwritten music symbols.pdf:/home/ptorras/zotpapers/Oh et al_2017_Online recognition of handwritten music symbols.pdf:application/pdf},
}

@misc{pacha_definitive_2019,
	title = {The definitive bibliography for research on {Optical} {Music} {Recognition}},
	url = {https://omr-research.github.io},
	author = {Pacha, Alexander},
	year = {2019},
}

@misc{pacha_omr_2017,
	title = {The {OMR} {Datasets} {Project}},
	url = {https://apacha.github.io/OMR-Datasets},
	author = {Pacha, Alexander},
	year = {2017},
}

@misc{pacha_documentation_2018,
	title = {Documentation of the {OMR} {Dataset} {Tools} {Python} package},
	url = {https://omr-datasets.readthedocs.io/en/latest},
	author = {Pacha, Alexander},
	year = {2018},
}

@misc{calvo-zaragoza_recording_2018,
	title = {The recording of the {ISMIR} {Tutorial} "{OMR} for {Dummies}" on {YouTube}},
	url = {https://www.youtube.com/playlist?list=PL1jvwDVNwQke-04UxzlzY4FM33bo1CGS0},
	author = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander and Fujinaga, Ichiro},
	year = {2018},
}

@inproceedings{pacha_self-learning_2018,
	title = {Self-learning {Optical} {Music} {Recognition}},
	isbn = {978-3-9504017-8-3},
	url = {http://vss.tuwien.ac.at/},
	booktitle = {Vienna {Young} {Scientists} {Symposium}},
	publisher = {Book-of-Abstracts.com, Heinz A. Krebs},
	author = {Pacha, Alexander},
	editor = {Hans, Philipp and Artner, Gerald and Grames, Johanna and Krebs, Heinz and Khosravi, Hamid Reza Mansouri and Rouhi, Taraneh},
	year = {2018},
	note = {Backup Publisher: TU Wien},
	pages = {34--35},
}

@inproceedings{pacha_optical_2018,
	address = {Paris, France},
	title = {Optical {Music} {Recognition} in {Mensural} {Notation} with {Region}-{Based} {Convolutional} {Neural} {Networks}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/32_Paper.pdf},
	abstract = {In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-toend fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76\% for symbol detection, and over 98\% accuracy for predicting the position.},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pacha, Alexander and Calvo-Zaragoza, Jorge},
	year = {2018},
	pages = {240--247},
	file = {Pacha and Calvo-Zaragoza - 2018 - OPTICAL MUSIC RECOGNITION IN MENSURAL NOTATION WIT.pdf:/home/ptorras/Zotero/storage/UTMH37XQ/Pacha and Calvo-Zaragoza - 2018 - OPTICAL MUSIC RECOGNITION IN MENSURAL NOTATION WIT.pdf:application/pdf},
}

@article{pacha_baseline_2018,
	title = {A {Baseline} for {General} {Music} {Object} {Detection} with {Deep} {Learning}},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/9/1488},
	doi = {10.3390/app8091488},
	abstract = {Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.},
	number = {9},
	journal = {Applied Sciences},
	author = {Pacha, Alexander and Hajič jr., Jan and Calvo-Zaragoza, Jorge},
	year = {2018},
	keywords = {deep learning, optical music recognition, object detection, music scores},
	pages = {1488--1508},
	file = {Pacha et al_2018_A Baseline for General Music Object Detection with Deep Learning.pdf:/home/ptorras/zotpapers/Pacha et al_2018_A Baseline for General Music Object Detection with Deep Learning.pdf:application/pdf},
}

@inproceedings{pacha_advancing_2018,
	address = {Paris, France},
	title = {Advancing {OMR} as a {Community}: {Best} {Practices} for {Reproducible} {Research}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Pacha, Alexander},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {19--20},
}

@phdthesis{pacha_self-learning_2019,
	type = {phdthesis},
	title = {Self-{Learning} {Optical} {Music} {Recognition}},
	url = {https://alexanderpacha.files.wordpress.com/2019/07/dissertation-self-learning-optical-music-recognition-alexander-pacha.pdf},
	abstract = {Music is an essential part of our culture and heritage. Throughout the centuries, millions of songs were composed and written down in documents using music notation. Optical Music Recognition (OMR) is the research field that investigates how the computer can learn to read those documents. Despite decades of research, OMR is still considered far from being solved. One reason is that traditional approaches rely heavily on heuristics and often do not generalize well. In this thesis, I propose a different approach to let the computer learn to read music notation documents mostly by itself using machine learning, especially deep learning. In several experiments, I have demonstrated that the computer can learn to robustly solve many tasks involved in OMR by using supervised learning. These include the structural analysis of the document, the detection and classification of symbols in the scores as well as the construction of the music notation graph, which is an intermediate representation that can be exported into a format suitable for further processing. A trained deep convolutional neural network can reliably detect whether an image contains music or not, while another one is capable of finding and linking individual measures across multiple sources for easy navigation between them. Detecting symbols in typeset and handwritten scores can be learned, given a sufficient amount of annotated data, and classifying isolated symbols can be performed at even lower error rates than those of humans. For scores written in mensural notation the complete recognition can even be simplified into just three steps, two of which can be solved with machine learning. Apart from publishing a number of scientific articles, I have gathered and documented the most extensive collection of datasets for OMR as well as the probably most comprehensive bibliography currently available. Both are available online. Moreover I was involved in the organization of the International Workshop on Reading Music Systems, in a joint tutorial at the International Society For Music Information Retrieval Conference on OMR as well as in another workshop at the Music Encoding Conference. Many challenges of OMR can be solved efficiently with deep learning, such as the layout analysis or music object detection. As music notation is a configurational writing system where the relations and interplay between symbols determine the musical semantic, these relationships have to be recognized as well. A music notation graph is a suitable representation for storing this information. It allows to clearly distinguish between the challenges involved in recovering information from the music score image and the encoding of the recovered information into a specific output format while complying with the rules of music notation. While the construction of such a graph can be learned as well, there are still many open issues that need future research. But I am confident that training the computer on a sufficiently large dataset under human supervision is a sustainable approach that will help to solve many applications of OMR in the future.},
	school = {TU Wien},
	author = {Pacha, Alexander},
	year = {2019},
	file = {Pacha_2019_Self-Learning Optical Music Recognition.pdf:/home/ptorras/zotpapers/Pacha_2019_Self-Learning Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{pacha_incremental_2019,
	address = {Delft, The Netherlands},
	title = {Incremental {Supervised} {Staff} {Detection}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Pacha, Alexander},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {16--20},
}

@inproceedings{pacha_challenge_2021,
	address = {Alicante, Spain},
	title = {The {Challenge} of {Reconstructing} {Digits} in {Music} {Scores}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Pacha, Alexander},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {4--7},
}

@inproceedings{padilla_improving_2014,
	address = {London, United Kingdom},
	title = {Improving {OMR} for {Digital} {Music} {Libraries} with {Multiple} {Recognisers} and {Multiple} {Sources}},
	isbn = {978-1-4503-3002-2},
	doi = {10.1145/2660168.2660175},
	abstract = {Large quantities of scanned music are now available in public digital music libraries. However, the information in such sources is represented as pixel data in images rather than symbolic information about the notes of a piece of music, and therefore it is opaque to musically meaningful computational processes (e.g., to search for a particular melodic pattern). Optical Music Recognition (Optical Character Recognition for music) holds out the prospect of a solution to this issue and allowing access to very large quantities of musical information in digital libraries. Despite the efforts made by the different commercial OMR developers to improve the accuracy of their systems, mistakes in the output are currently too frequent to make OMR a practical tool for bulk processing. One possibility for improving the accuracy of OMR is to use multiple recognisers and combine the results to achieve an output better than each of them individually. The general process presented here can be divided into three subtasks, S1, S2, and S3. S1 is focused in the correction of rhythmical errors at bar level, counting the errors of the different OMR outputs, establish a ranking of the results, and make a pairwise alignment to select the best measures. S2 is based on the alignment and voting of individual symbols. For this task we have implemented a conversion of the most important symbols to a simple grammar. Finally, S3 improves the output of S2 by comparing and adding symbols from S1 and detecting gaps through the alignment of wrong measures. The process described in this paper is part of our "Big Data Approach" where a large amount of data is available in music score libraries, such as the International Music Score Library Project (IMSLP), for the purpose of Music Information Retrieval (MIR).},
	booktitle = {1st {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Padilla, Victor and Marsden, Alan and McLean, Alex and Ng, Kia},
	year = {2014},
	keywords = {Optical Music Recognition, Image processing, Pattern Recognition, Big Data, Library},
	pages = {1--8},
}

@inproceedings{paakkonen_digitisation_2018,
	address = {Paris, France},
	title = {Digitisation and {Digital} {Library} {Presentation} {System} – {Sheet} {Music} to the {Mix}},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Pääkkönen, Tuula and Kervinen, Jukka and Kettunen, Kimmo},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {21--22},
}

@techreport{panadero_alignment_2019,
	title = {Alignment of handwritten music scores},
	url = {https://ddd.uab.cat/pub/tfg/2019/tfg_151917/TFG_-_Informe_Final.pdf},
	abstract = {There are musicologists that spend their time in analyzing musical pieces of more than a century ago in order to link them to another pre-existing pieces from the same author but written by different hands. It is a tedious task, since there are many representations done of a single piece through the time, and the writing variability among those representations can be extensive. The purpose would be in having a varied database of these old compositions for the study, reproduction and difusion. This work is divided into two phases. The first one, constitent in the detection of primitive present elements in each of the measures of a score using the existing transcription of the piece, thus obtaining the desired guided alignment. The second one will seek to analyze this alignment. Obtained results are encouraging.},
	institution = {Universitat Autónoma de Barcelona},
	author = {Panadero, Ivan Santos},
	year = {2019},
	file = {Panadero_2019_Alignment of handwritten music scores.pdf:/home/ptorras/zotpapers/Panadero_2019_Alignment of handwritten music scores.pdf:application/pdf},
}

@inproceedings{parada-cabaleiro_seils_2017,
	address = {Suzhou, China},
	title = {The {SEILS} {Dataset}: {Symbolically} {Encoded} {Scores} in {Modern}-{Early} {Notation} for {Computational} {Musicology}},
	isbn = {978-981-11-5179-8},
	url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/14_Paper.pdf},
	booktitle = {18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Parada-Cabaleiro, Emilia and Batliner, Anton and Baird, Alice and Schuller, Björn},
	year = {2017},
}

@inproceedings{parada-cabaleiro_diplomatic_2019,
	address = {Delft, The Netherlands},
	title = {A {Diplomatic} {Edition} of {Il} {Lauro} {Secco}: {Ground} {Truth} for {OMR} of {White} {Mensural} {Notation}},
	url = {http://archives.ismir.net/ismir2019/paper/000067.pdf},
	abstract = {Early musical sources in white mensural notation—the most common notation in European printed music during the Renaissance—are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of imagebased musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in **mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the **mens files.},
	booktitle = {20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Parada-Cabaleiro, Emilia and Batliner, Anton and Schuller, Björn},
	year = {2019},
	pages = {557--564},
	file = {Parada-Cabaleiro et al_2019_A Diplomatic Edition of Il Lauro Secco.pdf:/home/ptorras/zotpapers/Parada-Cabaleiro et al_2019_A Diplomatic Edition of Il Lauro Secco.pdf:application/pdf},
}

@article{pedersoli_document_2016,
	title = {Document segmentation and classification into musical scores and text},
	volume = {19},
	issn = {1433-2825},
	doi = {10.1007/s10032-016-0271-5},
	abstract = {A new algorithm for segmenting documents into regions containing musical scores and text is proposed. Such segmentation is a required step prior to applying optical character recognition and optical music recognition on scanned pages that contain both music notation and text. Our segmentation technique is based on the bag-of-visual-words representation followed by random block voting (RBV) in order to detect the bounding boxes containing the musical score and text within a document image. The RBV procedure consists of extracting a fixed number of blocks whose position and size are sampled from a discrete uniform distribution that “over”-covers the input image. Each block is automatically classified as either coming from musical score or text and votes with a particular posterior probability of classification in its spatial domain. An initial coarse segmentation is obtained by summarizing all the votes in a single image. Subsequently, the final segmentation is obtained by subdividing the image in microblocks and classifying them using a N-nearest neighbor classifier which is trained using the coarse segmentation. We demonstrate the potential of the proposed method by experiments on two different datasets. One is on a challenging dataset of images collected and artificially combined and manipulated for this project. The other is a music dataset obtained by the scanning of two music books. The results are reported using precision/recall metrics of the overlapping area with respect to the ground truth. The proposed system achieves an overall averaged F-measure of 85 \%. The complete source code package and associated data are available at https://github.com/fpeder/mscr under the FreeBSD license to support reproducibility.},
	number = {4},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Pedersoli, Fabrizio and Tzanetakis, George},
	year = {2016},
	pages = {289--304},
}

@inproceedings{penarrubia_efficient_2022,
	address = {Online},
	title = {Efficient {Approaches} for {Notation} {Assembly} in {Optical} {Music} {Recognition}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Penarrubia, Carlos and Garrido-Muñoz, Carlos and Valero-Mas, Jose J. and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {29--32},
	file = {Penarrubia et al_2022_Efficient Approaches for Notation Assembly in Optical Music Recognition.pdf:/home/ptorras/zotpapers/Penarrubia et al_2022_Efficient Approaches for Notation Assembly in Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{pham_virtual_2015,
	title = {Virtual {Music} {Teacher} for {New} {Music} {Learners} with {Optical} {Music} {Recognition}},
	doi = {10.1007/978-3-319-20609-7_39},
	booktitle = {International {Conference} on {Learning} and {Collaboration} {Technologies}},
	publisher = {Springer},
	author = {Pham, Viet-Khoi and Nguyen, Hai-Dang and Tran, Minh-Triet},
	year = {2015},
	keywords = {others},
	pages = {415--426},
	file = {Pham et al_2015_Virtual Music Teacher for New Music Learners with Optical Music Recognition.pdf:/home/ptorras/zotpapers/Pham et al_2015_Virtual Music Teacher for New Music Learners with Optical Music Recognition.pdf:application/pdf},
}

@article{pham_music_2015,
	title = {Music {Score} {Recognition} {Based} on a {Collaborative} {Model}},
	volume = {10},
	issn = {1975-0080},
	doi = {10.14257/ijmue.2015.10.8.37},
	abstract = {Recognition musical symbols are very important in music score system and they depend on these methods of researchers. Most of existing approaches for OMR (optical music recognition) removes staff lines before symbols are detected, therefore the symbols can get damaged easily. Another method recognizes symbols without staff line removal but all of them have a low accuracy rate and high processing time for recognizing symbols. In this paper, none staff removal and staff removal are suggested and these new methods are proposed to improve appreciation result of symbols. A lot of symbols are detected before deleted staff line as vertical lines, note head, pitch, beam, tail and then these staff lines are removed to identify other symbols using connected component. The proposed method is applied to the Samsung smart phone which embeds a high resolution camera. Experimental results show that the recognition rate is higher than existing methods and the computation time is reduced significantly.},
	number = {8},
	journal = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Pham, Van Khien and Lee, Guee-Sang},
	year = {2015},
	note = {Publisher: Science and Engineering Research Support Society},
	keywords = {Music scores, Line removal, Line detection, Note head, Projection, Smartphones, Template matching},
	pages = {379--390},
	file = {Pham_Lee_2015_Music Score Recognition Based on a Collaborative Model.pdf:/home/ptorras/zotpapers/Pham_Lee_2015_Music Score Recognition Based on a Collaborative Model.pdf:application/pdf},
}

@inproceedings{pham_apply_2015,
	title = {Apply lightweight recognition algorithms in optical music recognition},
	isbn = {978-1-62841-560-5},
	doi = {10.1117/12.2180715},
	abstract = {The problems of digitalization and transformation of musical scores into machine-readable format are necessary to be solved since they help people to enjoy music, to learn music, to conserve music sheets, and even to assist music composers. However, the results of existing methods still require improvements for higher accuracy. Therefore, the authors propose lightweight algorithms for Optical Music Recognition to help people to recognize and automatically play musical scores. In our proposal, after removing staff lines and extracting symbols, each music symbol is represented as a grid of identical M â- N cells, and the features are extracted and classified with multiple lightweight SVM classifiers. Through experiments, the authors find that the size of 10 â- 12 cells yields the highest precision value. Experimental results on the dataset consisting of 4929 music symbols taken from 18 modern music sheets in the Synthetic Score Database show that our proposed method is able to classify printed musical scores with accuracy up to 99.56\%.},
	booktitle = {7th {International} {Conference} on {Machine} {Vision}},
	publisher = {SPIE},
	author = {Pham, Viet-Khoi and Nguyen, Hai-Dang and Nguyen-Khac, Tung-Anh and Tran, Minh-Triet},
	year = {2015},
	note = {ISSN: 0277-786X},
	keywords = {Optical music recognition, Support vector machines, Computer vision, Algorithms, Classification (of information), Musical score, Machine-readable format, Music composers, Recognition algorithm, Stable Paths approach, SVM classifiers},
}

@misc{neuratron_photoscore_2018,
	title = {{PhotoScore} 2018},
	url = {http://www.neuratron.com/photoscore.htm},
	author = {{Neuratron}},
	year = {2018},
}

@inproceedings{pinheiro_pereira_deep_2016,
	address = {Teresina, Piau; Brazil},
	title = {A {Deep} {Approach} for {Handwritten} {Musical} {Symbols} {Recognition}},
	isbn = {978-1-4503-4512-5},
	doi = {10.1145/2976796.2988171},
	booktitle = {22nd {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	publisher = {ACM},
	author = {Pinheiro Pereira, Roberto M. and Matos, Caio E.F. and Braz, Geraldo Jr. and de Almeida, João D.S. and de Paiva, Anselmo C.},
	year = {2016},
	keywords = {deep learning, convolutional neural network, document analyses, optical musical recognition},
	pages = {191--194},
}

@inproceedings{pinto_ancient_2000,
	address = {Berlin, Heidelberg},
	title = {Ancient {Music} {Recovery} for {Digital} {Libraries}},
	isbn = {978-3-540-45268-3},
	doi = {10.1007/3-540-45268-0_3},
	abstract = {The purpose of this paper is to present a description and current state of the “ROMA” (Reconhecimento Óptico de Música Antiga or Ancient Music Optical Recognition) Project that consists on building an application, for the recognition and restoration specialised in ancient music manuscripts (from XVI to XVIII century). This project, beyond the inventory of the Biblioteca Geral da Universidade de Coimbra musical funds aims to develop algorithms for scores restoration and musical symbols recognition in order to allow a suitable representation and restoration on digital format. Both objectives have an intrinsic research nature one in the area of musicology and other in digital libraries.},
	booktitle = {Research and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer Berlin Heidelberg},
	author = {Pinto, João Caldas and Vieira, Pedro and Ramalho, M. and Mengucci, M. and Pina, P. and Muge, F.},
	editor = {Borbinha, José and Baker, Thomas},
	year = {2000},
	pages = {24--34},
}

@article{pinto_new_2003,
	title = {A new graph-like classification method applied to ancient handwritten musical symbols},
	volume = {6},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-003-0102-3},
	doi = {10.1007/s10032-003-0102-3},
	abstract = {Several algorithms have been proposed in the past to solve the problem of binary pattern recognition. The problem of finding features that clearly distinguish two or more different patterns is a key issue in the design of such algorithms. In this paper, a graph-like recognition process is proposed that combines a number of different classifiers to simplify the type of features and classifiers used in each classification step. The graph-like classification method is applied to ancient music optical recogniti on, and a high degree of accuracy has been achieved.},
	number = {1},
	journal = {Document Analysis and Recognition},
	author = {Pinto, João Caldas and Vieira, Pedro and Sousa, João M.},
	year = {2003},
	pages = {10--22},
}

@techreport{pinto_content_2010,
	title = {Content {Aware} {Music} {Score} {Binarization}},
	url = {http://www.inescporto.pt/~jsc/publications/conferences/2010TPintoACCV.pdf},
	institution = {INESC Porto},
	author = {Pinto, Telmo and Rebelo, Ana and Giraldi, Gilson and Cardoso, Jamie dos Santos},
	year = {2010},
	note = {Backup Publisher: Universidade do Porto, Portugal},
	keywords = {binarization},
}

@inproceedings{pinto_music_2011,
	title = {Music {Score} {Binarization} {Based} on {Domain} {Knowledge}},
	isbn = {978-3-642-21257-4},
	doi = {10.1007/978-3-642-21257-4_87},
	abstract = {Image binarization is a common operation in the pre- processing stage in most Optical Music Recognition (OMR) systems. The choice of an appropriate binarization method for handwritten music scores is a difficult problem. Several works have already evaluated the performance of existing binarization processes in diverse applications. However, no goal-directed studies for music sheets documents were carried out. This paper presents a novel binarization method based in the content knowledge of the image. The method only needs the estimation of the staffline thickness and the vertical distance between two stafflines. This information is extracted directly from the gray level music score. The proposed binarization procedure is experimentally compared with several state of the art methods.},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Pinto, Telmo and Rebelo, Ana and Giraldi, Gilson and Cardoso, Jamie dos Santos},
	editor = {Vitrià, Jordi and Sanches, João Miguel and Hernández, Mario},
	year = {2011},
	keywords = {Optical Music Recognition, Computer Vision, Image Processing},
	pages = {700--708},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/2EM7QHB9/Pinto et al. - 2011 - Music Score Binarization Based on Domain Knowledge.pdf:application/pdf},
}

@misc{organum_playscore_2016,
	title = {{PlayScore}},
	url = {http://www.playscore.co},
	author = {{Organum}},
	year = {2016},
}

@inproceedings{poulain_dandecy_kalman_1994,
	title = {Kalman filtering for segment detection: application to music scores analysis},
	doi = {10.1109/icpr.1994.576283},
	booktitle = {12th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Poulain d'Andecy, Vincent and Camillerapp, Jean and Leplumey, Ivan},
	year = {1994},
}

@inproceedings{poulain_dandecy_detecteur_1994,
	title = {Détecteur robuste de segments; {Application} à l'analyse de partitions musicales},
	booktitle = {Actes 9 ème {Congrés} {AFCET} {Reconnaissance} des {Formes} et {Intelligence} {Artificielle}},
	author = {Poulain d'Andecy, Vincent and Camillerapp, Jean and Leplumey, Ivan},
	year = {1994},
}

@article{poulain_dandecy_analyse_1995,
	title = {Analyse de {Partitions} {Musicales}},
	volume = {12},
	url = {http://hdl.handle.net/2042/1939},
	language = {French},
	number = {6},
	journal = {Traitement du Signal},
	author = {Poulain d'Andecy, Vincent and Camillerapp, Jean and Leplumey, Ivan},
	year = {1995},
	pages = {653--661},
}

@phdthesis{prerau_computer_1970,
	type = {{PhD} {Thesis}},
	title = {Computer pattern recognition of standard engraved music notation},
	school = {Massachusetts Institute of Technology},
	author = {Prerau, David S.},
	year = {1970},
}

@inproceedings{prerau_computer_1971,
	title = {Computer pattern recognition of printed music},
	abstract = {The standard notation used to specify most instrumental and vocal music forms a conventionalized, two-dimensional, visual pattern class. This paper discusses computer recognition of the music information specified by a sample of this standard notation. A sample of printed music notation is scanned optically, and a digitized version of the music sample is fed into the computer. The digitized sample may be considered the data-set sensed by the computer. The computer performs the recognition and then produces an output in the Ford-Columbia music representation. Ford-Columbia is an alphanumeric language isomorphic to standard music notation It is therefore capable of representing the music information specified by the original sample},
	booktitle = {Fall {Joint} {Computer} {Conference}},
	author = {Prerau, David S.},
	year = {1971},
	keywords = {to classify, printed music, computer pattern recognition, Ford},
	pages = {153--162},
}

@phdthesis{pruslin_automatic_1966,
	address = {Cambridge, Massachusetts, USA},
	type = {{PhD} {Thesis}},
	title = {Automatic {Recognition} of {Sheet} {Music}},
	school = {Massachusetts Institute of Technology},
	author = {Pruslin, Dennis Howard},
	year = {1966},
}

@techreport{pugin_realisation_2001,
	address = {Geneva, Switzerland},
	title = {Réalisation d'un système de superposition de partitions de musique anciennes},
	url = {http://www.unige.ch/lettres/armus/music/devrech/aruspix/pdf/licence.pdf},
	institution = {Geneva University},
	author = {Pugin, Laurent},
	year = {2001},
}

@inproceedings{pugin_optical_2006,
	address = {Victoria, Canada},
	title = {Optical {Music} {Recognitoin} of {Early} {Typographic} {Prints} using {Hidden} {Markov} {Models}},
	url = {http://ismir2006.ismir.net/PAPERS/ISMIR06152_Paper.pdf},
	booktitle = {7th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Pugin, Laurent},
	year = {2006},
	pages = {53--56},
	file = {Pugin_2006_Optical Music Recognitoin of Early Typographic Prints using Hidden Markov Models.pdf:/home/ptorras/zotpapers/Pugin_2006_Optical Music Recognitoin of Early Typographic Prints using Hidden Markov Models.pdf:application/pdf},
}

@article{pugin_aruspix_2006,
	title = {Aruspix: an {Automatic} {Source}-{Comparison} {System}},
	volume = {14},
	issn = {1057-9478},
	url = {https://dialnet.unirioja.es/servlet/articulo?codigo=3476563},
	journal = {Computing in Musicology},
	author = {Pugin, Laurent},
	year = {2006},
	note = {Place: Cambridge, MA},
	pages = {49--59},
}

@phdthesis{pugin_lecture_2006,
	address = {Geneva, Switzerland},
	type = {{PhD} {Thesis}},
	title = {Lecture et traitement informatique de typographies musicales anciennes: un logiciel de reconnaissance de partitions par modèles de {Markov} cachés},
	school = {Geneva University},
	author = {Pugin, Laurent},
	year = {2006},
	doi = {10.13097/archive-ouverte/unige:30024},
	doi = {10.13097/archive-ouverte/unige:30024},
}

@inproceedings{pugin_goal-directed_2007,
	address = {Vancouver, Canada},
	title = {Goal-directed {Evaluation} for the {Improvement} of {Optical} {Music} {Recognition} on {Early} {Music} {Prints}},
	isbn = {978-1-59593-644-8},
	doi = {10.1145/1255175.1255233},
	booktitle = {7th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {ACM},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Fujinaga, Ichiro},
	year = {2007},
	keywords = {optical music recognition, adaptive binarization, goal-directed evaluation, early music, test-driven development},
	pages = {303--304},
}

@inproceedings{pugin_map_2007,
	title = {{MAP} {Adaptation} to {Improve} {Optical} {Music} {Recognition} of {Early} {Music} {Documents} {Using} {Hidden} {Markov} {Models}},
	url = {http://ismir2007.ismir.net/proceedings/ISMIR2007_p513_pugin.pdf},
	booktitle = {8th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Fujinaga, Ichiro},
	year = {2007},
	pages = {513--516},
	file = {Pugin et al_2007_MAP Adaptation to Improve Optical Music Recognition of Early Music Documents.pdf:/home/ptorras/zotpapers/Pugin et al_2007_MAP Adaptation to Improve Optical Music Recognition of Early Music Documents.pdf:application/pdf},
}

@inproceedings{pugin_reducing_2007,
	address = {Berlin, Heidelberg},
	title = {Reducing {Costs} for {Digitising} {Early} {Music} with {Dynamic} {Adaptation}},
	isbn = {978-3-540-74851-9},
	abstract = {Optical music recognition (OMR) enables librarians to digitise early music sources on a large scale. The cost of expert human labour to correct automatic recognition errors dominates the cost of such projects. To reduce the number of recognition errors in the OMR process, we present an innovative approach to adapt the system dynamically, taking advantage of the human editing work that is part of any digitisation project. The corrected data are used to perform MAP adaptation, a machine-learning technique used previously in speech recognition and optical character recognition (OCR). Our experiments show that this technique can reduce editing costs by more than half.},
	booktitle = {Research and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer Berlin Heidelberg},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Fujinaga, Ichiro},
	editor = {Kovács, László and Fuhr, Norbert and Meghini, Carlo},
	year = {2007},
	pages = {471--474},
}

@techreport{pugin_book-adaptive_2007,
	address = {Whistler, BC},
	title = {Book-{Adaptive} and {Book}-{Dependent} {Models} to {Accelerate} {Digitization} of {Early} {Music}},
	url = {https://www.researchgate.net/publication/255604238_Book-Adaptive_and_Book-Dependent_Models_to_Accelerate_Digitization_of_Early_Music},
	institution = {McGill University},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Eck, Douglas and Fujinaga, Ichiro},
	year = {2007},
	pages = {1--8},
}

@inproceedings{pugin_gamera_2008,
	title = {Gamera versus {Aruspix} – {Two} {Optical} {Music} {Recognition} {Approaches}},
	url = {http://ismir2008.ismir.net/papers/ISMIR2008_247.pdf},
	booktitle = {9th {International} {Conference} on {Music} {Information} {Retrieval}},
	author = {Pugin, Laurent and Hockman, Jason and Burgoyne, John Ashley and Fujinaga, Ichiro},
	year = {2008},
	file = {Pugin et al_2008_Gamera versus Aruspix – Two Optical Music Recognition Approaches.pdf:/home/ptorras/zotpapers/Pugin et al_2008_Gamera versus Aruspix – Two Optical Music Recognition Approaches.pdf:application/pdf},
}

@inproceedings{pugin_evaluating_2013,
	address = {Curitiba, Brazil},
	title = {Evaluating {OMR} on the {Early} {Music} {Online} {Collection}},
	url = {http://ismir2013.ismir.net/wp-content/uploads/2013/09/65_Paper.pdf},
	booktitle = {14th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pugin, Laurent and Crawford, Tim},
	editor = {Jr, Alceu de Souza Britto and Gouyon, Fabien and Dixon, Simon},
	year = {2013},
	pages = {439--444},
	file = {Pugin_Crawford_2013_Evaluating OMR on the Early Music Online Collection.pdf:/home/ptorras/zotpapers/Pugin_Crawford_2013_Evaluating OMR on the Early Music Online Collection.pdf:application/pdf},
}

@article{ramirez_automatic_2014,
	title = {Automatic {Recognition} of {Square} {Notation} {Symbols} in {Western} {Plainchant} {Manuscripts}},
	volume = {43},
	issn = {0929-8215},
	doi = {10.1080/09298215.2014.931438},
	abstract = {Abstract: While the Optical Music Recognition (OMR) of printed and handwritten music scores in modern standard notation has been broadly studied, this is not the case for early music manuscripts. This is mainly due to the high variability in the sources introduced by their severe physical degradation, the lack of notation standards and, in the case of the scanned versions, by non-homogenous image-acquisition protocols. The volume of early musical manuscripts available is considerable, and therefore we believe that computational methods can be extremely useful in helping to preserve, share and analyse this information. This paper presents an approach to recognizing handwritten square musical notation in degraded western plainchant manuscripts from the XIVth to XVIth centuries. We propose the use of image processing techniques that behave robustly under high data variability and which do not require strong hypotheses regarding the condition of the sources. The main differences from traditional OMR approaches are our avoidance of the staff line removal stage and the use of grey-level images to perform primitive segmentation and feature extraction. We used 136 images from the Digital Scriptorium repository (DS, 2007), from which we were able to extract over 90\% of the staves and over 88\% of all symbols present. For symbol classification, we used gradient-based features and SVM classifiers, obtaining over 90\% precision and recall over eight basic symbol classes.},
	number = {4},
	journal = {Journal of New Music Research},
	author = {Ramirez, Carolina and Ohya, Jun},
	year = {2014},
	note = {Publisher: Taylor and Francis Ltd.},
	pages = {390--399},
}

@inproceedings{randriamahefa_printed_1993,
	title = {Printed music recognition},
	doi = {10.1109/ICDAR.1993.395592},
	abstract = {The different steps to recognize printed music are described. The first step is to detect and to eliminate the staff lines. A robust method based on finding regions where are only the staff lines, linking between them the staff lines pieces in these regions is used. After staff lines elimination, symbols are isolated and a representation called attributed graph is constructed for each symbol. Thinning, polygonalization, spurious segments cleaning, and segment fusion are performed. A first classification, separating all notes with black heads from others, is performed. To recognize notes with black heads (beamed group or quarter notes), a straightforward structural approach using this representation is sufficient and efficient in most cases. In the ambiguous cases (chord or black head linked to two stems), an ellipse matching method is used. To recognize half notes and bar lines, a structural method using the graph is used.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {2nd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Randriamahefa, R. and Cocquerez, J. P. and Fluhr, C. and Pepin, F. and Philipp, S.},
	year = {1993},
	keywords = {Image segmentation, Robustness, Multiple signal classification, System testing, Head, Automatic speech recognition, Speech recognition, music, image recognition, Image resolution, symbols, staff lines, Joining processes, printed music recognition, attributed graph, bar lines, beamed group, Cleaning, ellipse matching method, graph, half notes, polygonalization, quarter notes, robust method, segment fusion, spurious segments cleaning},
	pages = {898--901},
}

@techreport{raphael_optical_2011,
	title = {Optical {Music} {Recognition} on the {IMSLP}},
	institution = {Indiana University, Bloomington},
	author = {Raphael, Christopher},
	year = {2011},
}

@inproceedings{raphael_new_2011,
	address = {Miami, Florida},
	title = {New {Approaches} to {Optical} {Music} {Recognition}},
	url = {http://ismir2011.ismir.net/papers/OS3-3.pdf},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {University of Miami},
	author = {Raphael, Christopher and Wang, Jingya},
	editor = {Klapuri, Anssi and Leider, Colby},
	year = {2011},
	pages = {305--310},
	file = {Raphael_Wang_2011_New Approaches to Optical Music Recognition.pdf:/home/ptorras/zotpapers/Raphael_Wang_2011_New Approaches to Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{raphael_optical_2013,
	title = {Optical music recognition on the international music score library project},
	doi = {10.1117/12.2040247},
	booktitle = {{IS}\&{T}/{SPIE} {Electronic} {Imaging}},
	publisher = {International Society for Optics and Photonics},
	author = {Raphael, Christopher and Jin, Rong},
	year = {2013},
	file = {Raphael_Jin_2013_Optical music recognition on the international music score library project.pdf:/home/ptorras/zotpapers/Raphael_Jin_2013_Optical music recognition on the international music score library project.pdf:application/pdf},
}

@inproceedings{rebelo_shortest_2007,
	title = {A {Shortest} {Path} {Approach} for {Staff} {Line} {Detection}},
	doi = {10.1109/AXMEDIS.2007.16},
	abstract = {Many music works produced in the past still exist only as original manuscripts or as photocopies. Preserving them entails their digitalization and consequent accessibility in a digital format easy-to-manage. The manual process to carry out this task is very time consuming and error prone. Optical music recognition (OMR) is a form of structured document image analysis where music symbols are isolated and identified so that the music can be conveniently processed. While OMR systems perform well on printed scores, current methods for reading handwritten musical scores by computers remain far from ideal. One of the fundamental stages of this process is the staff line detection. In this paper a new method for the automatic detection of music stave lines based on a shortest path approach is presented. Lines with some curvature, discontinuities, and inclination are robustly detected. The proposed algorithm behaves favourably when compared experimentally with well-established algorithms.},
	booktitle = {3rd {International} {Conference} on {Automated} {Production} of {Cross} {Media} {Content} for {Multi}-{Channel} {Distribution}},
	author = {Rebelo, Ana and Capela, Artur and Pinto da Costa, Joaquim F. and Guedes, Carlos and Carrapatoso, Eurico and Cardoso, Jamie dos Santos},
	year = {2007},
	keywords = {Handwriting recognition, Text analysis, Image recognition, Robustness, optical music recognition, Ordinary magnetoresistance, optical character recognition, document image processing, graph theory, music, Music information retrieval, Image analysis, Computer errors, Cultural differences, staff line detection, automatic detection, music stave lines, music symbol, Production, shortest path approach, structured document image analysis},
	pages = {79--85},
}

@mastersthesis{rebelo_new_2008,
	title = {New {Methodologies} {Towards} an {Automatic} {Optical} {Recognition} of {Handwritten} {Musical} {Scores}},
	url = {http://www.inescporto.pt/~arebelo/education/2008MScThesisDefenseAnaRebelo.pdf},
	school = {Universidade do Porto},
	author = {Rebelo, Ana},
	year = {2008},
}

@inproceedings{rebelo_metric_2011,
	title = {Metric {Learning} for {Music} {Symbol} {Recognition}},
	doi = {10.1109/ICMLA.2011.94},
	abstract = {Although Optical Music Recognition (OMR) has been the focus of much research for decades, the processing of handwritten musical scores is not yet satisfactory. The efforts made to find robust symbol representations and learning methodologies have not found a similar quality in the learning of the dissimilarity concept. Simple Euclidean distances are often used to measure dissimilarity between different examples. However, such distances do not necessarily yield the best performance. In this paper, we propose to learn the best distance for the k-nearest neighbor (k-NN) classifier. The distance concept will be tuned both for the application domain and the adopted representation for the music symbols. The performance of the method is compared with the support vector machine (SVM) classifier using both real and synthetic music scores. The synthetic database includes four types of deformations inducing variability in the printed musical symbols which exist in handwritten music sheets. The work presented here can open new research paths towards a novel automatic musical symbols recognition module for handwritten scores.},
	booktitle = {10th {International} {Conference} on {Machine} {Learning} and {Applications} and {Workshops}},
	author = {Rebelo, Ana and Tkaczuk, Jakub and Sousa, Sousa and Cardoso, Jamie dos Santos},
	year = {2011},
	keywords = {Training, Feature extraction, Machine learning, optical music recognition, Measurement, Support vector machines, music, Vectors, learning (artificial intelligence), Kernel, printed musical symbols, automatic musical symbols recognition module, database management systems, dissimilarity concept, handwritten music sheets, k-nearest neighbor classifier, metric learning, real music scores, robust symbol representations, support vector machine, support vector machines, SVM classifier, synthetic database, synthetic music scores},
	pages = {106--111},
}

@inproceedings{rebelo_method_2011,
	title = {A {Method} for {Music} {Symbols} {Extraction} based on {Musical} {Rules}},
	isbn = {0-9846042-6-X},
	url = {http://www.inescporto.pt/~jsc/publications/conferences/2011ARebeloBRIDGES.pdf},
	booktitle = {Bridges 2011: {Mathematics}, {Music}, {Art}, {Architecture}, {Culture}},
	author = {Rebelo, Ana and Paszkiewicz, Filipe and Guedes, Carlos and Marcal, Andre R. S. and Cardoso, Jamie dos Santos},
	year = {2011},
	pages = {81--88},
}

@phdthesis{rebelo_robust_2012,
	type = {{PhD} {Thesis}},
	title = {Robust {Optical} {Recognition} of {Handwritten} {Musical} {Scores} based on {Domain} {Knowledge}},
	url = {http://www.inescporto.pt/~arebelo/arebeloThesis.pdf},
	school = {University of Porto},
	author = {Rebelo, Ana},
	year = {2012},
	note = {Backup Publisher: INESC Porto},
}

@inproceedings{rebelo_global_2013,
	title = {Global constraints for syntactic consistency in {OMR}: an ongoing approach},
	url = {http://www.inescporto.pt/~jsc/publications/conferences/2013ARebeloICIAR.pdf},
	booktitle = {International {Conference} on {Image} {Analysis} and {Recognition}},
	author = {Rebelo, Ana and Marçal, André and Cardoso, Jamie dos Santos},
	year = {2013},
}

@inproceedings{rebelo_staff_2013,
	title = {Staff {Line} {Detection} and {Removal} in the {Grayscale} {Domain}},
	doi = {10.1109/ICDAR.2013.20},
	abstract = {The detection of staff lines is the first step of most Optical Music Recognition (OMR) systems. Its great significance derives from the ease with which we can then proceed with the extraction of musical symbols. All OMR tasks are usually achieved using binary images by setting thresholds that can be local or global. These techniques however, may remove relevant information of the music sheet and introduce artifacts which will degrade results in the later stages of the process. It arises therefore a need to create a method that reduces the loss of information due to the binarization. The baseline for the methodology proposed in this paper follows the shortest path algorithm proposed in [CardosoTPAMI08]. The concept of strong staff pixels (SSP's), which is a set of pixels with a high probability of belonging to a staff line, is proposed to guide the cost function. The SSP allows to overcome the results of the binary based detection and to generalize the binary framework to grayscale music scores. The proposed methodology achieves good results.},
	booktitle = {12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Rebelo, Ana and Cardoso, Jamie dos Santos},
	year = {2013},
	note = {ISSN: 1520-5363},
	keywords = {Optical imaging, Standards, Image segmentation, Robustness, Noise, optical character recognition, graph theory, feature extraction, music, object detection, staff line detection, staff line removal, OMR systems, binary based detection, binary framework, binary images, cost function, Cost function, global threshold, Gray-scale, grayscale domain, grayscale music scores, local threshold, musical symbol extraction, optical music recognition system, shortest path algorithm, SSP, strong staff pixels},
	pages = {57--61},
	file = {Rebelo_Cardoso_2013_Staff Line Detection and Removal in the Grayscale Domain.pdf:/home/ptorras/zotpapers/Rebelo_Cardoso_2013_Staff Line Detection and Removal in the Grayscale Domain.pdf:application/pdf},
}

@inproceedings{reed_automatic_1996,
	title = {Automatic {Computer} {Recognition} of {Printed} {Music}},
	isbn = {0-8186-7282-X},
	doi = {10.1109/ICPR.1996.547279},
	abstract = {This paper provides an overview to the implementation of Lemon, a complete optical music recognition system. Among the techniques employed by the implementation are: template matching, the Hough transform, line adjacency graphs, character profiles, and graph grammars. Experimental results, including comparisons with commercial systems, are provided},
	booktitle = {13th {International} {Conference} on {Pattern} {Recognition}},
	author = {Reed, K. Todd and Parker, J. R.},
	year = {1996},
	note = {ISSN: 1051-4651},
	keywords = {Optical Music Recognition, to classify},
	pages = {803--807},
}

@inproceedings{regimbal_neon2_2019,
	address = {Vienna, Austria},
	title = {Neon2: {A} {Verovio}-based square-notation editor},
	url = {https://music-encoding.org/conference/2019/abstracts_mec2019/Neon2.pdf},
	booktitle = {Music {Encoding} {Conference} 2019},
	author = {Regimbal, Juliette and Zoé, McLennan and Vigliensoni, Gabriel and Tran, Andrew and Fujinaga, Ichiro},
	year = {2019},
}

@inproceedings{de_reuse_robust_2019,
	address = {Delft, The Netherlands},
	title = {Robust {Transcript} {Alignment} on {Medieval} {Chant} {Manuscripts}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {de Reuse, Timothy and Fujinaga, Ichiro},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {21--26},
}

@incollection{rhodes_duplicate_2016,
	address = {Cham},
	title = {Duplicate {Detection} in {Facsimile} {Scans} of {Early} {Printed} {Music}},
	isbn = {978-3-319-25226-1},
	abstract = {There is a growing number of collections of readily available scanned musical documents, whether generated and managed by libraries, research projects, or volunteer efforts. They are typically digital images; for computational musicology we also need the musical data in machine-readable form. Optical Music Recognition (OMR) can be used on printed music, but is prone to error, depending on document condition and the quality of intermediate stages in the digitization process such as archival photographs. This work addresses the detection of one such error—duplication of images—and the discovery of other relationships between images in the process.},
	booktitle = {Analysis of {Large} and {Complex} {Data}},
	publisher = {Springer International Publishing},
	author = {Rhodes, Christophe and Crawford, Tim and d'Inverno, Mark},
	year = {2016},
	doi = {10.1007/978-3-319-25226-1_38},
	pages = {449--459},
	file = {Rhodes et al_2016_Duplicate Detection in Facsimile Scans of Early Printed Music.pdf:/home/ptorras/zotpapers/Rhodes et al_2016_Duplicate Detection in Facsimile Scans of Early Printed Music.pdf:application/pdf},
}

@inproceedings{rico_blanes_camera-based_2017,
	address = {Kyoto, Japan},
	title = {Camera-{Based} {Optical} {Music} {Recognition} {Using} a {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICDAR.2017.261},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE},
	author = {Rico Blanes, Adrià and Fornés Bisquerra, Alicia},
	year = {2017},
	pages = {27--28},
}

@article{riley_recommended_2003,
	title = {Recommended best practices for digital image capture of musical scores},
	volume = {19},
	issn = {1065-075X},
	doi = {10.1108/10650750310481784},
	number = {2},
	journal = {OCLC Systems \& Services},
	author = {Riley, Jenn and Fujinaga, Ichiro},
	year = {2003},
	keywords = {music, to classify, digital documents, file structures, imaging},
	pages = {62--69},
	file = {Riley_Fujinaga_2003_Recommended best practices for digital image capture of musical scores.pdf:/home/ptorras/zotpapers/Riley_Fujinaga_2003_Recommended best practices for digital image capture of musical scores.pdf:application/pdf},
}

@inproceedings{ringwalt_optical_2015,
	address = {Baton Rouge, Louisiana, USA},
	title = {Optical {Music} {Recognition} for {Interactive} {Score} {Display}},
	isbn = {978-0-692-49547-6},
	url = {http://dl.acm.org/citation.cfm?id=2993778.2993805},
	booktitle = {International {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {The School of Music and the Center for Computation and Technology (CCT), Louisiana State University},
	author = {Ringwalt, Dan and Dannenberg, Roger and Russell, Andrew},
	editor = {Berdahl, Edgar and Allison, Jesse T.},
	year = {2015},
	pages = {95--98},
}

@inproceedings{ringwalt_image_2015,
	title = {Image {Quality} {Estimation} for {Multi}-{Score} {OMR}},
	isbn = {978-84-606-8853-2},
	url = {http://ismir2015.uma.es/articles/37_Paper.pdf},
	booktitle = {16th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Ringwalt, Dan and Dannenberg, Roger B.},
	year = {2015},
	pages = {17--23},
	file = {Ringwalt_Dannenberg_2015_Image Quality Estimation for Multi-Score OMR.pdf:/home/ptorras/zotpapers/Ringwalt_Dannenberg_2015_Image Quality Estimation for Multi-Score OMR.pdf:application/pdf},
}

@inproceedings{rios-vila_readsco_2019,
	address = {Delft, The Netherlands},
	title = {{ReadSco}: {An} {Open}-{Source} {Web}-{Based} {Optical} {Music} {Recognition} {Tool}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Ríos-Vila, Antonio and Calvo-Zaragoza, Jorge and Rizo, David and Iñesta, José M.},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {27--30},
}

@inproceedings{rios-vila_completing_2021,
	address = {Alicante, Spain},
	title = {Completing {Optical} {Music} {Recognition} with {Agnostic} {Transcription} and {Machine} {Translation}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Ríos-Vila, Antonio and Rizo, David and Calvo-Zaragoza, Jorge and Iñesta, José Manuel},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {28--32},
}

@inproceedings{rios-vila_end--end_2022,
	address = {Online},
	title = {End-{To}-{End} {Full}-{Page} {Optical} {Music} {Recognition} of {Monophonic} {Documents} via {Score} {Unfolding}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Ríos-Vila, Antonio and Iñesta, Jose M. and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {20--24},
	file = {Rios-Vila et al_2022_End-To-End Full-Page Optical Music Recognition of Monophonic Documents via.pdf:/home/ptorras/zotpapers/Rios-Vila et al_2022_End-To-End Full-Page Optical Music Recognition of Monophonic Documents via.pdf:application/pdf},
}

@misc{eitner_repertoire_1952,
	title = {Répertoire {International} des {Sources} {Musicales}},
	url = {http://www.rism.info},
	author = {Eitner, Robert},
	year = {1952},
}

@phdthesis{rizo_symbolic_2010,
	type = {{PhD} {Thesis}},
	title = {Symbolic music comparison with tree data structures},
	url = {http://rua.ua.es/dspace/bitstream/10045/18331/1/Tesis_Rizo.pdf},
	school = {Universidad de Alicante},
	author = {Rizo, David},
	year = {2010},
	file = {Rizo_2010_Symbolic music comparison with tree data structures.pdf:/home/ptorras/zotpapers/Rizo_2010_Symbolic music comparison with tree data structures.pdf:application/pdf},
}

@article{roach_using_1988,
	title = {Using domain knowledge in low-level visual processing to interpret handwritten music: an experiment},
	volume = {21},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/0031320388900696},
	doi = {10.1016/0031-3203(88)90069-6},
	abstract = {Turning handwritten scores into engraved scores consumes a significant portion of music publishing companies' budgets. Pattern recognition is the major bottleneck holding up automation of this process. Human beings who know music can easily read a handwritten score, but without musical knowledge, even people cannot correctly perceive the markings in a handwritten score. This paper reports an experiment in which knowledge of music, a highly structured domain is applied to extract primitive musical features. This experiment shows that if the domain of image processing is well defined, significant improvements in low-level segmentations can be achieved (17 Refs.) recognition; computerised picture processing; expert systems; music},
	number = {1},
	journal = {Pattern Recognition},
	author = {Roach, JW W and Tatem, J E},
	year = {1988},
	keywords = {handwritten music recognition, to classify, character recogniti},
	pages = {33--44},
}

@article{roads_tsukuba_1986,
	title = {The {Tsukuba} {Musical} {Robot}},
	volume = {10},
	issn = {01489267, 15315169},
	url = {http://www.jstor.org/stable/3679483},
	number = {2},
	journal = {Computer Music Journal},
	author = {Roads, Curtis},
	year = {1986},
	note = {Publisher: The MIT Press},
	pages = {39--43},
}

@inproceedings{roggenkemper_how_2018,
	address = {Paris, France},
	title = {How can {Machine} {Learning} make {Optical} {Music} {Recognition} more relevant for practicing musicians?},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Roggenkemper, Heinz and Roggenkemper, Ryan},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {25--26},
}

@inproceedings{roland_music_2002,
	title = {The music encoding initiative ({MEI})},
	url = {https://pdfs.semanticscholar.org/7fc4/16754b0508837dde8b505b3fd4dc517c7292.pdf},
	booktitle = {1st {International} {Conference} on {Musical} {Applications} {Using} {XML}},
	author = {Roland, Perry},
	year = {2002},
	pages = {55--59},
	file = {Roland_2002_The music encoding initiative (MEI).pdf:/home/ptorras/zotpapers/Roland_2002_The music encoding initiative (MEI).pdf:application/pdf},
}

@inproceedings{rossant_reconnaissance_2001,
	address = {Toulouse, France},
	title = {Reconnaissance de {Partitions} {Musicales} par {Modélisation} {Floue} et {Intégration} de {Règles} {Musicales}},
	url = {http://hdl.handle.net/2042/13310},
	language = {French},
	booktitle = {{GRETSI}},
	author = {Rossant, Florence and Bloch, Isabelle},
	year = {2001},
}

@article{rossant_global_2002,
	title = {A global method for music symbol recognition in typeset music sheets},
	volume = {23},
	issn = {0167-8655},
	doi = {10.1016/S0167-8655(02)00036-3},
	abstract = {This paper presents an optical music recognition (OMR) system that can automatically recognize the main musical symbols of a scanned paper-based music score. Two major stages are distinguished: the first one, using low-level pre-processing, detects the isolated objects and outputs some hypotheses about them; the second one has to take the final correct decision, through high-level processing including contextual information and music writing rules. This article exposes both stages of the method: after explaining in detail the first one, the symbol analysis process, it shows through first experiments that its outputs can efficiently be used as inputs for a high-level decision process.},
	number = {10},
	journal = {Pattern Recognition Letters},
	author = {Rossant, Florence},
	year = {2002},
	keywords = {Optical music recognition},
	pages = {1129--1141},
	file = {Rossant_2002_A global method for music symbol recognition in typeset music sheets.pdf:/home/ptorras/zotpapers/Rossant_2002_A global method for music symbol recognition in typeset music sheets.pdf:application/pdf},
}

@article{rossant_fuzzy_2004,
	title = {A fuzzy model for optical recognition of musical scores},
	volume = {141},
	issn = {0165-0114},
	url = {http://www.sciencedirect.com/science/article/pii/S0165011403000940},
	doi = {10.1016/S0165-0114(03)00094-0},
	abstract = {Optical music recognition aims at reading automatically scanned scores in order to convert them in an electronic format, such as a midi file. We only consider here classical monophonic music: we exclude any music written on several staves, but also any music that contains chords. In order to overcome recognition failures due to the lack of methods dealing with structural information, non-local rules and corrections, we propose a recognition approach integrating structural information in the form of relationships between symbols and of musical rules. Another contribution of this paper is to solve ambiguities by accounting for sources of imprecision and uncertainty, within the fuzzy set and possibility theory framework. We add to a single symbol analysis several rules for checking the consistency of hypotheses: graphical consistency (compatibility between accidental and note, between grace note and note, between note and augmentation dot, etc.), and syntactic consistency (accidentals, tonality, metric). All these rules are combined in order to lead to better decisions. Experimental results on 65 music sheets show that our approach leads to very good results, and is able to correct errors made by other approaches, such as the one of SmartScore.},
	number = {2},
	journal = {Fuzzy Sets and Systems},
	author = {Rossant, Florence and Bloch, Isabelle},
	year = {2004},
	keywords = {Optical music recognition, Image processing, Pattern recognition, Flexible rules, Fusion, Fuzzy sets and possibility theory, Structural music information},
	pages = {165--201},
	file = {Rossant_Bloch_2004_A fuzzy model for optical recognition of musical scores.pdf:/home/ptorras/zotpapers/Rossant_Bloch_2004_A fuzzy model for optical recognition of musical scores.pdf:application/pdf},
}

@inproceedings{rossant_optical_2005,
	title = {Optical music recognition based on a fuzzy modeling of symbol classes and music writing rules},
	doi = {10.1109/ICIP.2005.1530111},
	abstract = {We propose an OMR method based on fuzzy modeling of the information extracted from the scanned score and of musical rules. The aim is to disambiguate the recognition hypotheses output by the individual symbol analysis process. Fuzzy modeling allows to account for imprecision in symbol detection, for typewriting variations, and for flexibility of rules. Tests conducted on a hundred of music sheets result in a global recognition rate of 98.55\%, and show good performances compared to SmartScore.},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} 2005},
	author = {Rossant, Florence and Bloch, Isabelle},
	year = {2005},
	note = {ISSN: 1522-4880},
	keywords = {Testing, Writing, Image recognition, optical music recognition, Multiple signal classification, Ordinary magnetoresistance, Pattern recognition, Performance evaluation, Printing, music, image recognition, object detection, Data mining, fuzzy modeling, fuzzy set theory, individual symbol analysis process, information extraction, music writing rules, optical information processing, Pattern analysis, recognition hypotheses, rules flexibility, scanned score, SmartScore, symbol classes, symbol detection, typewriting variations},
	pages = {II--538},
	file = {Rossant_Bloch_2005_Optical music recognition based on a fuzzy modeling of symbol classes and music.pdf:/home/ptorras/zotpapers/Rossant_Bloch_2005_Optical music recognition based on a fuzzy modeling of symbol classes and music.pdf:application/pdf},
}

@techreport{roth_approach_1994,
	title = {An approach to recognition of printed music},
	institution = {Swiss Federal Institute of Technology},
	author = {Roth, Martin},
	year = {1994},
	doi = {10.3929/ethz-a-000930574},
	file = {Roth_1994_An approach to recognition of printed music.pdf:/home/ptorras/zotpapers/Roth_1994_An approach to recognition of printed music.pdf:application/pdf},
}

@article{roy_hmm-based_2017,
	title = {{HMM}-based writer identification in music score documents without staff-line removal},
	volume = {89},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417305080},
	doi = {https://doi.org/10.1016/j.eswa.2017.07.031},
	abstract = {Writer identification from musical score documents is a challenging task due to its inherent problem of overlapping of musical symbols with staff-lines. Most of the existing works in the literature of writer identification in musical score documents were performed after a pre-processing stage of staff-lines removal. In this paper we propose a novel writer identification framework in musical score documents without removing staff-lines from the documents. In our approach, Hidden Markov Model (HMM) has been used to model the writing style of the writers without removing staff-lines. The sliding window features are extracted from musical score-lines and they are used to build writer specific HMM models. Given a query musical sheet, writer specific confidence for each musical line is returned by each writer specific model using a log-likelihood score. Next, a log-likelihood score in page level is computed by weighted combination of these scores from the corresponding line images of the page. A novel Factor Analysis-based feature selection technique is applied in sliding window features to reduce the noise appearing from staff-lines which proves efficiency in writer identification performance. In our framework we have also proposed a novel score-line detection approach in musical sheet using HMM. The experiment has been performed in CVC-MUSCIMA data set and the results obtained show that the proposed approach is efficient for score-line detection and writer identification without removing staff-lines. To get the idea of computation time of our method, detail analysis of execution time is also provided.},
	journal = {Expert Systems with Applications},
	author = {Roy, Partha Pratim and Bhunia, Ayan Kumar and Pal, Umapada},
	year = {2017},
	keywords = {Writer identification, Factor analysis, Hidden Markov model, Music score documents},
	pages = {222--240},
	file = {Roy et al_2017_HMM-based writer identification in music score documents without staff-line.pdf:/home/ptorras/zotpapers/Roy et al_2017_HMM-based writer identification in music score documents without staff-line.pdf:application/pdf},
}

@mastersthesis{ruttenberg_optical_1991,
	address = {Boston, MA},
	title = {Optical {Reading} of {Typeset} {Music}},
	url = {https://dspace.mit.edu/bitstream/handle/1721.1/69715/24680744-MIT.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Ruttenberg, Alan},
	year = {1991},
	file = {Ruttenberg_1991_Optical Reading of Typeset Music.pdf:/home/ptorras/zotpapers/Ruttenberg_1991_Optical Reading of Typeset Music.pdf:application/pdf},
}

@inproceedings{saitis_correcting_2014,
	title = {Correcting {Large}-{Scale} {OMR} {Data} with {Crowdsourcing}},
	doi = {10.1145/2660168.2660186},
	booktitle = {1st {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Saitis, Charalampos and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2014},
	pages = {1--3},
}

@inproceedings{saleh_pixeljs_2017,
	address = {Kyoto, Japan},
	title = {Pixel.js: {Web}-{Based} {Pixel} {Classification} {Correction} {Platform} for {Ground} {Truth} {Creation}},
	doi = {10.1109/ICDAR.2017.267},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Saleh, Zeyad and Zhang, Ke and Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	keywords = {Image segmentation, Text analysis, Mice, Graphics, document analysis, classification, Tools, Labeling, image segmentation, document recognition, image classification, ground truth, correction, Internet, Algorithm design and analysis, aforementioned ground truth data, graphics, inaccurate heuristic trained image segmentation algorithms, labeling, layers, misclassified pixels, open-source, pixel, pixel-level, pixel-level classification correction platform, Pixel.js, platform, segmentation algorithm output, user interface, web-based, Web-based pixel classification correction platform},
	pages = {39--40},
}

@inproceedings{samiotis_hybrid_2021,
	address = {Alicante, Spain},
	title = {Hybrid {Annotation} {Systems} for {Music} {Transcription}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Samiotis, Ioannis Petros and Lofi, Christoph and Bozzon, Alessandro},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {23--27},
}

@misc{sapp_omr_2013,
	title = {{OMR} {Comparison} of {SmartScore} and {SharpEye}},
	url = {https://ccrma.stanford.edu/~craig/mro-compare-beethoven},
	author = {Sapp, Craig},
	year = {2013},
}

@inproceedings{seales_interpreting_1995,
	address = {Berlin, Heidelberg},
	title = {Interpreting music manuscripts: {A} logic-based, object-oriented approach},
	isbn = {978-3-540-49298-6},
	doi = {10.1007/3-540-60697-1_101},
	abstract = {This paper presents a complete framework for recognizing classes of machine-printed musical manuscripts. Our framework is designed around the decomposition of a manuscript into objects such as staves and bars which are processed with a knowledge base module that encodes rules in Prolog. Object decomposition focuses the recognition problem, and the rule base provides a powerful and flexible way to encode the rules of a particular manuscript class. Our rule-base registers notes and stems, eliminates false-positives and correctly labels notes according to their position on the staff. We present results that show 99\% accuracy at detecting note-heads and 95\% accuracy in finding stems.},
	booktitle = {Image {Analysis} {Applications} and {Computer} {Graphics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Seales, W. Brent and Rajasekar, Arcot},
	editor = {Chin, Roland T. and Ip, Horace H. S. and Naiman, Avi C. and Pong, Ting-Chuen},
	year = {1995},
	pages = {181--188},
}

@inproceedings{sebastien_score_2012,
	address = {Porto, Portugal},
	title = {Score {Analyzer}: {Automatically} {Determining} {Scores} {Difficulty} {Level} for {Instrumental} e-{Learning}},
	url = {http://ismir2012.ismir.net/event/papers/571-ismir-2012.pdf},
	booktitle = {13th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Sébastien, Véronique and Ralambondrainy, Henri and Sébastien, Olivier and Conruyt, Noël},
	editor = {Gouyon, Fabien and Herrera, Perfecto and Martins, Luis Gustavo and Müller, Meinard},
	year = {2012},
	pages = {571--576},
}

@inproceedings{sharif_comscan_2009,
	title = {[{COMSCAN}]: {An} {Optical} {Music} {Recognition} {System}},
	doi = {10.1145/1838002.1838040},
	booktitle = {7th {International} {Conference} on {Frontiers} of {Information} {Technology}},
	publisher = {ACM},
	author = {Sharif, Muhammad and Arshad, Quratul-Ain and Raza, Mudassar and Khan, Wazir Zada},
	year = {2009},
	pages = {34},
}

@inproceedings{shatri_doremi_2021,
	address = {Alicante, Spain},
	title = {{DoReMi}: {First} glance at a universal {OMR} dataset},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Shatri, Elona and Fazekas, György},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {43--49},
}

@inproceedings{sheridan_defacing_2004,
	title = {Defacing {Music} {Scores} for {Improved} {Recognition}},
	url = {https://sites.google.com/site/theauscc/auscc04/papers/sheridan-auscc04.pdf},
	abstract = {The area of Optical Music Recognition (OMR) has long been plagued by an inability to provide a definitive method for locating and identifying musical objects superimposed on musical stave lines. The first step in the process of recognising musical symbols in OMR has previously been to either remove the stave lines, or ignore them. Removing stave lines leads to many problems of fragmented and deformed musical symbols, or in the case of ignoring them, a lowered chance of recognition. Most OMR systems attempt to correct these deficiencies later on in the process through many varied approaches including bounding box analysis, k-nearest-neighbour (k-NN) and neural network (ANN) classification schemes. All of these have a level of success, but none have provided nearly the desired level of accuracy. This paper aims to show that this removal of the stave lines before symbol recognition is not the only first step and may not be the best. Instead of removing stave lines, more should be added! This process is called ‘defacing’ since it adds stave lines to the score at a 1/2 stave line width, and actually overwrites the score - apparently complicating the recognition procedure. However, the addition of signal to the image means that subsequent symbol recognition is ‘normalised’ and a musical symbol will look the same whether it was above, below or on a stave line. As a result of this, a classification system trained with double stave lines should provide a higher level of accuracy than the traditional approaches of removing/ignoring the stave lines.},
	booktitle = {2nd {Australian} {Undergraduate} {Students}' {Computing} {Conference}},
	author = {Sheridan, Scott and George, Susan E.},
	year = {2004},
	pages = {142--148},
	file = {Sheridan_George_2004_Defacing Music Scores for Improved Recognition.pdf:/home/ptorras/zotpapers/Sheridan_George_2004_Defacing Music Scores for Improved Recognition.pdf:application/pdf},
}

@article{shi_end--end_2017,
	title = {An {End}-to-{End} {Trainable} {Neural} {Network} for {Image}-{Based} {Sequence} {Recognition} and {Its} {Application} to {Scene} {Text} {Recognition}},
	volume = {39},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2016.2646371},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for realworld application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	year = {2017},
	keywords = {Neural networks, Text recognition, Feature extraction, Image recognition, optical music recognition, neural network, feature extraction, music, image recognition, image segmentation, neural nets, learning (artificial intelligence), text detection, transcription, Convolutional codes, computer vision, convolutional neural network, Context, end-to-end trainable neural network, ICDAR datasets, IIIT-5K datasets, image-based music score recognition, image-based sequence recognition, lexicon-based scene text recognition tasks, lexicon-free scene text recognition tasks, Logic gates, long-short term memory, neural network architecture, scene text recognition, sequence modeling, Sequence recognition, street view text datasets},
	pages = {2298--2304},
	file = {Shi et al_2017_An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and.pdf:/home/ptorras/zotpapers/Shi et al_2017_An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and.pdf:application/pdf},
}

@inproceedings{sicard_efficient_1992,
	title = {An efficient method for the recognition of printed music},
	doi = {10.1109/ICPR.1992.202052},
	abstract = {Deals with the recognition mechanisms of printed music scores. The techniques for extracting linear features, keys, noteheads and other musical figures from a digitized image are presented. Experimental results are given to show the effectiveness of the proposed methodology with a discussion of its performances and limits. Applications to full automated music score extraction, printed or handwritten are also discussed.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {11th {International} {Conference} on {Pattern} {Recognition}},
	author = {Sicard, Etienne},
	year = {1992},
	keywords = {Handwriting recognition, Feature extraction, Multiple signal classification, pattern recognition, Pattern recognition, feature extraction, music, Robot vision systems, Machine vision, Image converters, printed music scores, Histograms, digitized image, handwritten music, keys, linear features, noteheads, Real time systems, Robotics and automation},
	pages = {573--576},
}

@mastersthesis{silva_mobile_2013,
	title = {Mobile framework for recognition of musical characters},
	url = {https://repositorio-aberto.up.pt/bitstream/10216/68500/2/26777.pdf},
	school = {Universidade do Porto},
	author = {Silva, Rui Miguel Filipe da},
	year = {2013},
	file = {Silva_2013_Mobile framework for recognition of musical characters.pdf:/home/ptorras/zotpapers/Silva_2013_Mobile framework for recognition of musical characters.pdf:application/pdf},
}

@misc{musitek_smartscore_2017,
	title = {{SmartScore} {X2}},
	url = {http://www.musitek.com/smartscore-pro.html},
	author = {{Musitek}},
	year = {2017},
}

@inproceedings{smiatacz_matrix-based_2008,
	title = {Matrix-based classifiers applied to recognition of musical notation symbols},
	doi = {10.1109/INFTECH.2008.4621678},
	abstract = {The paper presents the application of matrix-based classifiers to the problem of automatic recognition of musical notation symbols. The idea of classification algorithms operating on matrices instead of feature vectors is briefly introduced together with a short description of methods that we have recently proposed. The experiments that we report show that the matrix-based approach can be used to improve the effectiveness and usefulness of the OMR system developed in our department as a part of the digital library of musical documents.},
	booktitle = {1st {International} {Conference} on {Information} {Technology}},
	author = {Smiatacz, Maciej and Malina, Witold},
	year = {2008},
	keywords = {Training, Feature extraction, optical music recognition, optical character recognition, music, image classification, Classification algorithms, Pixel, Support vector machine classification, Covariance matrix, Entropy, matrix algebra, matrix-based classifier, musical document, musical notation symbol recognition},
	pages = {1--4},
}

@inproceedings{soak_music_2002,
	title = {Music recognition system using {ART}-1 and {GA}},
	doi = {10.1117/12.458413},
	abstract = {Previously, most optical music recognition (OMR) systems have used the neural network, and used mainly back- propagation training method. One of the disadvantages of BP is that much time is required to train data sets. For example, when new data sets are added, all data sets have to be trained. Another disadvantage is that weighting values cannot be guaranteed as global optima after training them. It means that weighting values can fall down to local optimum solution. In this paper, we propose the new OMR method which combines the adaptive resonance theory (ART-1) with the genetic algorithms (GA). For reducing the training time, we use ART-1 which classifies several music symbols. It has another advantage to reduce the number of datasets, because classified symbols through ART-1 are used as input vectors of BP. And for guaranteeing the global optima in training data set, we use GA which is known as one of the best method for finding optimal solutions at complex problems.},
	booktitle = {{AeroSense} 2002},
	author = {Soak, Sang Moon and Chang, Seok Cheol and Shin, Taehwan and Ahn, Byung-Ha},
	year = {2002},
}

@inproceedings{sober-mira_pen-based_2017,
	address = {Kyoto, Japan},
	title = {Pen-{Based} {Music} {Document} {Transcription}},
	doi = {10.1109/ICDAR.2017.258},
	booktitle = {14th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE},
	author = {Sober-Mira, Javier and Calvo-Zaragoza, Jorge and Rizo, David and Iñesta, José Manuel},
	year = {2017},
	pages = {21--22},
}

@inproceedings{sober-mira_multimodal_2017,
	address = {Barcelona, Spain},
	title = {Multimodal {Recognition} for {Music} {Document} {Transcription}},
	url = {https://grfia.dlsi.ua.es/repositori/grfia/pubs/380/mml17proceedings-67.pdf},
	booktitle = {10th {International} {Workshop} on {Machine} {Learning} and {Music}},
	author = {Sober-Mira, Javier and Calvo-Zaragoza, Jorge and Rizo, David and Iñesta, José Manuel},
	year = {2017},
	file = {Sober-Mira et al_2017_Multimodal Recognition for Music Document Transcription.pdf:/home/ptorras/zotpapers/Sober-Mira et al_2017_Multimodal Recognition for Music Document Transcription.pdf:application/pdf},
}

@article{sotoodeh_music_2018,
	title = {A music symbols recognition method using pattern matching along with integrated projection and morphological operation techniques},
	volume = {77},
	issn = {1573-7721},
	doi = {10.1007/s11042-017-5256-y},
	abstract = {Optical Music Recognition (OMR) can be divided into three main phases: (i) staff line detection and removal. The goal of this phase is to detect and to remove staff lines from sheet music images. (ii) music symbol detection and segmentation. The propose of this phase is to detect the remaining musical symbols such as single symbols and group symbols, then segment the group symbols to single or primitive symbols after removing staff lines. (iii) musical symbols recognition. In this phase, recognition of musical symbols is the main objective. The method presented in this paper, covers all three phases. One advantage of the first phase of the proposed method is that it is robust to staff lines rotation and staff lines which have curvature in sheet music images. Moreover, the staff lines are removed accurately and quickly and also fewer details of the musical symbols are omitted. The proposed method in the first phase focuses on the hand-written documents databases which have been introduced in the CVC-MUSCIMA and ICDAR 2013. It has the lowest error rate among well-known methods and outperforms the state of the art in CVC-MUSCIMA database. In ICDAR 2013, the specificity measure of this method is 99.71\% which is the highest specificity among available methods. Also, in terms of accuracy, recall rate and f-measure is only slightly less than the best method. Therefor our method is comparable favorably to the existing methods. In the second phase, the symbols are divided into two categories, single and group. In the recognition phase, we use a pattern matching method to identify single symbols. For recognizing group symbols, a hierarchical method is proposed. The proposed method in the third phase has several advantages over the previous methods. It is quite robust to skewness of musical group symbols. Furthermore, it provides high accuracy in recognition of the symbols.},
	number = {13},
	journal = {Multimedia Tools and Applications},
	author = {Sotoodeh, Mahmood and Tajeripour, Farshad and Teimori, Sadegh and Jorgensen, Kirk},
	year = {2018},
	pages = {16833--16866},
}

@misc{staffpad_ltd_staffpad_2017,
	title = {{StaffPad}},
	url = {http://www.staffpad.net},
	author = {{StaffPad Ltd.}},
	year = {2017},
}

@article{stevens_comparison_1992,
	title = {A comparison of connectionist models of music recognition and human performance},
	volume = {2},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/BF00419420},
	doi = {10.1007/BF00419420},
	abstract = {Current artificial neural network or connectionist models of music cognition embody feature-extraction and feature-weighting principles. This paper reports two experiments which seek evidence for similar processes mediating recognition of short musical compositions by musically trained and untrained listeners. The experiments are cast within a pattern recognition framework based on the vision-audition analogue wherein music is considered an auditory pattern consisting of local and global features. Local features such as inter-note interval, and global features such as melodic contour, are derived from a two-dimensional matrix in which music is represented as a series of frequencies plotted over time.},
	number = {4},
	journal = {Minds and Machines},
	author = {Stevens, Catherine and Latimer, Cyril},
	year = {1992},
	pages = {379--400},
}

@techreport{stramer_digitizing_2014,
	title = {Digitizing sheet music},
	url = {https://web.stanford.edu/class/ee368/Project_Spring_1415/Reports/Stramer.pdf},
	institution = {Stanford University},
	author = {Stramer, Tal},
	year = {2014},
	file = {Stramer_2014_Digitizing sheet music.pdf:/home/ptorras/zotpapers/Stramer_2014_Digitizing sheet music.pdf:application/pdf},
}

@inproceedings{su_musical_2001,
	title = {Musical symbol recognition using {SOM}-based fuzzy systems},
	doi = {10.1109/NAFIPS.2001.944402},
	abstract = {A large number of research activities have been undertaken to investigate optical music recognition (OMR). OMR involves identifying musical symbols on a scanned sheet of music and transforming them into a computer readable format. We propose an efficient method based on SOM-based fuzzy systems to recognize musical symbols. A database consisting of 9 kinds of musical symbols were used to test the performance of the SOM-based fuzzy systems.},
	booktitle = {Joint 9th {IFSA} {World} {Congress} and 20th {NAFIPS} {International} {Conference}},
	author = {Su, Mu-Chun and Tew, Chee-Yuen and Chen, Hsin-Hua},
	year = {2001},
	keywords = {Neural networks, Optical character recognition software, Computer science, optical music recognition, Ordinary magnetoresistance, pattern recognition, Pattern recognition, Optical computing, optical character recognition, document image processing, music, performance evaluation, visual databases, musical symbol recognition, Computational efficiency, computer readable format, database, fuzzy neural nets, Fuzzy neural networks, fuzzy self organising feature maps, fuzzy SOM systems, Fuzzy systems, neurofuzzy systems, self-organising feature maps, Vector quantization},
	pages = {2150--2153 vol.4},
}

@inproceedings{su_effective_2012,
	title = {An effective staff detection and removal technique for musical documents},
	isbn = {978-0-7695-4661-2},
	doi = {10.1109/DAS.2012.16},
	abstract = {Abstract Musical staff line detection and removal techniques detect the staff positions in musical documents and segment musical score from musical documents by removing those staff lines. It is an important preprocessing step for ensuing the Optical Music Recognition ...},
	booktitle = {10th {International} {Workshop} on {Document} {Analysis} {Systems}},
	publisher = {IEEE},
	author = {Su, Bolan and Lu, Shijian and Pal, Umapada and Tan, Chew Lim},
	year = {2012},
	keywords = {Optical Music Recognition, to classify, Musical Staff, Staff Line Removal, Staff Line Segmentation, Staff Line Shape Modeling},
	pages = {160--164},
}

@inproceedings{sugano_wabot-2_1987,
	title = {{WABOT}-2: {Autonomous} robot with dexterous finger-arm–{Finger}-arm coordination control in keyboard performance},
	doi = {10.1109/ROBOT.1987.1088025},
	abstract = {Advanced robots will have to not only have 'hard' functions but also have 'soft' functions. Therefore, the purpose of this study is to realize 'soft' functions of robots such as dexterity, speediness and intelligence by the development of an anthropomorphic intelligent robot playing keyboard instrument. This paper describes the development of keyboard playing robot WABOT-2(WAseda roBOT-2) with a focus on the mechanisms of arm-and-hand which has 21 degrees of freedom in total, their hierarchically structured control computer system, the information processing method at the high level computer and finger-arm coordination control which realizes the autonomous movement of WABOT-2.},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Sugano, Shigeki and Kato, Ichiro},
	year = {1987},
	keywords = {Information processing, Humans, Keyboards, Anthropomorphism, Control systems, Fingers, Humanoid robots, Intelligent robots, Robot kinematics, Service robots},
	pages = {90--97},
}

@inproceedings{szwoch_robust_2005,
	address = {Berlin, Heidelberg},
	title = {A {Robust} {Detector} for {Distorted} {Music} {Staves}},
	isbn = {978-3-540-32011-1},
	doi = {10.1007/11556121_86},
	abstract = {In this paper an algorithm for music staves detection is presented. The algorithm bases on horizontal projections in local windows of a score image and farther processing of resulting histograms and their connections. Experiments carried out, proved high efficiency of presented algorithm and its robustness in case of non-ideal staff lines: skew and with barrel and pincushion distortions. The algorithm allows for usage of acquisition devices alternative to scanner such as digital cameras.},
	booktitle = {Computer {Analysis} of {Images} and {Patterns}},
	publisher = {Springer Berlin Heidelberg},
	author = {Szwoch, Mariusz},
	editor = {Gagalowicz, André and Philips, Wilfried},
	year = {2005},
	pages = {701--708},
}

@inproceedings{szwoch_guido_2007,
	title = {Guido: {A} {Musical} {Score} {Recognition} {System}},
	doi = {10.1109/ICDAR.2007.4377027},
	abstract = {This paper presents an optical music recognition system Guido that can automatically recognize the main musical symbols of music scores that were scanned or taken by a digital camera. The application is based on object model of musical notation and uses linguistic approach for symbol interpretation and error correction. The system offers musical editor with a partially automatic error correction.},
	booktitle = {9th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Szwoch, Mariusz},
	year = {2007},
	note = {ISSN: 1520-5363},
	keywords = {Image segmentation, Speech analysis, Ordinary magnetoresistance, music, image recognition, Performance analysis, linguistics, Books, Error correction, musical symbols, optical music recognition system, automatic error correction, digital camera, Digital cameras, error correction, Guido, linguistic, musical score recognition system, Nonlinear optics, Optical devices, Optical distortion, symbol interpretation},
	pages = {809--813},
}

@inproceedings{szwoch_using_2008,
	address = {Herrsching, Germany},
	title = {Using {MusicXML} to {Evaluate} {Accuracy} of {OMR} {Systems}},
	isbn = {978-3-540-87729-5},
	doi = {10.1007/978-3-540-87730-1_53},
	abstract = {In this paper a methodology for automatic accuracy evaluation in optical music recognition (OMR) applications is proposed. Presented approach assumes using ground truth images together with digital music scores describing their content. The automatic evaluation algorithm measures differences between the tested score and the reference one, both stored in MusicXML format. Some preliminary test results of this approach are presented based on the algorithm’s implementation in OMR Guido application.},
	booktitle = {International {Conference} on {Theory} and {Application} of {Diagrams}},
	publisher = {Springer-Verlag},
	author = {Szwoch, Mariusz},
	year = {2008},
	note = {Backup Publisher: Springer},
	keywords = {Optical Music Recognition, MusicXML, Performance of Systems},
	pages = {419--422},
}

@inproceedings{taele_maestoso_2015,
	address = {Austin, Texas},
	title = {Maestoso: {An} {Intelligent} {Educational} {Sketching} {Tool} for {Learning} {Music} {Theory}},
	isbn = {0-262-51129-0},
	url = {http://dl.acm.org/citation.cfm?id=2888116.2888271},
	abstract = {Learning music theory not only has practical benefits for musicians to write, perform, understand, and express music better, but also for both non-musicians to improve critical thinking, math analytical skills, and music appreciation. However, current external tools applicable for learning music theory through writing when human instruction is unavailable are either limited in feedback, lacking a written modality, or assuming already strong familiarity of music theory concepts. In this paper, we describe Maestoso, an educational tool for novice learners to learn music theory through sketching practice of quizzed music structures. Maestoso first automatically recognizes students' sketched input of quizzed concepts, then relies on existing sketch and gesture recognition techniques to automatically recognize the input, and finally generates instructor-emulated feedback. From our evaluations, we demonstrate that Maestoso performs reasonably well on recognizing music structure elements and that novice students can comfortably grasp introductory music theory in a single session.},
	booktitle = {27th {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Taele, Paul and Barreto, Laura and Hammond, Tracy},
	year = {2015},
	pages = {3999--4005},
}

@inproceedings{tambouratzis_identification_2011,
	title = {Identification of key music symbols for optical music recognition and on-screen presentation},
	doi = {10.1109/IJCNN.2011.6033461},
	abstract = {A novel optical music recognition (OMR) system is put forward, where the custom-made on-screen presentation of the music score (MS) is promoted via the recognition of key music symbols only. The proposed system does not require perfect manuscript alignment or noise removal. Following the segmentation of each MS page into systems and, subsequently, into staves, staff lines, measures and candidate music symbols (CMS's), music symbol recognition is limited to the identification of the clefs, accidentals and time signatures. Such an implementation entails significantly less computational effort than that required by classic OMR systems, without an observable compromise in the quality of the on-screen presentation of the MS. The identification of the music symbols of interest is performed via probabilistic neural networks (PNN's), which are trained on a small set of exemplars from the MS itself. The initial results are promising in terms of efficiency, identification accuracy and quality of viewing.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Tambouratzis, Tatiana},
	year = {2011},
	note = {ISSN: 2161-4407},
	keywords = {Training, Image segmentation, optical music recognition, Multiple signal classification, Accuracy, Noise, music, image recognition, image segmentation, neural nets, Educational institutions, key music symbol identification, MS segmentation, on-screen music score presentation, probabilistic neural networks, Time measurement},
	pages = {1935--1942},
}

@article{tambouratzis_digital_2013,
	title = {The {Digital} {Music} {Stand} as a {Minimal} {Processing} {Custom}-{Made} {Optical} {Music} {Recognition} {System}, {Part} 1: {Key} {Music} {Symbol} {Recognition}},
	volume = {28},
	issn = {0884-8173},
	doi = {10.1002/int.21586},
	abstract = {The digital music stand is proposed as a minimal-processing optical music recognition implementation, where music score (MS) presentation is realized without prior alignment, noise, or staff line removal. After each MS page is segmented into systems, staves, measures, and candidate music symbols, music symbol recognition is accomplished via probabilistic neural networks: Only the key music symbols (namely clefs, global accidentals, time signatures) of the MS are identified, while the remaining music symbols are generally classified. Subsequently, satisfactory quality of on-screen MS viewing is accomplished via the concatenation and/or substitution of appropriately selected parts and isolated music symbols of the original MS. In this piece of research, the processing stages leading to on-screen MS presentation are detailed. © 2013 Wiley Periodicals, Inc.},
	number = {5},
	journal = {International Journal of Intelligent Systems},
	author = {Tambouratzis, Tatiana},
	year = {2013},
	pages = {474--504},
}

@article{tardon_automatic_2020,
	title = {Automatic {Staff} {Reconstruction} within {SIMSSA} {Project}},
	volume = {10},
	url = {https://www.mdpi.com/2076-3417/10/7/2468},
	doi = {10.3390/app10072468},
	number = {7},
	journal = {Applied Sciences},
	author = {Tardón, Lorenzo J. and Barbancho, Isabel and Barbancho, Ana M. and Fujinaga, Ichiro},
	year = {2020},
	pages = {2468--2484},
	file = {Tardon et al_2020_Automatic Staff Reconstruction within SIMSSA Project.pdf:/home/ptorras/zotpapers/Tardon et al_2020_Automatic Staff Reconstruction within SIMSSA Project.pdf:application/pdf},
}

@inproceedings{thomae_mensural_2019,
	address = {New York, NY, USA},
	series = {{DLfM} ’19},
	title = {The {Mensural} {Scoring}-up {Tool}},
	isbn = {978-1-4503-7239-8},
	url = {https://doi.org/10.1145/3358664.3358668},
	doi = {10.1145/3358664.3358668},
	booktitle = {6th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {Association for Computing Machinery},
	author = {Thomae, Martha E. and Cumming, Julie E. and Fujinaga, Ichiro},
	year = {2019},
	note = {event-place: The Hague, Netherlands},
	keywords = {encoding, mensural notation, automatic transcription, Mensural MEI, parts to score transformation},
	pages = {9--19},
}

@inproceedings{thompson_searching_2011,
	title = {Searching the {Liber} {Usualis}: {Using} {CouchDB} and {ElasticSearch} to {Query} {Graphical} {Music} {Documents}},
	url = {http://ismir2011.ismir.net/latebreaking/LB-10.pdf},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Thompson, Jessica and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2011},
	file = {Thompson et al_2011_Searching the Liber Usualis.pdf:/home/ptorras/zotpapers/Thompson et al_2011_Searching the Liber Usualis.pdf:application/pdf},
}

@inproceedings{timofte_automatic_2013,
	address = {Berlin, Heidelberg},
	title = {Automatic {Stave} {Discovery} for {Musical} {Facsimiles}},
	isbn = {978-3-642-37447-0},
	doi = {10.1007/978-3-642-37447-0_39},
	abstract = {Lately, there is an increased interest in the analysis of music score facsimiles, aiming at automatic digitization and recognition. Noise, corruption, variations in handwriting, non-standard page layouts and notations are common problems affecting especially the centuries-old manuscripts.},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Timofte, Radu and Van Gool, Luc},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	pages = {510--523},
	file = {Timofte_Van Gool_2013_Automatic Stave Discovery for Musical Facsimiles.pdf:/home/ptorras/zotpapers/Timofte_Van Gool_2013_Automatic Stave Discovery for Musical Facsimiles.pdf:application/pdf},
}

@inproceedings{torras_improving_2022,
	address = {Online},
	title = {Improving {Handwritten} {Music} {Recognition} through {Language} {Model} {Integration}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Torras, Pau and Baró, Arnau and Kang, Lei and Fornés, Alicia},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	file = {Torras et al_2022_Improving Handwritten Music Recognition through Language Model Integration.pdf:/home/ptorras/zotpapers/Torras et al_2022_Improving Handwritten Music Recognition through Language Model Integration.pdf:application/pdf},
}

@inproceedings{toyama_symbol_2006,
	title = {Symbol {Recognition} of {Printed} {Piano} {Scores} with {Touching} {Symbols}},
	doi = {10.1109/ICPR.2006.1099},
	abstract = {To build a music database efficiently, an automatic score recognition system is a critical component. Many previous methods are applicable only to some simple music scores. In case of complex music scores it becomes difficult to detect symbols correctly because of noise and connection between symbols included in the scores. In this paper, we propose a score recognition method which is applicable to the complex music scores. Symbol candidates are detected by template matching. From these candidates correct symbols are selected by considering their relative positions and mutual connections. Under the presence of noise and connected symbols, the proposed method outperformed "Score Maker" which is an optical music score recognition software},
	booktitle = {18th {International} {Conference} on {Pattern} {Recognition}},
	author = {Toyama, Fubito and Shoji, Kenji and Miyamichi, Juichi},
	year = {2006},
	note = {ISSN: 1051-4651},
	keywords = {Optical character recognition software, Databases, Image recognition, Ordinary magnetoresistance, Head, Character recognition, music, template matching, music scores, symbol recognition, Optical noise, printed piano scores, automatic score recognition system, Data engineering, image matching, music database, Noise shaping, Software performance, touching symbols},
	pages = {480--483},
}

@article{tsai_using_2020,
	title = {Using {Cell} {Phone} {Pictures} of {Sheet} {Music} {To} {Retrieve} {MIDI} {Passages}},
	url = {https://arxiv.org/abs/2004.11724},
	doi = {10.1109/TMM.2020.2973831},
	journal = {IEEE Transactions on Multimedia},
	author = {Tsai, Timothy J. and Yang, Daniel and Shan, Mengyi and Tanprasert, Thitaree and Jenrungrot, Teerapat},
	year = {2020},
	pages = {1--13},
	file = {Tsai et al_2020_Using Cell Phone Pictures of Sheet Music To Retrieve MIDI Passages.pdf:/home/ptorras/zotpapers/Tsai et al_2020_Using Cell Phone Pictures of Sheet Music To Retrieve MIDI Passages.pdf:application/pdf},
}

@inproceedings{tsandilas_interpreting_2012,
	address = {Cambridge, Massachusetts, USA},
	title = {Interpreting {Strokes} on {Paper} with a {Mobile} {Assistant}},
	isbn = {978-1-4503-1580-7},
	doi = {10.1145/2380116.2380155},
	booktitle = {25th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Tsandilas, Theophanis},
	year = {2012},
	keywords = {bimanual interaction, interactive recognition, mobile, music interfaces., paper interfaces, pen + touch},
	pages = {299--308},
	file = {Tsandilas_2012_Interpreting Strokes on Paper with a Mobile Assistant.pdf:/home/ptorras/zotpapers/Tsandilas_2012_Interpreting Strokes on Paper with a Mobile Assistant.pdf:application/pdf},
}

@inproceedings{tuggener_deepscores_2018,
	address = {Beijing, China},
	title = {{DeepScores} - {A} {Dataset} for {Segmentation}, {Detection} and {Classification} of {Tiny} {Objects}},
	url = {https://arxiv.org/abs/1804.00525},
	doi = {10.21256/zhaw-4255},
	abstract = {We present the DeepScores dataset with the goal of advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding. DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. With close to a hundred millions of small objects, this makes our dataset not only unique, but also the largest public dataset. DeepScores comes with ground truth for object classification, detection and semantic segmentation. DeepScores thus poses a relevant challenge for computer vision in general, beyond the scope of optical music recognition (OMR) research. We present a detailed statistical analysis of the dataset, comparing it with other computer vision datasets like Caltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer vision datasets, as well as with other OMR datasets. Finally, we provide baseline performances for object classification and give pointers to future research based on this dataset.},
	booktitle = {24th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {ZHAW},
	author = {Tuggener, Lukas and Elezi, Ismail and Schmidhuber, Jürgen and Pelillo, Marcello and Stadelmann, Thilo},
	year = {2018},
	file = {Tuggener et al_2018_DeepScores - A Dataset for Segmentation, Detection and Classification of Tiny.pdf:/home/ptorras/zotpapers/Tuggener et al_2018_DeepScores - A Dataset for Segmentation, Detection and Classification of Tiny.pdf:application/pdf},
}

@inproceedings{tuggener_deepscoresv2_2020,
	address = {Milan, Italy},
	title = {The {DeepScoresV2} {Dataset} and {Benchmark} for {Music} {Object} {Detection}},
	doi = {10.21256/zhaw-20647},
	abstract = {In this paper, we present DeepScoresV2, an extended version of the DeepScores dataset for optical music recognition (OMR). We improve upon the original DeepScores dataset by providing much more detailed annotations, namely (a) annotations for 135 classes including fundamental symbols of non-fixed size and shape, increasing the number of annotated symbols by 23\%; (b) oriented bounding boxes; (c) higher-level rhythm and pitch information (onset beat for all symbols and line position for noteheads); and (d) a compatibility mode for easy use in conjunction with the MUSCIMA++ dataset for OMR on handwritten documents. These additions open up the potential for future advancement in OMR research. Additionally, we release two state-of-the-art baselines for DeepScoresV2 based on Faster R-CNN and the Deep Watershed Detector. An analysis of the baselines shows that regular orthogonal bounding boxes are unsuitable for objects which are long, small, and potentially rotated, such as ties and beams, which demonstrates the need for detection algorithms that naturally incorporate object angles.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Pattern} {Recognition}},
	author = {Tuggener, Lukas and Satyawan, Yvan Putra and Pacha, Alexander and Schmidhuber, Jürgen and Stadelmann, Thilo},
	year = {2020},
	file = {Tuggener et al_2020_The DeepScoresV2 Dataset and Benchmark for Music Object Detection.pdf:/home/ptorras/zotpapers/Tuggener et al_2020_The DeepScoresV2 Dataset and Benchmark for Music Object Detection.pdf:application/pdf},
}

@techreport{vidal_optical_2012,
	title = {Optical {Music} {Recognition} in the grey-scale domain},
	url = {https://paginas.fe.up.pt/~ee03270/downloads/FinalReportPDI.pdf},
	institution = {Universidade do Porto},
	author = {Vidal, Vitor Hugo Couto},
	year = {2012},
	file = {Vidal_2012_Optical Music Recognition in the grey-scale domain.pdf:/home/ptorras/zotpapers/Vidal_2012_Optical Music Recognition in the grey-scale domain.pdf:application/pdf},
}

@inproceedings{vieira_recognition_2001,
	title = {Recognition of musical symbols in ancient manuscripts},
	doi = {10.1109/ICIP.2001.958045},
	abstract = {This paper presents a system for the automatic retrieval of music from ancient music collections (XVI-XVIII century), creating digital documents of music from images of music sheets. This is an optical music recognition system that uses image processing and pattern recognition techniques. Finally, we obtain a document that contains the music semantics: description of the notes, in time and pitches, as well as other relevant information.},
	booktitle = {International {Conference} on {Image} {Processing}},
	author = {Vieira, Pedro and Pinto, João Caldas},
	year = {2001},
	keywords = {Image segmentation, Optical character recognition software, Software libraries, Image reconstruction, Bars, Image recognition, optical music recognition, Ordinary magnetoresistance, pattern recognition, Pattern recognition, Labeling, optical character recognition, feature extraction, music, image segmentation, image classification, image processing, Cleaning, digital documents, ancient music collections, automatic music retrieval, Bayes classifier, Bayes methods, best class cluster search, music semantics, pattern clustering},
	pages = {38--41 vol.3},
}

@inproceedings{vigliensoni_automatic_2011,
	address = {Miami, Florida},
	title = {Automatic {Pitch} {Detection} in {Printed} {Square} {Notation}},
	url = {http://ismir2011.ismir.net/papers/PS3-12.pdf},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {University of Miami},
	author = {Vigliensoni, Gabriel and Burgoyne, John Ashley and Hankinson, Andrew and Fujinaga, Ichiro},
	editor = {Klapuri, Anssi and Leider, Colby},
	year = {2011},
	pages = {423--428},
	file = {Vigliensoni et al_2011_Automatic Pitch Detection in Printed Square Notation.pdf:/home/ptorras/zotpapers/Vigliensoni et al_2011_Automatic Pitch Detection in Printed Square Notation.pdf:application/pdf},
}

@inproceedings{vigliensoni_optical_2013,
	address = {Curitiba, Brazil},
	title = {Optical measure recognition in common music notation},
	url = {http://ismir2013.ismir.net/wp-content/uploads/2013/09/207_Paper.pdf},
	booktitle = {14th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Vigliensoni, Gabriel and Burlet, Gregory and Fujinaga, Ichiro},
	year = {2013},
	file = {Vigliensoni et al_2013_Optical measure recognition in common music notation.pdf:/home/ptorras/zotpapers/Vigliensoni et al_2013_Optical measure recognition in common music notation.pdf:application/pdf},
}

@inproceedings{vigliensoni_developing_2018,
	address = {Paris, France},
	title = {Developing an environment for teaching computers to read music},
	url = {https://sites.google.com/view/worms2018/proceedings},
	booktitle = {1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Vigliensoni, Gabriel and Calvo-Zaragoza, Jorge and Fujinaga, Ichiro},
	editor = {Calvo-Zaragoza, Jorge and Hajič jr., Jan and Pacha, Alexander},
	year = {2018},
	pages = {27--28},
}

@inproceedings{vigliensoni_image_2019,
	title = {From image to encoding: {Full} optical music recognition of {Medieval} and {Renaissance} music},
	url = {https://music-encoding.org/conference/2019/abstracts_mec2019/vigliensoni19from%20camera%20ready.pdf},
	booktitle = {Music {Encoding} {Conference}},
	author = {Vigliensoni, Gabriel and Daigle, Alex and Liu, Eric and Calvo-Zaragoza, Jorge and Regimbal, Juliette and Nguyen, Minh Anh and Baxter, Noah and McLennan, Zoé and Fujinaga, Ichiro},
	year = {2019},
}

@inproceedings{viro_peachnote_2011,
	address = {Miami, FL},
	title = {Peachnote: {Music} {Score} {Search} and {Analysis} {Platform}},
	url = {http://ismir2011.ismir.net/papers/PS3-1.pdf},
	abstract = {Our system takes the scores in PDF format, runs optical music recognition (OMR) softwareover them, indexes the data and makes them accessible for querying and data min- ing. Thesearch engine is built upon Hadoop and HBase and runs on a cluster.},
	booktitle = {12th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Viro, Vladimir},
	year = {2011},
	pages = {359--362},
	file = {Viro_2011_Peachnote.pdf:/home/ptorras/zotpapers/Viro_2011_Peachnote.pdf:application/pdf},
}

@inproceedings{visaniy_icdar_2013,
	title = {The {ICDAR} 2013 {Music} {Scores} {Competition}: {Staff} {Removal}},
	doi = {10.1109/ICDAR.2013.284},
	abstract = {The first competition on music scores that was organized at ICDAR in 2011 awoke the interest of researchers, who participated both at staff removal and writer identification tasks. In this second edition, we focus on the staff removal task and simulate a real case scenario: old music scores. For this purpose, we have generated a new set of images using two kinds of degradations: local noise and 3D distortions. This paper describes the dataset, distortion methods, evaluation metrics, the participant's methods and the obtained results.},
	booktitle = {12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Visaniy, Muriel and Kieu, V.C. and Fornés, Alicia and Journet, Nicholas},
	year = {2013},
	note = {ISSN: 1520-5363},
	keywords = {Training, Databases, Noise, Degradation, music, image recognition, Educational institutions, staff removal, writer identification, evaluation metrics, Staff Removal, 3D distortions, Competition, distortion methods, ICDAR 2013 music scores competition, local noise, Music Scores, old music, Solid modeling, Three-dimensional displays},
	pages = {1407--1411},
	file = {Visaniy et al_2013_The ICDAR 2013 Music Scores Competition.pdf:/home/ptorras/zotpapers/Visaniy et al_2013_The ICDAR 2013 Music Scores Competition.pdf:application/pdf},
}

@inproceedings{vo_distorted_2014,
	title = {Distorted music score recognition without {Staffline} removal},
	isbn = {978-1-4799-5208-3},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6977222},
	doi = {10.1109/ICPR.2014.510},
	abstract = {This paper proposes a new approach for recognizing the primitive musical symbols in distorted music scores without the staff line removal. We try to overcome two main issues. The first problem is the difficult and unreliable removal of staff lines required as a pre-processing step for most of recognition systems. The second problem is the non-linear distortion of the music score images captured by digital cameras. At the beginning, we detect the locations of bar-lines on each staff and segment it into sub-areas which can be rectified into undistorted shapes by biquadratic transformation. Then, musical rules, template matching, run length coding and projection methods are employed to extract the musical note information without the application of staff removal. The proposed method is implemented on smart phones and shows promising results. © 2014 IEEE.},
	booktitle = {22nd {International} {Conference} on {Pattern} {Recognition}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Vo, Quang Nhat and Nguyen, Tam and Kim, Soo-Hyung and Yang, Hyung-Jeong and Lee, Guee-Sang},
	year = {2014},
	note = {ISSN: 1051-4651},
	keywords = {Optical music recognition, Pattern recognition, Computer music, Music scores, Recognition systems, Pre-processing step, Smartphones, Template matching, Biquadratic transformations, Image matching, Projection method, Run-length coding, Smart-phone applications, Telephone sets},
	pages = {2956--2960},
}

@article{vo_mrf_2016,
	title = {An {MRF} model for binarization of music scores with complex background},
	volume = {69},
	issn = {0167-8655},
	doi = {10.1016/j.patrec.2015.10.017},
	abstract = {We present a Gaussian Mixture Markov Random Field (GMMRF) model that is effective for the binarization of music score images with complex backgrounds. The binarization of music score documents containing noises with arbitrary shapes and/or non-uniform colors in the background area is a very challenging problem. In order to extract the content knowledge of music score documents, the staff lines are extracted by first applying a stroke width transform. With the color and spatial information of the detected staff lines, we can accurately model the foreground and background color distribution, in which a GMMRF framework is used to make the binarization robust to variations in colors. Then, the staff line information is employed for guiding the GMMRF labeling process. In the experiment, the music score images captured by camera show promising results compared to existing methods.},
	journal = {Pattern Recognition Letters},
	author = {Vo, Quang Nhat and Kim, Soo Hyung and Yang, Hyung Jeong and Lee, Gueesang},
	year = {2016},
	keywords = {Music score binarization},
	pages = {88--95},
}

@article{vo_recognition_2018,
	title = {Recognition of {Music} {Scores} with {Non}-{Linear} {Distortions} in {Mobile} {Devices}},
	volume = {77},
	issn = {1573-7721},
	doi = {10.1007/s11042-017-5169-9},
	abstract = {Optical music recognition (OMR), when the input music score is captured by a handheld or a mobile phone camera, suffers from severe degradation in the image quality and distortions caused by non-planar document curvature and perspective projection. Hence the binarization of the input often fails to preserve the details of the original music score, leading to a poor performance in recognition of music symbols. This paper addresses the issue of staff line detection, which is the most important step in OMR, in the presence of nonlinear distortions and describes how to cope with severe degradations in recognition of music symbols. First, a RANSAC-based detection of curved staff lines is presented and staves are segmented into sub-areas for the rectification with bi-quadratic transformation. Then, run length coding is used to recognize music symbols such as stem, note head, flag, and beam. The proposed system is implemented on smart phones, and it shows promising results with music score images captured in the mobile environment.},
	number = {12},
	journal = {Multimedia Tools and Applications},
	author = {Vo, Quang Nhat and Lee, Guee Sang and Kim, Soo Hyung and Yang, Hyung Jeong},
	year = {2018},
	pages = {15951--15969},
}

@misc{vrist_optical_2009,
	title = {Optical {Music} {Recognition} for structural information from high-quality scanned music},
	author = {Vrist, Søren Bjerregaard},
	year = {2009},
}

@techreport{vuilleumier_stuckelberg_preview_1997,
	address = {Geneva, Switzerland},
	title = {A preview of an architecture for musical score recognition},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.3172&rep=rep1&type=pdf},
	institution = {University of Geneva},
	author = {Vuilleumier Stückelberg, Marc and Pellegrini, Christian and Hillario, Mélanie},
	year = {1997},
	file = {Vuilleumier Stuckelberg et al_1997_A preview of an architecture for musical score recognition.pdf:/home/ptorras/zotpapers/Vuilleumier Stuckelberg et al_1997_A preview of an architecture for musical score recognition.pdf:application/pdf},
}

@inproceedings{vuilleumier_stuckelberg_architecture_1997,
	title = {An architecture for musical score recognition using high-level domain knowledge},
	doi = {10.1109/ICDAR.1997.620624},
	abstract = {Proposes an original approach to musical score recognition, a particular case of high-level document analysis. In order to overcome the limitations of existing systems, we propose an architecture which allows for a continuous and bidirectional interaction between high-level knowledge and low-level data, and which is able to improve itself over time by learning. This architecture is made of three cooperating layers, one made of parameterized feature detectors, another working as an object-oriented knowledge repository and the other as a supervising Bayesian metaprocessor. Although the implementation is still in progress, we show how this architecture is adequate for modeling and processing knowledge.},
	booktitle = {4th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Vuilleumier Stückelberg, Marc and Pellegrini, Christian and Hilario, Mélanie},
	year = {1997},
	keywords = {Image segmentation, Artificial intelligence, Text analysis, Detectors, Pattern recognition, Computer architecture, document image processing, feature extraction, music, Image analysis, image recognition, Computer vision, learning (artificial intelligence), knowledge based systems, Bayes methods, Bayesian methods, continuous bidirectional interaction, cooperating layers, deductive databases, high-level document analysis, high-level domain knowledge, knowledge modelling, knowledge processing, learning, low-level data, musical score recognition architecture, Object oriented modeling, object-oriented databases, object-oriented knowledge repository, parameterized feature detectors, supervising Bayesian metaprocessor},
	pages = {813--818 vol.2},
}

@inproceedings{vuilleumier_stuckelberg_musical_1999,
	title = {On musical score recognition using probabilistic reasoning},
	isbn = {0-7695-0318-7},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=791738},
	doi = {10.1109/ICDAR.1999.791738},
	abstract = {We present a probabilistic framework for document analysis and recognition and illustrate it on the problem of musical score recognition. Our system uses an explicit descriptive model of the document class to find the most likely interpretation of a scanned document image. In contrast to the traditional pipeline architecture, we carry out all stages of the analysis with a single inference engine, allowing for an end-to-end propagation of the uncertainty. The global modeling structure is similar to a stochastic attribute grammar, and local parameters are estimated using hidden Markov models (10 Refs.) image processing; image recognition; inference mechanisms; music; uncertainty handling},
	booktitle = {5th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Vuilleumier Stückelberg, Marc and Doermann, David},
	year = {1999},
	keywords = {musical score recognition, to classify, probabilistic reasoning},
	pages = {115--118},
	file = {Vuilleumier Stuckelberg_Doermann_1999_On musical score recognition using probabilistic reasoning.pdf:/home/ptorras/zotpapers/Vuilleumier Stuckelberg_Doermann_1999_On musical score recognition using probabilistic reasoning.pdf:application/pdf},
}

@mastersthesis{wallner_system_2014,
	title = {A {System} for {Optical} {Music} {Recognition} and {Audio} {Synthesis}},
	url = {https://www.ims.tuwien.ac.at/topics/331/downloads/masterarbeit-wallner.pdf},
	school = {TU Wien},
	author = {Wallner, Matthias},
	year = {2014},
	file = {Wallner_2014_A System for Optical Music Recognition and Audio Synthesis.pdf:/home/ptorras/zotpapers/Wallner_2014_A System for Optical Music Recognition and Audio Synthesis.pdf:application/pdf},
}

@inproceedings{waloschek_identification_2019,
	title = {Identification and {Cross}-{Document} {Alignment} of {Measures} in {Music} {Score} {Images}},
	url = {https://archives.ismir.net/ismir2019/paper/000014.pdf},
	booktitle = {20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Waloschek, Simon and Hadjakos, Aristotelis and Pacha, Alexander},
	year = {2019},
	pages = {137--143},
	file = {Waloschek et al_2019_Identification and Cross-Document Alignment of Measures in Music Score Images.pdf:/home/ptorras/zotpapers/Waloschek et al_2019_Identification and Cross-Document Alignment of Measures in Music Score Images.pdf:application/pdf},
}

@inproceedings{walwadkar_compidnet_2022,
	address = {Online},
	title = {{CompIdNet}: {Sheet} {Music} {Composer} {Identification} using {Deep} {Neural} {Network}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Walwadkar, Dnyanesh and Shatri, Elona and Timms, Benjamin and Fazekas, György},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {9--14},
	file = {Walwadkar et al_2022_CompIdNet.pdf:/home/ptorras/zotpapers/Walwadkar et al_2022_CompIdNet.pdf:application/pdf},
}

@inproceedings{wei_optical_2008,
	title = {Optical {Tablature} {Recognition} ({OTR}) system: {Using} {Fourier} {Descriptors} as a recognition tool},
	doi = {10.1109/ICALIP.2008.4590235},
	abstract = {This paper presents an optical recognition system for the guitar tablature. Images of guitar tablature are fed as input to the system whereby each image undergoes four main stages of processing to produce a music output in MIDI format. Algorithms both existing and self-devised were used. Each input image was first cropped to the desired region, followed by a process for removal of the string lines and detection of the numbers. Recognition of the numbers was carried out using Fourier descriptors based on 8 selected feature points. Once completed, the numbers were matched to their corresponding chords and then rearranged and played. The algorithms and methods used within the system are presented here with a justification on the selection of Fourier descriptors as the recognition tool.},
	booktitle = {International {Conference} on {Audio}, {Language} and {Image} {Processing}},
	author = {Wei, Lee Ling and Salih, Qussay A. and Hock, Ho Sooi},
	year = {2008},
	keywords = {Feature extraction, Music, Shape, music, image recognition, Image edge detection, Pixel, Equations, Fourier descriptor, Fourier transforms, guitar tablature, Mathematical model, MIDI format, musical instruments, optical tablature recognition, recognition tool},
	pages = {1532--1539},
}

@inproceedings{van_der_wel_optical_2017,
	address = {Suzhou, China},
	title = {Optical {Music} {Recognition} with {Convolutional} {Sequence}-to-{Sequence} {Models}},
	isbn = {978-981-11-5179-8},
	url = {https://archives.ismir.net/ismir2017/paper/000069.pdf},
	booktitle = {18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {van der Wel, Eelco and Ullrich, Karen},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/2RW5YLYQ/van der Wel and Ullrich - 2017 - Optical Music Recognition with Convolutional Seque.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/LWFZRGDA/1707.html:text/html;van der Wel_Ullrich_2017_Optical Music Recognition with Convolutional Sequence-to-Sequence Models.pdf:/home/ptorras/zotpapers/van der Wel_Ullrich_2017_Optical Music Recognition with Convolutional Sequence-to-Sequence Models.pdf:application/pdf},
}

@inproceedings{wen_classification_2014,
	title = {Classification of optical music symbols based on combined neural network},
	doi = {10.1109/ICMC.2014.7231590},
	abstract = {In this paper, a new method for music symbol classification named Combined Neural Network (CNN) is proposed. Tests are conducted on more than 9000 music symbols from both real and scanned music sheets, which show that the proposed technique offers superior classification capability. At the same time, the performance of the new network is compared with the single Neural Network (NN) classifier using the same music scores. The average classification accuracy increased more than ten percent, reaching 98.82\%.},
	booktitle = {International {Conference} on {Mechatronics} and {Control}},
	author = {Wen, Cuihong and Rebelo, Ana and Zhang, Jing and Cardoso, Jamie dos Santos},
	year = {2014},
	keywords = {Hidden Markov models, Databases, Accuracy, Artificial neural networks, Integrated optics, optical character recognition, CNN, music, image classification, neural nets, music scores, Biological neural networks, classification accuracy, combined neural network, music sheets, NN classifier, optical music symbols classification},
	pages = {419--423},
	file = {Wen et al_2014_Classification of optical music symbols based on combined neural network.pdf:/home/ptorras/zotpapers/Wen et al_2014_Classification of optical music symbols based on combined neural network.pdf:application/pdf},
}

@article{wen_directed_2016,
	title = {A {Directed} {Acyclic} {Graph}-{Large} {Margin} {Distribution} {Machine} {Model} for {Music} {Symbol} {Classification}},
	volume = {11},
	doi = {10.1371/journal.pone.0149688},
	abstract = {Optical Music Recognition (OMR) has received increasing attention in recent years. In this paper, we propose a classifier based on a new method named Directed Acyclic Graph-Large margin Distribution Machine (DAG-LDM). The DAG-LDM is an improvement of the Large margin Distribution Machine (LDM), which is a binary classifier that optimizes the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously. We modify the LDM to the DAG-LDM to solve the multi-class music symbol classification problem. Tests are conducted on more than 10000 music symbol images, obtained from handwritten and printed images of music scores. The proposed method provides superior classification capability and achieves much higher classification accuracy than the state-of-the-art algorithms such as Support Vector Machines (SVMs) and Neural Networks (NNs).},
	number = {3},
	journal = {PLoS ONE},
	author = {Wen, Cuihong and Zhang, Jing and Rebelo, Ana and Cheng, Fanyong},
	editor = {Ebrahimi, Mansour},
	year = {2016},
	note = {Publisher: Public Library of Science},
	pages = {1--11},
	file = {Wen et al_2016_A Directed Acyclic Graph-Large Margin Distribution Machine Model for Music.pdf:/home/ptorras/zotpapers/Wen et al_2016_A Directed Acyclic Graph-Large Margin Distribution Machine Model for Music.pdf:application/pdf},
}

@inproceedings{wenzlitschke_implementation_2021,
	address = {Alicante, Spain},
	title = {Implementation and evaluation of a neural network for the recognition of handwritten melodies},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Wenzlitschke, Nils},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {38--42},
}

@article{wick_staff_2019,
	title = {Staff, {Symbol} and {Melody} {Detection} of {Medieval} {Manuscripts} {Written} in {Square} {Notation} {Using} {Deel} {Fully} {Convolutional} {Networks}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/13/2646},
	doi = {10.3390/app9132646},
	abstract = {Even today, the automatic digitisation of scanned documents in general, but especially the automatic optical music recognition (OMR) of historical manuscripts, still remains an enormous challenge, since both handwritten musical symbols and text have to be identified. This paper focuses on the Medieval so-called square notation developed in the 11th–12th century, which is already composed of staff lines, staves, clefs, accidentals, and neumes that are roughly spoken connected single notes. The aim is to develop an algorithm that captures both the neumes, and in particular its melody, which can be used to reconstruct the original writing. Our pipeline is similar to the standard OMR approach and comprises a novel staff line and symbol detection algorithm based on deep Fully Convolutional Networks (FCN), which perform pixel-based predictions for either staff lines or symbols and their respective types. Then, the staff line detection combines the extracted lines to staves and yields an F1-score of over 99\% for both detecting lines and complete staves. For the music symbol detection, we choose a novel approach that skips the step to identify neumes and instead directly predicts note components (NCs) and their respective affiliation to a neume. Furthermore, the algorithm detects clefs and accidentals. Our algorithm predicts the symbol sequence of a staff with a diplomatic symbol accuracy rate (dSAR) of about 87\%, which includes symbol type and location. If only the NCs without their respective connection to a neume, all clefs and accidentals are of interest, the algorithm reaches an harmonic symbol accuracy rate (hSAR) of approximately 90\%. In general, the algorithm recognises a symbol in the manuscript with an F1-score of over 96\%.},
	number = {13},
	journal = {Applied Sciences},
	author = {Wick, Christoph and Hartelt, Alexander and Puppe, Frank},
	year = {2019},
	keywords = {fully convolutional neural networks, historical document analysis, medieval manuscripts, neume notation, optical music recognition},
	pages = {2646--2673},
	file = {Wick et al_2019_Staff, Symbol and Melody Detection of Medieval Manuscripts Written in Square.pdf:/home/ptorras/zotpapers/Wick et al_2019_Staff, Symbol and Melody Detection of Medieval Manuscripts Written in Square.pdf:application/pdf},
}

@inproceedings{wick_ommr4all_2019,
	address = {Delft, The Netherlands},
	title = {{OMMR4all} — a {Semiautomatic} {Online} {Editor} for {Medieval} {Music} {Notations}},
	url = {https://sites.google.com/view/worms2019/proceedings},
	booktitle = {2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Wick, Christoph and Puppe, Frank},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2019},
	pages = {31--34},
}

@techreport{wick_automatic_2020,
	title = {Automatic {Neume} {Transcription} of {Medieval} {Music} {Manuscripts} using {CNN}/{LSTM}-{Networks} and the segmentation-free {CTC}-{Algorithm}},
	institution = {University of Würzburg},
	author = {Wick, Christoph and Puppe, Frank},
	year = {2020},
	doi = {10.20944/preprints202001.0149.v1},
	file = {Wick_Puppe_2020_Automatic Neume Transcription of Medieval Music Manuscripts using.pdf:/home/ptorras/zotpapers/Wick_Puppe_2020_Automatic Neume Transcription of Medieval Music Manuscripts using.pdf:application/pdf},
}

@inproceedings{wijaya_staff_1999,
	title = {Staff line restoration},
	doi = {10.1049/cp:19990426},
	abstract = {Optical music recognition (OMR), the conversion of scanned pages of music into a musical database, has reached an exciting level of maturity. Like optical character recognition, it has now reached the point where the returns in accuracy from increasingly sophisticated pattern recognition algorithms appears saturated and more significant gains are being made from the application of structured a priori knowledge. This paper describes one such technique for improved staff line processing-the detection and subsequent correction of bowing in the staff lines, which is an important category given the significant source of music in book form. Two versions of the algorithm are tested: the first, based on mathematical morphology, has the added benefit of automatically fusing small breaks in staff lines, common for example in older works; the second, based on a flood-fill algorithm, requires a minor modification if fragmented staff lines are to be repaired. The correct detection and processing of staff lines is fundamental to OMR. Without adequate knowledge of staff line location, notation superimposed on the staves cannot be correctly separated, classified and processed.},
	booktitle = {7th {International} {Conference} on {Image} {Processing} and its {Applications}},
	publisher = {Institution of Engineering and Technology},
	author = {Wijaya, K. and Bainbridge, David},
	year = {1999},
	keywords = {optical music recognition, optical character recognition, music, mathematical morphology, bowing correction, bowing detection, flood-fill algorithm, fragmented staff lines repair, machine readable notation, musical database, pattern recognition algorithms, scanned pages conversion, staff line location, staff line processing, staff line restoration, structured a priori knowledge},
	pages = {760--764},
}

@misc{witt_optical_2013,
	title = {Optical {Music} {Recognition} {Symbol} {Detection} using {Contour} {Traces}},
	abstract = {A novel approach to symbol detection in optical music recognition is presented. The binarized image of a scanned score is transformed into an intermediate representation by computing its contours and assigning additional visual features to them. The resulting contour points are accessed via a high dimensional spatial index that aids a heuristic search to detect a given symbol as described by a template image. An automatic and a manual method for generating ground truth data are presented, amongst other web-based tools to evaluate and supervise the recognition process.},
	publisher = {Freie Universität Berlin},
	author = {Witt, Carl},
	year = {2013},
	note = {Backup Publisher: Institut für Informatik
Type: Bachelor Thesis},
}

@inproceedings{wolman_recognition_1992,
	title = {Recognition of {Handwritten} {Music} {Notation}},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Wolman, Amnon and Choi, James and Asgharzadeh, Shahab and Kahana, Jason},
	year = {1992},
}

@inproceedings{wu_evaluation_2016,
	title = {An {Evaluation} {Framework} of {Optical} {Music} {Recognition} in {Numbered} {Music} {Notation}},
	doi = {10.1109/ISM.2016.0134},
	abstract = {In this study, we refine the ecosystem for optical music recognition (OMR) of numbered music notation with better accuracy. The ecosystem includes users, OMR system, dataset of music scores, groundtruth building, symbolic representation of sheet music, checking by musicological rules and performance evaluation. Especially, the evaluation metric includes exact and approximate approach to count accuracy automatically. The hands-on dataset comprises of 110 music score manuscripts in a songbook for singing reference. The experimental results justify the value of evaluation framework and show the necessity of checks complying with musicological properties.},
	booktitle = {International {Symposium} on {Multimedia}},
	author = {Wu, Fu-Hai Frank},
	year = {2016},
	keywords = {Optical imaging, Optical music recognition, Music, Bars, Semantics, optical music recognition, dataset, Performance evaluation, OMR, music, evaluation, acoustic signal processing, Adaptive optics, groundtruth, groundtruth building, music score dataset, music score manuscripts, musicological rules, numbered music notation, sheet music symbolic representation, singing reference},
	pages = {626--631},
}

@incollection{wu_applying_2017,
	title = {Applying {Machine} {Learning} in {Optical} {Music} {Recognition} of {Numbered} {Music} {Notation}},
	abstract = {Although research of optical music recognition (OMR) has existed for few decades, most of efforts were put in step of image processing to approach upmost accuracy and evaluations were not in common ground. And major music notations explored were the conventional western music notations with staff. On contrary, the authors explore the challenges of numbered music notation, which is popular in Asia and used in daily life for sight reading. The authors use different way to improve recognition accuracy by applying elementary image processing with rough tuning and supplementing with methods of machine learning. The major contributions of this work are the architecture of machine learning specified for this task, the dataset, and the evaluation metrics, which indicate the performance of OMR system, provide objective function for machine learning and highlight the challenges of the scores of music with the specified notation.},
	booktitle = {International {Journal} of {Multimedia} {Data} {Engineering} and {Management}},
	publisher = {IGI Global},
	author = {Wu, Fu-Hai Frank},
	year = {2017},
	doi = {10.4018/IJMDEM.2017070102},
	pages = {21},
}

@article{xiao_real-time_2019,
	title = {Real-{Time} {Optical} {Music} {Recognition} {System} for {Dulcimer} {Musical} {Robot}},
	volume = {23},
	doi = {10.20965/jaciii.2019.p0782},
	abstract = {Traditional optical music recognition (OMR) is an important technology that automatically recognizes scanned paper music sheets. In this study, traditional OMR is combined with robotics, and a real-time OMR system for a dulcimer musical robot is proposed. This system gives the musical robot a stronger ability to perceive and understand music. The proposed OMR system can read music scores, and the recognized information is converted into a standard electronic music file for the dulcimer musical robot, thus achieving real-time performance. During the recognition steps, we treat note groups and isolated notes separately. Specially structured note groups are identified by primitive decomposition and structural analysis. The note groups are decomposed into three fundamental elements: note stem, note head, and note beams. Isolated music symbols are recognized based on shape model descriptors. We conduct tests on real pictures taken live by a camera. The tests show that the proposed method has a higher recognition rate.},
	number = {4},
	journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
	author = {Xiao, Zhe and Chen, Xin and Zhou, Li},
	year = {2019},
	pages = {782--790},
}

@article{yadid-pecht_recognition_1996,
	title = {Recognition of handwritten musical notes by a modified {Neocognitron}},
	volume = {9},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/BF01214361},
	doi = {10.1007/BF01214361},
	abstract = {A neural network for recognition of handwritten musical notes, based on the well-known Neocognitron model, is described. The Neocognitron has been used for the “what” pathway (symbol recognition), while contextual knowledge has been applied for the “where” (symbol placement). This way, we benefit from dividing the process for dealing with this complicated recognition task. Also, different degrees of intrusiveness in “learning” have been incorporated in the same network: More intrusive supervised learning has been implemented in the lower neuron layers and less intrusive in the upper one. This way, the network adapts itself to the handwriting of the user. The network consists of a 13x49 input layer and three pairs of “simple” and “complex” neuron layers. It has been trained to recognize 20 symbols of unconnected notes on a musical staff and was tested with a set of unlearned input notes. Its recognition rate for the individual unseen notes was up to 93\%, averaging 80\% for all categories. These preliminary results indicate that a modified Neocognitron could be a good candidate for identification of handwritten musical notes.},
	number = {2},
	journal = {Machine Vision and Applications},
	author = {Yadid-Pecht, Orly and Gerner, Moty and Dvir, Lior and Brutman, Eliyahu and Shimony, Uri},
	year = {1996},
	pages = {65--72},
}

@inproceedings{yin_transcribing_2018,
	address = {London, United Kingdom},
	title = {Transcribing {Content} from {Structural} {Images} with {Spotlight} {Mechanism}},
	isbn = {978-1-4503-5552-0},
	url = {http://doi.acm.org/10.1145/3219819.3219962},
	doi = {10.1145/3219819.3219962},
	booktitle = {24th {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Yin, Yu and Huang, Zhenya and Chen, Enhong and Liu, Qi and Zhang, Fuzheng and Xie, Xing and Hu, Guoping},
	year = {2018},
	keywords = {reinforcement learning, spotlight transcribing network, structural image},
	pages = {2643--2652},
	file = {Yin et al_2018_Transcribing Content from Structural Images with Spotlight Mechanism.pdf:/home/ptorras/zotpapers/Yin et al_2018_Transcribing Content from Structural Images with Spotlight Mechanism.pdf:application/pdf},
}

@inproceedings{yin-xian_staff_2012,
	title = {Staff {Line} {Removal} {Algorithm} {Based} on {Trajectory} {Tracking} and {Topological} {Structure} of {Score}},
	abstract = {Staff line removal plays a vital role in OMR technology, and is the preconditions of succeeding segmentation \& recognition of music sheets. For the phenomena of over-deletion or mistaken deletion and under-deletion which often appear in removal process of staff lines, a novel staff line removal algorithm based on tra1jectory tracking and topological structure of music symbols is put forward to solve the deletion faults of partial notions, Experimental results show the presented algorithms can remove staff lines fast and effectively.},
	booktitle = {4th {International} {Conference} on {Computer} {Modeling} and {Simulation}},
	author = {Yin-xian, Yang and Ding-li, Yang},
	year = {2012},
	keywords = {OMR, Staff Line Removal, Correlation Computation, Subsection Projection},
}

@inproceedings{yoda_automatic_1995,
	title = {Automatic {Construction} of {Recognition} {Procedures} for {Musical} {Notes} by {Genetic} {Algorithm}},
	doi = {10.1142/9789812797933},
	abstract = {The Table of Contents for the full book PDF is as follows: System Architecture Data Structures for Page Readers Palace: A Multilingual Document Recognition System Experiences with High-Volume, High Accuracy Document Capture OfficeMAID - A System for Office Mail Analysis, Interpretation and Delivery Programmable Contextual Analysis A System for Exploiting Context in Automatic Recognition An Adaptive Approach to Document Classification and Understanding Class Evaluation Document Image Analysis: Automated Performance Evaluation Using Consensus Sequence Voting to Correct OCR Errors A Handwritten Character Recognition System by Efficient Combination of Multiple Classifiers A Region-Based System for the Automatic Evaluation of Page Segmentation Algorithms Integration of Contextual Knowledge Sources into a Blackboard-Based Text Recognition System Automatic Construction of Recognition Procedures for Musical Notes by Genetic Algorithm Recognition of Handwritten Responses on US Census Forms A System for the Recognition of Handwritten Literal Amounts of Checks Handwritten Text Recognition Line Drawing Knowledge Organization and Interpretation Process in Engineering Drawing Interpretation Processing Imprecise and Structural Distorted Line Drawings by An Adaptable Drawing Interpretation Kernal Vector-Based Arc Segmentation in the Machine Drawing Understanding System Environment Robust Drawing Recognition Based on Model-Guided Segmentation Innovations Document Image Matching and Retrieval with Multiple Distortion-Invariant Descriptors Off-Line Interpretation and Execution of Corrections on Text Documents Analysis of Scanned Braille Documents Document Analysis by Fractal Signatures Working Groups Possibilities for International Collaboration Document Analysis and Learning Needs of the Market and User Requirements Evaluation-Criteria Handwriting Line Drawing and Music Recognition Multilingual Documents and Natural Language Processing Form Recognition},
	booktitle = {Document {Analysis} {Systems}},
	author = {Yoda, Ikushi and Yamamoto, Kazuhiko and Yamada, Hiromitsu},
	editor = {Spitz, A. Lawrence and Dengel, Andreas},
	year = {1995},
}

@inproceedings{yoo_advanced_2008,
	title = {Advanced {Binarization} {Method} for {Music} {Score} {Recognition} {Using} {Local} {Thresholds}},
	doi = {10.1109/CIT.2008.Workshops.101},
	abstract = {Application technology of mobile phone has been developing for the delivery of various contents over a simple voice channel. Music score recognition is one of such application services provided by mobile phone manufacturers which transform a music score taken by the phone camera into a midi file. For the successful recognition of the music score, the input image should be properly binarized to be fed into the recognition process. In this paper, Adaptive binary algorithm is proposed which exploits local thresholds with several levels to deal with illumination changes over the entire image. Experimental results shown advanced performance of music score recognition.},
	booktitle = {8th {International} {Conference} on {Computer} and {Information} {Technology} {Workshops}},
	author = {Yoo, JaeMyeong and Toan, Nguyen Dinh and Choi, DeokJai and Park, HyukRo and Lee, Gueesang},
	year = {2008},
	keywords = {Computer science, Image recognition, Speech recognition, music, Application software, music score recognition, Histograms, local threshold, adaptive binary algorithm, Binarization, Brightness, Conferences, Frequency, Information technology, mobile computing, Mobile handsets, mobile phone},
	pages = {417--420},
}

@inproceedings{zalkow_tools_2019,
	address = {Delft, The Netherlands},
	title = {Tools {For} {Semi}-{Automatic} {Bounding} {Box} {Annotation} {Of} {Musical} {Measures} {In} {Sheet} {Music}},
	booktitle = {Late {Breaking}/{Demo} at 20th {International} {Society} for {Music} {Information} {Retrieval}},
	author = {Zalkow, Frank and Corrales, Angel Villar and Tsai, TJ and Arifi-Müller, Vlora and Müller, Meinard},
	year = {2019},
}

@mastersthesis{zhang_efficient_2017,
	title = {An {Efficient} {Score} {Alignment} {Algorithm} and its {Applications}},
	url = {http://hdl.handle.net/1721.1/113457},
	abstract = {String alignment and comparison in Computer Science is a well-explored space with classic problems such as Longest Common Subsequence that have practical application in bioinformatic genomic sequencing and data comparison in revision control systems. In the field of musicology, score alignment and comparison is a problem with many similarities to string comparison and alignment but also vast differences. In particular we can use ideas in string alignment and comparison to compare a music score in the MIDI format with a music score generated from Optical Musical Recognition (OMR), both of which have incomplete or wrong information, and correct errors that were introduced in the OMR process to create an improved third score. This thesis creates a set of algorithms that align and compare MIDI and OMR music scores to produce a corrected version of the OMR score that borrows ideas from classic computer science string comparison and alignment algorithm but also incorporates optimizations and heuristics from music theory.},
	school = {Massachusetts Institute of Technology},
	author = {Zhang, Emily H.},
	year = {2017},
	file = {Zhang_2017_An Efficient Score Alignment Algorithm and its Applications.pdf:/home/ptorras/zotpapers/Zhang_2017_An Efficient Score Alignment Algorithm and its Applications.pdf:application/pdf},
}


@misc{noauthor_beethoven-haus_nodate,
	title = {Beethoven-{Haus} {Bonn}},
	url = {https://www.beethoven.de/en/archive/list},
	abstract = {Das Beethoven-Haus in Bonn ist Gedächtnisstätte, Museum und Kulturinstitut mit vielfältigen Aufgaben. 1889 vom Verein Beethoven-Haus gegründet, verbinden sich hier die Person von Ludwig van Beethoven mit der Pflege seiner Musik und der Erforschung von Leben und Werk des Komponisten.},
	language = {en},
	urldate = {2023-02-15},
	journal = {Archiv - Beethoven-Haus Bonn},
	file = {Snapshot:/home/ptorras/Zotero/storage/PXXB2L79/list.html:text/html},
    year = {2023},
    note = {Accessed: 2023-03-01},
}

@misc{noauthor_bach_nodate,
	title = {Bach {Digital}},
	url = {https://www.bach-digital.de/content/index.xed},
	urldate = {2023-02-15},
	file = {Bach digital - Start:/home/ptorras/Zotero/storage/NP3V9Q7T/index.html:text/html},
    year = {2008},
    note = {Accessed: 2023-03-01},
}

@misc{noauthor_musescore_nodate,
	title = {MuseScore},
	url = {https://musescore.com/},
	urldate = {2022-12-12},
    year = {2023},
    note = {Accessed: 2022-12-12},
}

@misc{stringquartet_corpus,
	title = {String Quartet Corpus},
	url = {https://github.com/OpenScore/StringQuartets},
	urldate = {2023-11-13},
    year = {2023},
    note = {Accessed: 2023-10-10},
}


@mastersthesis{mayer_semi-supervised_2022,
	title = {Semi-supervised learning in {Optical} {Music} {Recognition}},
	url = {https://dspace.cuni.cz/handle/20.500.11956/173547},
	abstract = {Optical music recognition (OMR) is a niche subfield of computer vision, where some labeled datasets exist, but there is an order of magnitude more unlabeled data available. Recent advances in the field happened largely thanks to the adoption of deep learning. However, such neural networks are trained using labeled data only. Semi-supervised learning is a set of techniques that aim to incorporate unlabeled data during training to produce more capable models. We have modified a state-of-the-art object detection archi- tecture and designed a semi-supervised training scheme to utilize unlabeled data. These modifications have successfully allowed us to train the architecture in an unsupervised setting, and our semi-supervised experiments indicate improvements to training stability and reduced overfitting. 1},
	language = {en\_US},
	urldate = {2023-03-03},
	author = {Mayer, Jiří},
	month = jun,
	year = {2022},
	note = {Accepted: 2022-06-28T10:48:03Z
Publisher: Univerzita Karlova, Matematicko-fyzikální fakulta},
	file = {Mayer_2022_Semi-supervised learning in Optical Music Recognition.pdf:/home/ptorras/zotpapers/Mayer_2022_Semi-supervised learning in Optical Music Recognition.pdf:application/pdf},
}


@inproceedings{mayer_synthesizing_2021,
	address = {Cham},
	title = {Synthesizing {Training} {Data} for {Handwritten} {Music} {Recognition}},
	volume = {12823},
	isbn = {978-3-030-86333-3 978-3-030-86334-0},
	url = {https://link.springer.com/10.1007/978-3-030-86334-0\_41},
	abstract = {Handwritten music recognition is a challenging task that could be of great use if mastered, e.g., to improve the accessibility of archival manuscripts or to ease music composition. Many modern machine learning techniques, however, cannot be easily applied to this task because of the limi‘ted availability of high-quality training data. Annotating such data manually is expensive and thus not feasible at the necessary scale. This problem has already been tackled in other ﬁelds by training on automatically generated synthetic data. We bring this approach to handwritten music recognition and present a method to generate synthetic handwritten music images (limited to monophonic scores) and show that training on such data leads to state-of-the-art results.},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {Document {Analysis} and {Recognition} – {ICDAR} 2021},
	publisher = {Springer International Publishing},
	author = {Mayer, Jiří and Pecina, Pavel},
	editor = {Lladós, Josep and Lopresti, Daniel and Uchida, Seiichi},
	year = {2021},
	doi = {10.1007/978-3-030-86334-0\_41},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {626--641},
	file = {Mayer and Pecina - 2021 - Synthesizing Training Data for Handwritten Music R.pdf:/home/ptorras/Zotero/storage/L3H9F7L4/Mayer and Pecina - 2021 - Synthesizing Training Data for Handwritten Music R.pdf:application/pdf},
}


@inproceedings{he_masked_2022,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/He\_Masked\_Autoencoders\_Are\_Scalable\_Vision\_Learners\_CVPR\_2022\_paper.html},
	language = {en},
	urldate = {2022-11-07},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	year = {2022},
	keywords = {\_tablet},
	pages = {16000--16009},
	file = {He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf:/home/ptorras/zotpapers/He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/RS8TQK8T/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html:text/html},
}


@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-03-03},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:/home/ptorras/zotpapers/Chen et al_2020_A Simple Framework for Contrastive Learning of Visual Representations.pdf:application/pdf;Supplementary PDF:/home/ptorras/Zotero/storage/3G5CDQW8/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{yu_1st_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The 1st {Tiny} {Object} {Detection} {Challenge}: {Methods} and {Results}},
	isbn = {978-3-030-68238-5},
	shorttitle = {The 1st {Tiny} {Object} {Detection} {Challenge}},
	doi = {10.1007/978-3-030-68238-5_23},
	abstract = {The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in developing novel and accurate methods for tiny object detection in images which have wide views, with a current focus on tiny person detection. The TinyPerson dataset was used for the TOD Challenge and is publicly released. It has 1610 images and 72651 box-level annotations. Around 36 participating teams from the globe competed in the 1st TOD Challenge. In this paper, we provide a brief summary of the 1st TOD Challenge including brief introductions to the top three methods.The submission leaderboard will be reopened for researchers that are interested in the TOD challenge. The benchmark dataset and other information can be found at: https://github.com/ucas-vg/TinyBenchmark.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Yu, Xuehui and Han, Zhenjun and Gong, Yuqi and Jan, Nan and Zhao, Jian and Ye, Qixiang and Chen, Jie and Feng, Yuan and Zhang, Bin and Wang, Xiaodi and Xin, Ying and Liu, Jingwei and Mao, Mingyuan and Xu, Sheng and Zhang, Baochang and Han, Shumin and Gao, Cheng and Tang, Wei and Jin, Lizuo and Hong, Mingbo and Yang, Yuchao and Li, Shuiwang and Luo, Huan and Zhao, Qijun and Shi, Humphrey},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	keywords = {Tiny Object Detection, Visual recognition},
	pages = {315--323},
	file = {Yu et al_2020_The 1st Tiny Object Detection Challenge.pdf:/home/ptorras/zotpapers/Yu et al_2020_The 1st Tiny Object Detection Challenge.pdf:application/pdf},
}

@inproceedings{zhang_multi-scale_2018,
	title = {Multi-{Scale} {Attention} with {Dense} {Encoder} for {Handwritten} {Mathematical} {Expression} {Recognition}},
	doi = {10.1109/ICPR.2018.8546031},
	abstract = {Handwritten mathematical expression recognition is a challenging problem due to the complicated two-dimensional structures, ambiguous handwriting input and variant scales of handwritten math symbols. To settle this problem, recently we propose the attention based encoder-decoder model that recognizes mathematical expression images from two-dimensional layouts to one-dimensional LaTeX strings. In this study, we improve the encoder by employing densely connected convolutional networks as they can strengthen feature extraction and facilitate gradient propagation especially on a small training set. We also present a novel multi-scale attention model which is employed to deal with the recognition of math symbols in different scales and restore the fine-grained details dropped by pooling operations. Validated on the CROHME competition task, the proposed method significantly outperforms the state-of-the-art methods with an expression recognition accuracy of 52.8\% on CROHME 2014 and 50.1\% on CROHME 2016, by only using the official training dataset.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Zhang, Jianshu and Du, Jun and Dai, Lirong},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Cats, Convolution, Decoding, Feature extraction, Handwriting recognition, Mathematical model, Training},
	pages = {2245--2250},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/U8MRB8PR/8546031.html:text/html;Zhang et al_2018_Multi-Scale Attention with Dense Encoder for Handwritten Mathematical.pdf:/home/ptorras/zotpapers/Zhang et al_2018_Multi-Scale Attention with Dense Encoder for Handwritten Mathematical.pdf:application/pdf},
}

@article{zhang_simple_1989,
	title = {Simple {Fast} {Algorithms} for the {Editing} {Distance} between {Trees} and {Related} {Problems}},
	volume = {18},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/abs/10.1137/0218082},
	doi = {10.1137/0218082},
	abstract = {Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions:1. What is the distance between two trees? 2. What is the minimum distance between 
T
1
��
1
 and 
T
2
��
2
 when zero or more subtrees can be removed from 
T
2
��
2
? 3. Let the pruning of a tree at node n mean removing all the descendants of node n. The analogous question for prunings as for subtrees is answered.

A dynamic programming algorithm is presented to solve the three questions in sequential time 
O({\textbar}
T
1
{\textbar}×{\textbar}
T
2
{\textbar}×min(depth(
T
1
),leaves(
T
1
))×min(depth(
T
2
),leaves(
T
2
)))
��
(
{\textbar}
��
1
{\textbar}
×
{\textbar}
��
2
{\textbar}
×
min
(
depth
(
��
1
)
,
leaves
(
��
1
)
)
×
min
(
depth
(
��
2
)
,
leaves
(
��
2
)
)
)
 and space 
O({\textbar}
T
1
{\textbar}×{\textbar}
T
2
{\textbar})
��
(
{\textbar}
��
1
{\textbar}
×
{\textbar}
��
2
{\textbar}
)
 compared with 
O({\textbar}
T
1
{\textbar}×{\textbar}
T
2
{\textbar}×(depth(
T
1
)
)
2
×(depth(
T
2
)
)
2
)
��
(
{\textbar}
��
1
{\textbar}
×
{\textbar}
��
2
{\textbar}
×
(
depth
(
��
1
)
)
2
×
(
depth
(
��
2
)
)
2
)
 for the best previous published algorithm due to Tai [J. Assoc. Comput. Mach., 26 (1979), pp, 422-433]. Further, the algorithm presented here can be parallelized to give time 
O({\textbar}
T
1
{\textbar}×{\textbar}
T
2
{\textbar})
��
(
{\textbar}
��
1
{\textbar}
×
{\textbar}
��
2
{\textbar}
)
.},
	number = {6},
	urldate = {2023-01-31},
	journal = {SIAM J. Comput.},
	author = {Zhang, Kaizhong and Shasha, Dennis},
	month = dec,
	year = {1989},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {\_tablet, pattern recognition, 68P05, 68Q20, 68Q25, 68R10, dynamic programming, editing distance, parallel algorithm, trees},
	pages = {1245--1262},
	file = {Zhang_Shasha_1989_Simple Fast Algorithms for the Editing Distance between Trees and Related.pdf:/home/ptorras/zotpapers/Zhang_Shasha_1989_Simple Fast Algorithms for the Editing Distance between Trees and Related.pdf:application/pdf},
}


@article{bertolami_hidden_2008,
	title = {Hidden {Markov} model-based ensemble methods for offline handwritten text line recognition},
	volume = {41},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320308001349},
	doi = {10.1016/j.patcog.2008.04.003},
	abstract = {This paper investigates various ensemble methods for offline handwritten text line recognition. To obtain ensembles of recognisers, we implement bagging, random feature subspace, and language model variation methods. For the combination, the word sequences returned by the individual ensemble members are first aligned. Then a confidence-based voting strategy determines the final word sequence. A number of confidence measures based on normalised likelihoods and alternative candidates are evaluated. Experiments show that the proposed ensemble methods can improve the recognition accuracy over an optimised single reference recogniser.},
	language = {en},
	number = {11},
	urldate = {2023-03-14},
	journal = {Pattern Recognition},
	author = {Bertolami, Roman and Bunke, Horst},
	month = nov,
	year = {2008},
	keywords = {Confidence measures, Ensemble methods, Offline handwritten text line recognition},
	pages = {3452--3460},
	file = {ScienceDirect Snapshot:/home/ptorras/Zotero/storage/FT58SV42/S0031320308001349.html:text/html},
}


@article{tuggener_real_2023,
	title = {Real world music object recognition},
	copyright = {Licence according to publishing contract},
	issn = {2514-3298},
	url = {https://digitalcollection.zhaw.ch/handle/11475/28644},
	doi = {10.21256/zhaw-28644},
	abstract = {We present solutions to two of the most pressing issues in contemporary optical music recognition (OMR).We improve recognition accuracy on low-quality, real-world (i.e. containing ageing, lighting, or dirt artefacts among others) input data and provide confidence-rated model outputs to enable efficient human post-processing. Specifically, we present (i) a sophisticated input augmentation scheme that can reduce the gap between sanitised benchmarks and realistic tasks through a combination of synthetic data and noisy perturbations of real-world documents; (ii) an adversarial discriminative domain adaptation method that can be employed to improve the performance of OMR systems on low-quality data; (iii) a combination of model ensembles and prediction fusion, which generates trustworthy confidence ratings for each prediction. We evaluate our contributions on a newly created test set consisting of manually annotated pages of varying real-world quality, sourced from International Music Score Library Project (IMSLP) / the Petrucci Music Library. With the presented data augmentation scheme, we achieve a doubling in detection performance from 36.0\% to 73.3\% on noisy real-world data compared to state-of-the-art training. This result is then combined with robust confidence ratings paving the way forOMR to be deployed in the realworld. Additionally, we showthe merits of unsupervised adversarial domain adaptation for OMR raising the 36.0\% baseline to 48.9\%. All our code and data are freely available at: https://github.com/raember/s2anet/tree/TISMIR\_publication.},
	language = {en},
	urldate = {2023-09-12},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Tuggener, Lukas and Emberger, Raphael and Ghosh, Adhiraj and Sager, Pascal and Satyawan, Yvan Putra and Montoya, Javier and Goldschagg, Simon and Seibold, Florian and Gut, Urs and Ackermann, Philipp and Schmidhuber, Jürgen and Stadelmann, Thilo},
	month = sep,
	year = {2023},
	note = {Accepted: 2023-09-08T13:52:37Z
Publisher: Ubiquity Press},
	file = {Tuggener et al_2023_Real world music object recognition.pdf:/home/ptorras/zotpapers/Tuggener et al_2023_Real world music object recognition.pdf:application/pdf},
}

@article{rios-vila_end--end_2023,
	title = {End-to-end optical music recognition for pianoform sheet music},
	volume = {26},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-023-00432-z},
	doi = {10.1007/s10032-023-00432-z},
	abstract = {End-to-end solutions have brought about significant advances in the field of Optical Music Recognition. These approaches directly provide the symbolic representation of a given image of a musical score. Despite this, several documents, such as pianoform musical scores, cannot yet benefit from these solutions since their structural complexity does not allow their effective transcription. This paper presents a neural method whose objective is to transcribe these musical scores in an end-to-end fashion. We also introduce the GrandStaff dataset, which contains 53,882 single-system piano scores in common western modern notation. The sources are encoded in both a standard digital music representation and its adaptation for current transcription technologies. The method proposed in this paper is trained and evaluated using this dataset. The results show that the approach presented is, for the first time, able to effectively transcribe pianoform notation in an end-to-end manner.},
	language = {en},
	number = {3},
	urldate = {2023-09-27},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Ríos-Vila, Antonio and Rizo, David and Iñesta, José M. and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2023},
	keywords = {Neural networks, Optical music recognition, GrandStaff, Polyphonic music scores},
	pages = {347--362},
	file = {Rios-Vila et al_2023_End-to-end optical music recognition for pianoform sheet music.pdf:/home/ptorras/zotpapers/Rios-Vila et al_2023_End-to-end optical music recognition for pianoform sheet music2.pdf:application/pdf},
}
@inproceedings{GothamJonas2022,
 abstract = {The OpenScore Lieder Corpus is a collection of over 1,200 nineteenth century songs encoded by a dedicated team of mostly volunteers over several years. Having reported on the initial phase, motivations, design, and community-oriented aspects of the project before, we present here the first, stable, large-scale release of this corpus specifically designed for MIR researchers, complete with comprehensive, structured, linked metadata. The corpus continues to be available under the open CC0 licence and represents a compelling dataset for a range of MIR tasks, not least given its unusual balance of large-scale with high-quality encoding, and of diversity (songs by over 100 composers, from many countries, and in a range of languages) with unity (centred on the nineteenth-century lieder tradition).},
 author = {Gotham, Mark Robert Haigh and Jonas, Peter},
 title = {{The OpenScore Lieder Corpus}},
 keywords = {mec-proceedings, mec-proceedings-2021},
 pages = {131--136},
 publisher = {{Humanities Commons}},
 isbn = {978-84-1302-173-7},
 editor = {M{\"u}nnich, Stefan and Rizo, David},
 booktitle = {{Music Encoding Conference Proceedings 2021}},
 year = {2022},
 doi = {10.17613/1my2-dm23},
 bibbase_note = {<span style="color: green; font-weight: bold">Best Poster Award.</span>},
 displayby = {Contributions from MEC 2021}
}

@article{pawlikTreeEditDistance2016a,
  title = {Tree Edit Distance: {{Robust}} and Memory-Efficient},
  shorttitle = {Tree Edit Distance},
  author = {Pawlik, Mateusz and Augsten, Nikolaus},
  year = {2016},
  month = mar,
  journal = {Information Systems},
  volume = {56},
  pages = {157--173},
  issn = {0306-4379},
  doi = {10.1016/j.is.2015.08.004},
  urldate = {2024-03-12},
  abstract = {Hierarchical data are often modelled as trees. An interesting query identifies pairs of similar trees. The standard approach to tree similarity is the tree edit distance, which has successfully been applied in a wide range of applications. In terms of runtime, the state-of-the-art algorithm for the tree edit distance is RTED, which is guaranteed to be fast independent of the tree shape. Unfortunately, this algorithm requires up to twice the memory of its competitors. The memory is quadratic in the tree size and is a bottleneck for the tree edit distance computation. In this paper we present a new, memory efficient algorithm for the tree edit distance, AP-TED (All Path Tree Edit Distance). Our algorithm runs at least as fast as RTED without trading in memory efficiency. This is achieved by releasing memory early during the first step of the algorithm, which computes a decomposition strategy for the actual distance computation. We show the correctness of our approach and prove an upper bound for the memory usage. The strategy computed by AP-TED is optimal in the class of all-path strategies, which subsumes the class of LRH strategies used in RTED. We further present the AP-TED+ algorithm, which requires less computational effort for very small subtrees and improves the runtime of the distance computation. Our experimental evaluation confirms the low memory requirements and the runtime efficiency of our approach.},
  keywords = {Approximate matching,Similarity search,Tree edit distance},
  file = {/home/ptorras/Zotero/storage/M77PQVJC/S0306437915001611.html}
}

@misc{JoaoFelipeAptedPython,
  title = {{{JoaoFelipe}}/Apted: {{Python APTED}} Algorithm for the {{Tree Edit Distance}}},
  year = {2017},
  urldate = {2024-03-12},
  howpublished = {https://github.com/JoaoFelipe/apted/tree/master},
  note = {Accessed: 2024-03-10}
}


@misc{edirisooriyaEmpiricalEvaluationEndtoEnd2021,
  title = {An {{Empirical Evaluation}} of {{End-to-End Polyphonic Optical Music Recognition}}},
  author = {Edirisooriya, Sachinda and Dong, Hao-Wen and McAuley, Julian and {Berg-Kirkpatrick}, Taylor},
  year = {2021},
  month = aug,
  number = {arXiv:2108.01769},
  eprint = {2108.01769},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-01-17},
  abstract = {Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR---one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoderdecoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models---FlagDecoder and RNNDecoder---that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/ptorras/Zotero/storage/6MX8I5BR/Edirisooriya et al. - 2021 - An Empirical Evaluation of End-to-End Polyphonic O.pdf}
}


@inproceedings{foscarinDiffProcedureMusic2019,
  title = {A Diff Procedure for Music Score Files},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Digital Libraries}} for {{Musicology}}},
  author = {Foscarin, Francesco and Jacquemard, Florent and {Fournier-S'niehotta}, Rapha{\"e}l},
  year = {2019},
  month = nov,
  series = {{{DLfM}} '19},
  pages = {58--64},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3358664.3358671},
  urldate = {2024-01-17},
  abstract = {Comparing music score files is an important task for many activities such as collaborative score editing, version control and evaluation of optical music recognition (OMR) or music transcription. Following the Unix diff model for text files, we propose an original procedure for computing the differences between two score files, typically in XML format. It performs a comparison of scores at the notation (graphical) level, based on a new intermediate tree representation of the music notation content of a score and a combination of sequence- and tree-edit distances. We also propose a tool to visualize the differences between two scores side-by-side, using the music notation engraving library Verovio, and we employ it to test the procedure on an OMR dataset.},
  isbn = {978-1-4503-7239-8},
  file = {/home/ptorras/Zotero/storage/894S98EF/Foscarin et al_2019_A diff procedure for music score files.pdf}
}

@misc{HumdrumToolkitComputational,
  title = {The {{Humdrum Toolkit}} for {{Computational Music Analysis}} {\textbar} {{Humdrum}}},
  author = {Huron, David},
  urldate = {2024-03-14},
  howpublished = {https://www.humdrum.org/},
  note = {Accessed: 2024-03-14},
  year = {20??},
}

@misc{Verovio,
  title = {Verovio, a music notation engraving library},
  author = {Pugin, Laurent},
  urldate = {2024-03-14},
  abstract = {Music notation engraving library for MEI with MusicXML and Humdrum support and various toolkits (JavaScript, Python)},
  howpublished = {https://www.verovio.org/},
  langid = {english},
  note = {Accessed: 2024-03-14}, 
  year = {20??},
}

@misc{Audiveris,
  title = {Audiveris - Open-source Optical Music Recognition},
  author = {Audiveris Project},
  urldate = {2024-03-14},
  abstract = { Latest generation of Audiveris OMR engine },
  howpublished = {https://github.com/Audiveris/audiveris/},
  langid = {english},
  note = {Accessed: 2024-03-14}, 
  year = {20??},
}


@misc{mayerPracticalEndtoEndOptical2024,
  title = {Practical {{End-to-End Optical Music Recognition}} for {{Pianoform Music}}},
  author = {Mayer, Ji{\v r}{\'i} and Straka, Milan and {Haji{\v c} jr.}, Jan and Pecina, Pavel},
  year = {2024},
  month = mar,
  number = {arXiv:2403.13763},
  eprint = {2403.13763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.13763},
  urldate = {2024-05-28},
  abstract = {The majority of recent progress in Optical Music Recognition (OMR) has been achieved with Deep Learning methods, especially models following the end-to-end paradigm, reading input images and producing a linear sequence of tokens. Unfortunately, many music scores, especially piano music, cannot be easily converted to a linear sequence. This has led OMR researchers to use custom linearized encodings, instead of broadly accepted structured formats for music notation. Their diversity makes it difficult to compare the performance of OMR systems directly. To bring recent OMR model progress closer to useful results: (a) We define a sequential format called Linearized MusicXML, allowing to train an end-to-end model directly and maintaining close cohesion and compatibility with the industry-standard MusicXML format. (b) We create a dev and test set for benchmarking typeset OMR with MusicXML ground truth based on the OpenScore Lieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an image from IMSLP. (c) We train and fine-tune an end-to-end model to serve as a baseline on the dataset and employ the TEDn metric to evaluate the model. We also test our model against the recently published synthetic pianoform dataset GrandStaff and surpass the state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.9,J.5},
  file = {/home/ptorras/zotpapers/Mayer et al_2024_Practical End-to-End Optical Music Recognition for Pianoform Music.pdf;/home/ptorras/Zotero/storage/2BZDKA3W/2403.html}
}
