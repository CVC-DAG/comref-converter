
@misc{noauthor_optical_nodate,
	title = {Optical {Music} {Recognition} {Based} {Deep} {Neural} {Networks} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-981-19-4775-9_136},
	urldate = {2022-10-25},
	file = {Optical Music Recognition Based Deep Neural Networks | SpringerLink:/home/ptorras/Zotero/storage/V4WP9DLA/978-981-19-4775-9_136.html:text/html},
}

@inproceedings{rios-vila_exploring_2020,
	title = {Exploring the two-dimensional nature of music notation for score recognition with end-to-end approaches},
	doi = {10.1109/ICFHR2020.2020.00044},
	abstract = {Optical Music Recognition workflows perform several steps to retrieve the content in music score images, being symbol recognition one of the key stages. State-of-the-art approaches for this stage currently address the coding of the output symbols as if they were plain text characters. However, music symbols have a two-dimensional nature that is ignored in these approaches. In this paper, we explore alternative output representations to perform music symbol recognition with state-of-the-art end-to-end neural technologies. We propose and describe new output representations which take into account the mentioned two-dimensional nature. We seek answers to the question of whether it is possible to obtain better recognition results in both printed and handwritten music scores. In this analysis, we compare the results given using three output encodings and two neural approaches. We found that one of the proposed encodings outperforms the results obtained by the standard one. This permits us to conclude that it is interesting to keep researching on this topic to improve end-to-end music score recognition.},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Ríos-Vila, Antonio and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	month = sep,
	year = {2020},
	keywords = {Deep Learning, Encoding, Image coding, Machine Learning, Music Handwritten Recognition, Neural networks, Optical imaging, Optical Music Recognition, Printed Recognition, Standards, Task analysis, Vocabulary},
	pages = {193--198},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/AHFHJV5F/9257754.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/UCILJNU8/Ríos-Vila et al. - 2020 - Exploring the two-dimensional nature of music nota.pdf:application/pdf},
}

@inproceedings{rios-vila_use_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Use} of {Transformers} for {End}-to-{End} {Optical} {Music} {Recognition}},
	isbn = {978-3-031-04881-4},
	doi = {10.1007/978-3-031-04881-4\_37},
	abstract = {State-of-the-art end-to-end Optical Music Recognition (OMR) systems use Recurrent Neural Networks to produce music transcriptions, as these models retrieve a sequence of symbols from an input staff image. However, recent advances in Deep Learning have led other research fields that process sequential data to use a new neural architecture: the Transformer, whose popularity has increased over time. In this paper, we study the application of the Transformer model to the end-to-end OMR systems. We produced several models based on all the existing approaches in this field and tested them on various corpora with different types of encodings for the output. The obtained results allow us to make an in-depth analysis of the advantages and disadvantages of applying this architecture to these systems. This discussion leads us to conclude that Transformers, as they were conceived, do not seem to be appropriate to perform end-to-end OMR, so this paper raises interesting lines of future research to get the full potential of this architecture in this field.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Ríos-Vila, Antonio and Iñesta, José M. and Calvo-Zaragoza, Jorge},
	editor = {Pinho, Armando J. and Georgieva, Petia and Teixeira, Luís F. and Sánchez, Joan Andreu},
	year = {2022},
	keywords = {Transformers, Optical Music Recognition, Connectionist Temporal Classification, Image-to-sequence},
	pages = {470--481},
	file = {Full Text PDF:/home/ptorras/Zotero/storage/3FDVS2GD/Ríos-Vila et al. - 2022 - On the Use of Transformers for End-to-End Optical .pdf:application/pdf},
}

@article{calvo-zaragoza_end--end_2018,
	title = {End-to-{End} {Neural} {Optical} {Music} {Recognition} of {Monophonic} {Scores}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/8/4/606},
	doi = {10.3390/app8040606},
	abstract = {Optical Music Recognition is a field of research that investigates how to computationally decode music notation from images. Despite the efforts made so far, there are hardly any complete solutions to the problem. In this work, we study the use of neural networks that work in an end-to-end manner. This is achieved by using a neural model that combines the capabilities of convolutional neural networks, which work on the input image, and recurrent neural networks, which deal with the sequential nature of the problem. Thanks to the use of the the so-called Connectionist Temporal Classification loss function, these models can be directly trained from input images accompanied by their corresponding transcripts into music symbol sequences. We also present the Printed Music Scores dataset, containing more than 80,000 monodic single-staff real scores in common western notation, that is used to train and evaluate the neural approach. In our experiments, it is demonstrated that this formulation can be carried out successfully. Additionally, we study several considerations about the codification of the output musical sequences, the convergence and scalability of the neural models, as well as the ability of this approach to locate symbols in the input score.},
	language = {en},
	number = {4},
	urldate = {2022-10-25},
	journal = {Applied Sciences},
	author = {Calvo-Zaragoza, Jorge and Rizo, David},
	month = apr,
	year = {2018},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Deep Learning, end-to-end recognition, music score images, Optical Music Recognition},
	pages = {606},
	file = {Calvo-Zaragoza_Rizo_2018_End-to-End Neural Optical Music Recognition of Monophonic Scores.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza_Rizo_2018_End-to-End Neural Optical Music Recognition of Monophonic Scores.pdf:application/pdf;Full Text PDF:/home/ptorras/Zotero/storage/T9RAQ8FH/Calvo-Zaragoza and Rizo - 2018 - End-to-End Neural Optical Music Recognition of Mon.pdf:application/pdf;Snapshot:/home/ptorras/Zotero/storage/97MC58EZ/606.html:text/html},
}

@inproceedings{calvo-zaragoza_handwritten_2017,
	title = {Handwritten {Music} {Recognition} for {Mensural} {Notation}: {Formulation}, {Data} and {Baseline} {Results}},
	volume = {01},
	shorttitle = {Handwritten {Music} {Recognition} for {Mensural} {Notation}},
	doi = {10.1109/ICDAR.2017.179},
	abstract = {Music is a key element for cultural transmission, and so large collections of music manuscripts have been preserved over the centuries. In order to develop computational tools for analysis, indexing and retrieval from these sources, it is necessary to transcribe the content to some machine-readable format. In this paper we discuss the Handwritten Music Recognition problem, which refers to the development of automatic transcription systems for musical manuscripts. We focus on mensural notation, one of the most widespread varieties of Western classical music. For that, we present a labeled corpus containing 576 staves, along with a baseline recognition system based on a combination of hidden Markov models and N-gram language models. The baseline error obtained at symbol level is about 40 \% which, given the difficulty of the task, can be considered a good starting point for future developments. Our aim is that these data and preliminary results help to promote this research field, serving as a reference in future developments.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, Jorge and Toselli, Alejandro H. and Vidal, Enrique},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical Music Recognition, Task analysis, Hidden Markov models, Image segmentation, Handwriting recognition, Handwritten Music Recognition, Hidden Markov Models, Mensural notation, Music, Natural languages, Shape, hidden Markov models, speech recognition, indexing, music, automatic transcription systems, baseline error, baseline recognition system, computational tools, cultural transmission, handwritten music recognition problem, labeled corpus, machine-readable format, mensural notation, musical manuscripts, n-gram language models, retrieval, Western classical music},
	pages = {1081--1086},
	file = {IEEE Xplore Abstract Record:/home/ptorras/Zotero/storage/KDUXPCQN/8270110.html:text/html;IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/W883QAIZ/Calvo-Zaragoza et al. - 2017 - Handwritten Music Recognition for Mensural Notatio.pdf:application/pdf},
}

@inproceedings{hajic_muscima_2017,
	title = {The {MUSCIMA}++ {Dataset} for {Handwritten} {Optical} {Music} {Recognition}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.16},
	abstract = {Optical Music Recognition (OMR) promises to make accessible the content of large amounts of musical documents, an important component of cultural heritage. However, the field does not have an adequate dataset and ground truth for benchmarking OMR systems, which has been a major obstacle to measurable progress. Furthermore, machine learning methods for OMR require training data. We design and collect MUSCIMA++, a new dataset for OMR. Ground truth in MUSCIMA++ is a notation graph, which our analysis shows to be a necessary and sufficient representation of music notation. Building on the CVC-MUSCIMA dataset for staffline removal, the MUSCIMA++ dataset v1.0 consists of 140 pages of handwritten music, with 91254 manually annotated notation symbols and 82247 explicitly marked relationships between symbol pairs. The dataset allows training and directly evaluating models for symbol classification, symbol localization, and notation graph assembly, and indirectly musical content extraction, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and annotating further, and the data is made available under an open license.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Hajič, Jan and Pecina, Pavel},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Optical Music Recognition, Optical character recognition software, Text analysis, Music, Complexity theory, Dataset, Graph Representation, Image reconstruction, Musical Symbol Detection, Pipelines, handwriting recognition, optical character recognition, document image processing, history, music, music notation, handwritten character recognition, image classification, learning (artificial intelligence), machine learning, MUSCIMA++ dataset, cultural heritage, CVC-MUSCIMA dataset, data visualisation, data visualization, handwritten optical Music Recognition, image preprocessing, manually annotated notation symbols, musical content extraction, musical documents, notation graph assembly, OMR systems, symbol classification, symbol localization, symbol pairs, training data},
	pages = {39--46},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/LMLY4DC8/Hajič and Pecina - 2017 - The MUSCIMA++ Dataset for Handwritten Optical Musi.pdf:application/pdf},
}


@inproceedings{baro_handwritten_2020,
	title = {Handwritten {Historical} {Music} {Recognition} by {Sequence}-to-{Sequence} with {Attention} {Mechanism}},
	doi = {10.1109/ICFHR2020.2020.00046},
	abstract = {Despite decades of research in Optical Music Recognition (OMR), the recognition of old handwritten music scores remains a challenge because of the variabilities in the handwriting styles, paper degradation, lack of standard notation, etc. Therefore, the research in OMR systems adapted to the particularities of old manuscripts is crucial to accelerate the conversion of music scores existing in archives into digital libraries, fostering the dissemination and preservation of our music heritage. In this paper we explore the adaptation of sequence-to-sequence models with attention mechanism (used in translation and handwritten text recognition) and the generation of specific synthetic data for recognizing old music scores. The experimental validation demonstrates that our approach is promising, especially when compared with long short-term memory neural networks.},
	booktitle = {2020 17th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Baró, Arnau and Badal, Carles and Fornés, Alicia},
	month = sep,
	year = {2020},
	keywords = {Hidden Markov models, Training, Text recognition, Handwriting recognition, Feature extraction, Adaptation models, Decoding, Optical music recognition, Handwritten music recognition, Document image analysis and recognition, Historical Documents, Deep neural networks, Sequence to Sequence},
	pages = {205--210},
	file = {IEEE Xplore Full Text PDF:/home/ptorras/Zotero/storage/275RLHSR/Baró et al. - 2020 - Handwritten Historical Music Recognition by Sequen.pdf:application/pdf},
}


@misc{hajic_jr_search_2017,
	title = {In {Search} of a {Dataset} for {Handwritten} {Optical} {Music} {Recognition}: {Introducing} {MUSCIMA}++},
	shorttitle = {In {Search} of a {Dataset} for {Handwritten} {Optical} {Music} {Recognition}},
	url = {http://arxiv.org/abs/1703.04824},
	doi = {10.48550/arXiv.1703.04824},
	abstract = {Optical Music Recognition (OMR) has long been without an adequate dataset and ground truth for evaluating OMR systems, which has been a major problem for establishing a state of the art in the field. Furthermore, machine learning methods require training data. We analyze how the OMR processing pipeline can be expressed in terms of gradually more complex ground truth, and based on this analysis, we design the MUSCIMA++ dataset of handwritten music notation that addresses musical symbol recognition and notation reconstruction. The MUSCIMA++ dataset version 0.9 consists of 140 pages of handwritten music, with 91255 manually annotated notation symbols and 82261 explicitly marked relationships between symbol pairs. The dataset allows training and evaluating models for symbol classification, symbol localization, and notation graph assembly, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and further annotation, and the dataset itself is made available under an open license.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Hajič jr., Jan and Pecina, Pavel},
	month = mar,
	year = {2017},
	note = {arXiv:1703.04824 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.7.5},
	file = {arXiv Fulltext PDF:/home/ptorras/Zotero/storage/VZSFQD5D/Hajič jr. and Pecina - 2017 - In Search of a Dataset for Handwritten Optical Mus.pdf:application/pdf;arXiv.org Snapshot:/home/ptorras/Zotero/storage/AFCYJ9V9/1703.html:text/html;Hajic jr._Pecina_2017_In Search of a Dataset for Handwritten Optical Music Recognition.pdf:/home/ptorras/zotpapers/Hajic jr._Pecina_2017_In Search of a Dataset for Handwritten Optical Music Recognition.pdf:application/pdf},
}

@article{fornes_cvc-muscima_2012,
	title = {{CVC}-{MUSCIMA}: a ground truth of handwritten music score images for writer identification and staff removal},
	volume = {15},
	issn = {1433-2833, 1433-2825},
	shorttitle = {{CVC}-{MUSCIMA}},
	url = {http://link.springer.com/10.1007/s10032-011-0168-2},
	doi = {10.1007/s10032-011-0168-2},
	abstract = {The analysis of music scores has been an active research ﬁeld in the last decades. However, there are no publicly available databases of handwritten music scores for the research community. In this paper, we present the CVCMUSCIMA database and ground truth of handwritten music score images. The dataset consists of 1,000 music sheets written by 50 different musicians. It has been especially designed for writer identiﬁcation and staff removal tasks. In addition to the description of the dataset, ground truth, partitioning, and evaluation metrics, we also provide some baseline results for easing the comparison between different approaches.},
	language = {en},
	number = {3},
	urldate = {2022-10-25},
	journal = {IJDAR},
	author = {Fornés, Alicia and Dutta, Anjan and Gordo, Albert and Lladós, Josep},
	month = sep,
	year = {2012},
	keywords = {Graphics recognition, Staff removal, Performance evaluation, Music scores, Ground truths, Handwritten documents, Writer identification},
	pages = {243--251},
	file = {Fornés et al. - 2012 - CVC-MUSCIMA a ground truth of handwritten music s.pdf:/home/ptorras/Zotero/storage/D7ZKEEVR/Fornés et al. - 2012 - CVC-MUSCIMA a ground truth of handwritten music s.pdf:application/pdf},
}

@article{calvo-zaragoza_understanding_2021,
	title = {Understanding {Optical} {Music} {Recognition}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3397499},
	doi = {10.1145/3397499},
	abstract = {For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: Few introductory materials are available, and, furthermore, the field has struggled with defining itself and building a shared terminology. In this work, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, and (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.},
	language = {en},
	number = {4},
	urldate = {2022-10-26},
	journal = {ACM Comput. Surv.},
	author = {Calvo-Zaragoza, Jorge and Hajič Jr., Jan and Pacha, Alexander},
	month = jul,
	year = {2021},
	keywords = {Optical music recognition, music notation, music scores},
	pages = {1--35},
	file = {Calvo-Zaragoza et al_2019_Understanding Optical Music Recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2019_Understanding Optical Music Recognition.pdf:application/pdf;Calvo-Zaragoza et al_2020_Understanding Optical Music Recognition.pdf:/home/ptorras/zotpapers/Calvo-Zaragoza et al_2020_Understanding Optical Music Recognition.pdf:application/pdf;Calvo-Zaragoza et al. - 2021 - Understanding Optical Music Recognition.pdf:/home/ptorras/Zotero/storage/34XKQLKT/Calvo-Zaragoza et al. - 2021 - Understanding Optical Music Recognition.pdf:application/pdf},
}

@inproceedings{baro_handwritten_2021,
	address = {Alicante, Spain},
	title = {Handwritten {Historical} {Music} {Recognition} through {Sequence}-to-{Sequence} with {Attention} {Mechanism}},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Baró, Arnau and Badal, Carles and Torras, Pau and Fornés, Alicia},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {55--59},
}

@inproceedings{calvo-zaragoza_camera-primus_2018,
	address = {Paris, France},
	title = {Camera-{PrIMuS}: {Neural} {End}-to-{End} {Optical} {Music} {Recognition} on {Realistic} {Monophonic} {Scores}},
	isbn = {978-2-9540351-2-3},
	url = {http://ismir2018.ircam.fr/doc/pdfs/33_Paper.pdf},
	booktitle = {19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Calvo-Zaragoza, Jorge and Rizo, David},
	year = {2018},
	pages = {248--255},
}

@inproceedings{gotham_scores_2018,
	address = {Paris, France},
	title = {Scores of {Scores}: {An} {Openscore} {Project} to {Encode} and {Share} {Sheet} {Music}},
	isbn = {978-1-4503-6522-2},
	url = {http://doi.acm.org/10.1145/3273024.3273026},
	doi = {10.1145/3273024.3273026},
	booktitle = {5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Gotham, Mark and Jonas, Peter and Bower, Bruno and Bosworth, William and Rootham, Daniel and VanHandel, Leigh},
	year = {2018},
	keywords = {music information retrieval, musical scores, corpus study, crowdsourcing, digital music library, song},
	pages = {87--95},
}

@article{mengarelli_omr_2019,
	title = {{OMR} metrics and evaluation: a systematic review},
	issn = {1573-7721},
	doi = {10.1007/s11042-019-08200-0},
	abstract = {Music is rhythm, timbre, tones, intensity and performance. Conventional Western Music Notation (CWMN) is used to generate Music Scores in order to register music on paper. Optical Music Recognition (OMR) studies techniques and algorithms for converting music scores into a readable format for computers. work presents a systematic literature review (SLR) searching for metrics and methods of evaluation and comparing for OMR systems and algorithms. The most commonly used metrics on OMR works are described. A research protocol is elaborated and executed. From 802 publications found, 94 are evaluated. All results are organized and classified focusing on metrics, stages, comparisons, OMR datasets and related works. Although there is still no standard methodology for evaluating OMR systems, a good number of datasets and metrics are already available and apply to all the stages of OMR. Some of the analyzed works can give good directions for future works.},
	journal = {Multimedia Tools and Applications},
	author = {Mengarelli, Luciano and Kostiuk, Bruno and Vitório, João G. and Tibola, Maicon A. and Wolff, William and Silla, Carlos N.},
	month = dec,
	year = {2019},
}

@misc{pacha_music_2020,
	title = {The {Music} {Notation} {Graph} ({MuNG}) {Repository}},
	url = {https://github.com/OMR-Research/mung},
	author = {Pacha, Alexander and Hajič jr., Jan},
	year = {2020},
}

@misc{pacha_github_2017,
	title = {Github {Repository} of the {Music} {Score} {Classifier}},
	url = {https://github.com/apacha/MusicScoreClassifier},
	author = {Pacha, Alexander},
	year = {2017},
}

@inproceedings{parada-cabaleiro_seils_2017,
	address = {Suzhou, China},
	title = {The {SEILS} {Dataset}: {Symbolically} {Encoded} {Scores} in {Modern}-{Early} {Notation} for {Computational} {Musicology}},
	isbn = {978-981-11-5179-8},
	url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/14_Paper.pdf},
	booktitle = {18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Parada-Cabaleiro, Emilia and Batliner, Anton and Baird, Alice and Schuller, Björn},
	year = {2017},
}

@inproceedings{pugin_evaluating_2013,
	address = {Curitiba, Brazil},
	title = {Evaluating {OMR} on the {Early} {Music} {Online} {Collection}},
	url = {http://ismir2013.ismir.net/wp-content/uploads/2013/09/65_Paper.pdf},
	booktitle = {14th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pugin, Laurent and Crawford, Tim},
	editor = {Jr, Alceu de Souza Britto and Gouyon, Fabien and Dixon, Simon},
	year = {2013},
	pages = {439--444},
	file = {Pugin_Crawford_2013_Evaluating OMR on the Early Music Online Collection.pdf:/home/ptorras/zotpapers/Pugin_Crawford_2013_Evaluating OMR on the Early Music Online Collection.pdf:application/pdf},
}

@inproceedings{rios-vila_end--end_2022,
	address = {Online},
	title = {End-{To}-{End} {Full}-{Page} {Optical} {Music} {Recognition} of {Monophonic} {Documents} via {Score} {Unfolding}},
	url = {https://sites.google.com/view/worms2022/proceedings},
	doi = {10.48550/arXiv.2211.13285},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Ríos-Vila, Antonio and Iñesta, Jose M. and Calvo-Zaragoza, Jorge},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander and Shatri, Elona},
	year = {2022},
	pages = {20--24},
	file = {Rios-Vila et al_2022_End-To-End Full-Page Optical Music Recognition of Monophonic Documents via.pdf:/home/ptorras/zotpapers/Rios-Vila et al_2022_End-To-End Full-Page Optical Music Recognition of Monophonic Documents via.pdf:application/pdf},
}

@inproceedings{shatri_doremi_2021,
	address = {Alicante, Spain},
	title = {{DoReMi}: {First} glance at a universal {OMR} dataset},
	url = {https://sites.google.com/view/worms2021/proceedings},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Shatri, Elona and Fazekas, György},
	editor = {Calvo-Zaragoza, Jorge and Pacha, Alexander},
	year = {2021},
	pages = {43--49},
}

@inproceedings{tuggener_deepscores_2018,
	address = {Beijing, China},
	title = {{DeepScores} - {A} {Dataset} for {Segmentation}, {Detection} and {Classification} of {Tiny} {Objects}},
	url = {https://arxiv.org/abs/1804.00525},
	doi = {10.21256/zhaw-4255},
	abstract = {We present the DeepScores dataset with the goal of advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding. DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. With close to a hundred millions of small objects, this makes our dataset not only unique, but also the largest public dataset. DeepScores comes with ground truth for object classification, detection and semantic segmentation. DeepScores thus poses a relevant challenge for computer vision in general, beyond the scope of optical music recognition (OMR) research. We present a detailed statistical analysis of the dataset, comparing it with other computer vision datasets like Caltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer vision datasets, as well as with other OMR datasets. Finally, we provide baseline performances for object classification and give pointers to future research based on this dataset.},
	booktitle = {24th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {ZHAW},
	author = {Tuggener, Lukas and Elezi, Ismail and Schmidhuber, Jürgen and Pelillo, Marcello and Stadelmann, Thilo},
	year = {2018},
	file = {Tuggener et al_2018_DeepScores - A Dataset for Segmentation, Detection and Classification of Tiny.pdf:/home/ptorras/zotpapers/Tuggener et al_2018_DeepScores - A Dataset for Segmentation, Detection and Classification of Tiny.pdf:application/pdf},
}

@inproceedings{tuggener_deepscoresv2_2020,
	address = {Milan, Italy},
	title = {The {DeepScoresV2} {Dataset} and {Benchmark} for {Music} {Object} {Detection}},
	doi = {10.21256/zhaw-20647},
	abstract = {In this paper, we present DeepScoresV2, an extended version of the DeepScores dataset for optical music recognition (OMR). We improve upon the original DeepScores dataset by providing much more detailed annotations, namely (a) annotations for 135 classes including fundamental symbols of non-fixed size and shape, increasing the number of annotated symbols by 23\%; (b) oriented bounding boxes; (c) higher-level rhythm and pitch information (onset beat for all symbols and line position for noteheads); and (d) a compatibility mode for easy use in conjunction with the MUSCIMA++ dataset for OMR on handwritten documents. These additions open up the potential for future advancement in OMR research. Additionally, we release two state-of-the-art baselines for DeepScoresV2 based on Faster R-CNN and the Deep Watershed Detector. An analysis of the baselines shows that regular orthogonal bounding boxes are unsuitable for objects which are long, small, and potentially rotated, such as ties and beams, which demonstrates the need for detection algorithms that naturally incorporate object angles.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Pattern} {Recognition}},
	author = {Tuggener, Lukas and Satyawan, Yvan Putra and Pacha, Alexander and Schmidhuber, Jürgen and Stadelmann, Thilo},
	year = {2020},
	file = {Tuggener et al_2020_The DeepScoresV2 Dataset and Benchmark for Music Object Detection.pdf:/home/ptorras/zotpapers/Tuggener et al_2020_The DeepScoresV2 Dataset and Benchmark for Music Object Detection.pdf:application/pdf},
}


@misc{noauthor_bach_nodate,
	title = {Bach {Digital}},
	url = {https://www.bach-digital.de/content/index.xed},
	urldate = {2023-02-15},
	file = {Bach digital - Start:/home/ptorras/Zotero/storage/NP3V9Q7T/index.html:text/html},
    year = {2008},
    note = {Accessed: 2023-03-01},
}

@misc{noauthor_musescore_nodate,
	title = {MuseScore},
	url = {https://musescore.com/},
	urldate = {2022-12-12},
    year = {2023},
    note = {Accessed: 2022-12-12},
}

@misc{stringquartet_corpus,
	title = {String Quartet Corpus},
	url = {https://github.com/OpenScore/StringQuartets},
	urldate = {2023-11-13},
    year = {2023},
    note = {Accessed: 2023-10-10},
}


@article{tuggener_real_2023,
	title = {Real world music object recognition},
	copyright = {Licence according to publishing contract},
	issn = {2514-3298},
	url = {https://digitalcollection.zhaw.ch/handle/11475/28644},
	doi = {10.21256/zhaw-28644},
	abstract = {We present solutions to two of the most pressing issues in contemporary optical music recognition (OMR).We improve recognition accuracy on low-quality, real-world (i.e. containing ageing, lighting, or dirt artefacts among others) input data and provide confidence-rated model outputs to enable efficient human post-processing. Specifically, we present (i) a sophisticated input augmentation scheme that can reduce the gap between sanitised benchmarks and realistic tasks through a combination of synthetic data and noisy perturbations of real-world documents; (ii) an adversarial discriminative domain adaptation method that can be employed to improve the performance of OMR systems on low-quality data; (iii) a combination of model ensembles and prediction fusion, which generates trustworthy confidence ratings for each prediction. We evaluate our contributions on a newly created test set consisting of manually annotated pages of varying real-world quality, sourced from International Music Score Library Project (IMSLP) / the Petrucci Music Library. With the presented data augmentation scheme, we achieve a doubling in detection performance from 36.0\% to 73.3\% on noisy real-world data compared to state-of-the-art training. This result is then combined with robust confidence ratings paving the way forOMR to be deployed in the realworld. Additionally, we showthe merits of unsupervised adversarial domain adaptation for OMR raising the 36.0\% baseline to 48.9\%. All our code and data are freely available at: https://github.com/raember/s2anet/tree/TISMIR\_publication.},
	language = {en},
	urldate = {2023-09-12},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Tuggener, Lukas and Emberger, Raphael and Ghosh, Adhiraj and Sager, Pascal and Satyawan, Yvan Putra and Montoya, Javier and Goldschagg, Simon and Seibold, Florian and Gut, Urs and Ackermann, Philipp and Schmidhuber, Jürgen and Stadelmann, Thilo},
	month = sep,
	year = {2023},
	note = {Accepted: 2023-09-08T13:52:37Z
Publisher: Ubiquity Press},
	file = {Tuggener et al_2023_Real world music object recognition.pdf:/home/ptorras/zotpapers/Tuggener et al_2023_Real world music object recognition.pdf:application/pdf},
}

@article{rios-vila_end--end_2023,
	title = {End-to-end optical music recognition for pianoform sheet music},
	volume = {26},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-023-00432-z},
	doi = {10.1007/s10032-023-00432-z},
	abstract = {End-to-end solutions have brought about significant advances in the field of Optical Music Recognition. These approaches directly provide the symbolic representation of a given image of a musical score. Despite this, several documents, such as pianoform musical scores, cannot yet benefit from these solutions since their structural complexity does not allow their effective transcription. This paper presents a neural method whose objective is to transcribe these musical scores in an end-to-end fashion. We also introduce the GrandStaff dataset, which contains 53,882 single-system piano scores in common western modern notation. The sources are encoded in both a standard digital music representation and its adaptation for current transcription technologies. The method proposed in this paper is trained and evaluated using this dataset. The results show that the approach presented is, for the first time, able to effectively transcribe pianoform notation in an end-to-end manner.},
	language = {en},
	number = {3},
	urldate = {2023-09-27},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Ríos-Vila, Antonio and Rizo, David and Iñesta, José M. and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2023},
	keywords = {Neural networks, Optical music recognition, GrandStaff, Polyphonic music scores},
	pages = {347--362},
	file = {Rios-Vila et al_2023_End-to-end optical music recognition for pianoform sheet music.pdf:/home/ptorras/zotpapers/Rios-Vila et al_2023_End-to-end optical music recognition for pianoform sheet music2.pdf:application/pdf},
}
@inproceedings{GothamJonas2022,
 abstract = {The OpenScore Lieder Corpus is a collection of over 1,200 nineteenth century songs encoded by a dedicated team of mostly volunteers over several years. Having reported on the initial phase, motivations, design, and community-oriented aspects of the project before, we present here the first, stable, large-scale release of this corpus specifically designed for MIR researchers, complete with comprehensive, structured, linked metadata. The corpus continues to be available under the open CC0 licence and represents a compelling dataset for a range of MIR tasks, not least given its unusual balance of large-scale with high-quality encoding, and of diversity (songs by over 100 composers, from many countries, and in a range of languages) with unity (centred on the nineteenth-century lieder tradition).},
 author = {Gotham, Mark Robert Haigh and Jonas, Peter},
 title = {{The OpenScore Lieder Corpus}},
 keywords = {mec-proceedings, mec-proceedings-2021},
 pages = {131--136},
 publisher = {{Humanities Commons}},
 isbn = {978-84-1302-173-7},
 editor = {M{\"u}nnich, Stefan and Rizo, David},
 booktitle = {{Music Encoding Conference Proceedings 2021}},
 year = {2022},
 doi = {10.17613/1my2-dm23},
 bibbase_note = {<span style="color: green; font-weight: bold">Best Poster Award.</span>},
 displayby = {Contributions from MEC 2021}
}


@misc{Audiveris,
  title = {Audiveris - Open-source Optical Music Recognition},
  author = {Audiveris Project},
  urldate = {2024-03-14},
  abstract = { Latest generation of Audiveris OMR engine },
  howpublished = {https://github.com/Audiveris/audiveris/},
  langid = {english},
  note = {Accessed: 2024-03-14}, 
  year = {20??},
}


@misc{mayerPracticalEndtoEndOptical2024,
  title = {Practical {{End-to-End Optical Music Recognition}} for {{Pianoform Music}}},
  author = {Mayer, Ji{\v r}{\'i} and Straka, Milan and {Haji{\v c} jr.}, Jan and Pecina, Pavel},
  year = {2024},
  month = mar,
  number = {arXiv:2403.13763},
  eprint = {2403.13763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.13763},
  urldate = {2024-05-28},
  abstract = {The majority of recent progress in Optical Music Recognition (OMR) has been achieved with Deep Learning methods, especially models following the end-to-end paradigm, reading input images and producing a linear sequence of tokens. Unfortunately, many music scores, especially piano music, cannot be easily converted to a linear sequence. This has led OMR researchers to use custom linearized encodings, instead of broadly accepted structured formats for music notation. Their diversity makes it difficult to compare the performance of OMR systems directly. To bring recent OMR model progress closer to useful results: (a) We define a sequential format called Linearized MusicXML, allowing to train an end-to-end model directly and maintaining close cohesion and compatibility with the industry-standard MusicXML format. (b) We create a dev and test set for benchmarking typeset OMR with MusicXML ground truth based on the OpenScore Lieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an image from IMSLP. (c) We train and fine-tune an end-to-end model to serve as a baseline on the dataset and employ the TEDn metric to evaluate the model. We also test our model against the recently published synthetic pianoform dataset GrandStaff and surpass the state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.9,J.5},
  file = {/home/ptorras/zotpapers/Mayer et al_2024_Practical End-to-End Optical Music Recognition for Pianoform Music.pdf;/home/ptorras/Zotero/storage/2BZDKA3W/2403.html}
}
